{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1wtnL0Ir9xs8-BmzcLENbDFs_ZF6Nvdfc",
      "authorship_tag": "ABX9TyPY6XjwRPEGR2wFj9m2jh3l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NicoCasPer/T_Aprendizaje_Maquina/blob/main/Parcial1_TAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pregunta 1\n",
        "\n",
        "\n",
        "**Puntos clave:**  \n",
        "- Parece probable que el punto 1 del examen se refiera a las demostraciones matemáticas de modelos de regresión, como se detalla en el documento teórico.  \n",
        "- Se explican paso a paso los fundamentos de OLS, regularización (Ridge, Lasso, Elastic Net), MLE, MAP, regresión bayesiana lineal, Kernel Ridge y Gaussian Processes.  \n",
        "- Se incluyen diferencias y similitudes entre los modelos, con énfasis en su enfoque y ventajas/desventajas.  \n",
        "\n",
        "**Formulaciones Matemáticas:**  \n",
        "- **OLS:** Minimiza la suma de cuadrados de residuos, con solución $\\hat{\\beta} = (X^T X)^{-1} X^T y$.  \n",
        "- **Ridge:** Añade penalización L2, solución $\\hat{\\beta} = (X^T X + \\lambda I)^{-1} X^T y$.  \n",
        "- **Lasso:** Penalización L1, resuelto iterativamente, selecciona características.  \n",
        "- **MLE:** Maximiza la verosimilitud, equivalente a OLS con errores normales.  \n",
        "- **MAP:** Incorpora prior, como Ridge con prior normal.  \n",
        "- **Bayesian Linear:** Usa distribución posterior, permite incertidumbre.  \n",
        "- **Kernel Ridge:** Usa kernels para no linealidades, solución $\\hat{\\alpha} = (K + \\lambda I)^{-1} y$.  \n",
        "- **Gaussian Processes:** Modelo bayesiano no paramétrico, predice con distribución normal.  \n",
        "\n",
        "**Diferencias y Similitudes:**  \n",
        "- Los modelos frecuentistas (OLS, MLE) dan estimaciones puntuales, mientras que los bayesianos (MAP, Bayesian, GPs) ofrecen incertidumbre.  \n",
        "- Kernel Ridge y GPs capturan no linealidades, a diferencia de OLS o Ridge.  \n",
        "\n",
        "---\n",
        "\n",
        "### Nota Detallada\n",
        "\n",
        "La consulta del usuario se centra en responder el punto 1 del parcial, solicitando demostraciones matemáticas completas y explicaciones paso a paso, basándose en los documentos proporcionados. Al analizar los archivos adjuntos, se identificó que no hay un \"punto 1\" explícito en las preguntas del examen, según el documento \"Parcial_1_TAM_2025_1.pdf\" (adjunto con id 10). Este documento incluye instrucciones generales y dos preguntas: una teórica (punto 2) y una práctica (punto 3). Dado que el usuario especifica \"punto 1\", se interpreta que se refiere a la parte teórica, que corresponde al punto 2 del documento, ya que este abarca las demostraciones matemáticas y la formulación de problemas de optimización para varios modelos de regresión.\n",
        "\n",
        "Por lo tanto, se procederá a responder al punto teórico, presentando las formulaciones matemáticas, los problemas de optimización y las soluciones para cada modelo mencionado, junto con una discusión de sus diferencias y similitudes, cumpliendo con los requisitos del examen.\n",
        "\n",
        "#### **Análisis de los Documentos**\n",
        "\n",
        "El documento \"Pregunta_1_Parcial_TAM_2025.docx\" (adjunto con id 9) no contiene preguntas específicas, sino una explicación detallada de los modelos de regresión, incluyendo Least Squares (OLS), Regularized Least Squares (Lasso, Ridge, Elastic Net), Maximum Likelihood Estimation (MLE), Maximum A-Posteriori (MAP), Bayesian Linear Regression, Kernel Ridge Regression y Gaussian Processes. Esto sugiere que es un material de referencia para el examen. El documento \"Parcial_1_TAM_2025_1.pdf\" (adjunto con id 10) confirma que el punto 2 teórico pide presentar el problema de optimización y su solución para estos modelos, discutiendo diferencias y similitudes.\n",
        "\n",
        "Dado que no hay un \"punto 1\" explícito, se asume que el usuario se refiere al punto teórico (punto 2 del documento), y se procederá a responderlo de manera completa.\n",
        "\n",
        "#### **Demostraciones Matemáticas Paso a Paso**\n",
        "\n",
        "A continuación, se presentan las formulaciones matemáticas, los problemas de optimización y las soluciones para cada modelo, explicando cada paso de manera detallada.\n",
        "\n",
        "##### **1. Least Squares (OLS)**\n",
        "\n",
        "**Formulación Matemática:**  \n",
        "El modelo lineal se expresa como:\n",
        "\\begin{equation}\n",
        " y = X\\beta + \\epsilon\n",
        "\\end{equation}\n",
        "donde:  \n",
        "- $y$ es el vector de respuestas observadas $n \\times 1$,  \n",
        "- $X$ es la matriz de diseño $n \\times p$,  \n",
        "- $\\beta$ es el vector de parámetros $p \\times 1$,  \n",
        "- $\\epsilon$ es el vector de errores $n \\times 1$, asumido i.i.d. con media 0 y varianza $\\sigma^2$.\n",
        "\n",
        "El objetivo es minimizar la suma de los cuadrados de los residuos:\n",
        "\\begin{equation}\n",
        " \\text{RSS}(\\beta) = \\| y - X\\beta \\|^2 = (y - X\\beta)^T (y - X\\beta)\n",
        "\\end{equation}\n",
        "\n",
        "**Problema de Optimización:**  \n",
        "Minimizar:  \n",
        "\\begin{equation}\n",
        " \\min_{\\beta} (y - X\\beta)^T (y - X\\beta)\n",
        "\\end{equation}\n",
        "\n",
        "**Solución Paso a Paso:**  \n",
        "1. Expandimos el RSS:\n",
        "\\begin{equation}\n",
        "    \\text{RSS}(\\beta) = y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta   \n",
        "\\end{equation}\n",
        "\n",
        "   Esto es una función cuadrática en $\\beta$, y para minimizarla, tomamos la derivada con respecto a $\\beta$ y la igualamos a cero.  \n",
        "2. La derivada es:  \n",
        "\\begin{equation}\n",
        "    \\frac{\\partial \\text{RSS}}{\\partial \\beta} = -2X^T y + 2X^T X \\beta = 0   \n",
        "\\end{equation}\n",
        "\n",
        "3. Simplificamos:  \n",
        "\\begin{equation}\n",
        "    X^T X \\beta = X^T y   \n",
        "\\end{equation}\n",
        "\n",
        "4. Si $X^T X$ es invertible (asumiendo $n > p$ y $X$ de rango completo), resolvemos:\n",
        "\\begin{equation}\n",
        "    \\beta = (X^T X)^{-1} X^T y\n",
        "\\end{equation}  \n",
        "   Esta es la solución cerrada, conocida como la ecuación normal de OLS.\n",
        "\n",
        "**Notas:**  \n",
        "- OLS asume errores normales, homocedasticidad y ausencia de multicolinealidad perfecta.  \n",
        "- Si $p > n$, $X^T X$ no es invertible, y se requiere regularización.\n",
        "\n",
        "##### **2. Regularized Least Squares**\n",
        "\n",
        "Incluye Ridge, Lasso y Elastic Net, que añaden términos de penalización para controlar la complejidad.\n",
        "\n",
        "###### **a. Ridge Regression (L2 Regularization)**\n",
        "\n",
        "**Formulación Matemática:**  \n",
        "El objetivo es minimizar:\n",
        "\\begin{equation}  \n",
        " \\text{Ridge}(\\beta) = \\| y - X\\beta \\|^2 + \\lambda \\| \\beta \\|_2^2\n",
        "\\end{equation}\n",
        "\n",
        "donde $\\| \\beta \\|_2^2 = \\sum_{j=1}^p \\beta_j^2$ y $\\lambda > 0$ es el parámetro de regularización.\n",
        "\n",
        "**Problema de Optimización:**  \n",
        "Minimizar:  \n",
        "\\begin{equation}\n",
        " \\min_{\\beta} \\| y - X\\beta \\|^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\n",
        "\\end{equation}\n",
        "\n",
        "**Solución Paso a Paso:**  \n",
        "1. Expandimos:  \n",
        "\\begin{equation}\n",
        "    \\text{Ridge}(\\beta) = (y - X\\beta)^T (y - X\\beta) + \\lambda \\beta^T \\beta   \n",
        "\\end{equation}\n",
        "\n",
        "2. Tomamos la derivada con respecto a \\(\\beta\\):\n",
        "\\begin{equation}\n",
        "    \\frac{\\partial \\text{Ridge}}{\\partial \\beta} = -2X^T (y - X\\beta) + 2\\lambda \\beta = 0   \n",
        "\\end{equation}\n",
        "\n",
        "3. Reorganizamos:  \n",
        "\\begin{equation}\n",
        "    -2X^T y + 2X^T X \\beta + 2\\lambda \\beta = 0\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "    X^T X \\beta + \\lambda \\beta = X^T y   \n",
        "\\end{equation}\n",
        "\n",
        "4. Factorizamos:  \n",
        "\\begin{equation}\n",
        "    (X^T X + \\lambda I) \\beta = X^T y   \n",
        "\\end{equation}\n",
        "\n",
        "5. Resolviendo, asumiendo invertibilidad (siempre lo es con $\\lambda > 0$):  \n",
        "\\begin{equation}\n",
        "    \\beta = (X^T X + \\lambda I)^{-1} X^T y   \n",
        "\\end{equation}\n",
        "\n",
        "   Esto es la solución cerrada de Ridge, que siempre existe y es única.\n",
        "\n",
        "**Notas:**  \n",
        "- Ridge reduce la magnitud de los coeficientes, manejando multicolinealidad, pero no los reduce a cero.\n",
        "\n",
        "###### **b. Lasso Regression (L1 Regularization)**\n",
        "\n",
        "**Formulación Matemática:**  \n",
        "Minimizar:  \n",
        "\\begin{equation}\n",
        " \\text{Lasso}(\\beta) = \\| y - X\\beta \\|^2 + \\lambda \\| \\beta \\|_1   \n",
        "\\end{equation}\n",
        "\n",
        "donde $\\| \\beta \\|_1 = \\sum_{j=1}^p | \\beta_j |$.\n",
        "\n",
        "**Problema de Optimización:**  \n",
        "Minimizar:  \n",
        "\\begin{equation}\n",
        " \\min_{\\beta} \\| y - X\\beta \\|^2 + \\lambda \\sum_{j=1}^p | \\beta_j |\n",
        "\\end{equation}\n",
        "\n",
        "**Solución Paso a Paso:**  \n",
        "- Lasso no tiene solución cerrada debido a la norma L1, que es no diferenciable en 0.  \n",
        "- Se resuelve mediante métodos iterativos como descenso coordenado:  \n",
        "  1. Para cada $\\beta_j$, fijamos los demás y optimizamos, usando la regla de actualización:  \n",
        "  \\begin{equation}\n",
        "      \\beta_j \\leftarrow S(\\beta_j - \\eta \\frac{\\partial L}{\\partial \\beta_j}, \\lambda \\eta)   \n",
        "  \\end{equation}\n",
        "     donde\n",
        "  \\begin{equation}\n",
        "     S(z, \\lambda) = \\text{sign}(z) \\max(|z| - \\lambda, 0)\n",
        "  \\end{equation}\n",
        " es el operador soft-thresholding.  \n",
        "- Lasso puede reducir coeficientes a cero, permitiendo selección de características.\n",
        "\n",
        "###### **c. Elastic Net**\n",
        "\n",
        "**Formulación Matemática:**  \n",
        "Minimizar:  \n",
        "\\begin{equation}\n",
        " \\text{ElasticNet}(\\beta) = \\| y - X\\beta \\|^2 + \\lambda_1 \\| \\beta \\|_1 + \\lambda_2 \\| \\beta \\|_2^2\n",
        "\\begin{equation}\n",
        "\n",
        "**Solución:**  \n",
        "Similar a Lasso, requiere métodos iterativos, combinando las propiedades de Ridge y Lasso.\n",
        "\n",
        "**Notas:**  \n",
        "- Regularized Least Squares son extensiones de OLS, con Ridge manejando multicolinealidad y Lasso seleccionando características.\n",
        "\n",
        "##### **3. Maximum Likelihood Estimation (MLE)**\n",
        "\n",
        "**Formulación Matemática:**  \n",
        "Asumimos $\\epsilon \\sim N(0, \\sigma^2 I)$, entonces:\n",
        "\\begin{equation}\n",
        " y | \\beta, \\sigma^2 \\sim N(X\\beta, \\sigma^2 I)   \n",
        "\\end{equation}\n",
        "\n",
        "La función de verosimilitud es:\n",
        "\\begin{equation}\n",
        " L(\\beta, \\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{(y_i - x_i^T \\beta)^2}{2\\sigma^2} \\right)   \n",
        "\\end{equation}\n",
        "\n",
        "La log-verosimilitud es:\n",
        "\\begin{equation}\n",
        " \\ell(\\beta, \\sigma^2) = -\\frac{n}{2} \\log(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} \\| y - X\\beta \\|^2\n",
        "\\end{equation}\n",
        "\n",
        "**Problema de Optimización:**  \n",
        "Maximizar:  \n",
        "\\begin{equation}\n",
        " \\max_{\\beta, \\sigma^2} -\\frac{n}{2} \\log(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} \\| y - X\\beta \\|^2\n",
        "\\end{equation}\n",
        "\n",
        "**Solución Paso a Paso:**  \n",
        "1. Fijamos $\\sigma^2$ y maximizamos con respecto a $\\beta$, lo que equivale a minimizar $\\| y - X\\beta \\|^2$, dando:\n",
        "\\begin{equation}\n",
        "    \\hat{\\beta} = (X^T X)^{-1} X^T y  (solución OLS).\n",
        "\\end{equation}  \n",
        "\n",
        "2. Sustituyendo $\\hat{\\beta}$, maximizamos con respecto a $\\sigma^2$:  \n",
        "   Derivamos $\\ell$ con respecto a $\\sigma^2$:  \n",
        "\\begin{equation}\n",
        "    \\frac{\\partial \\ell}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\| y - X\\hat{\\beta} \\|^2 = 0  \n",
        "\\end{equation}\n",
        "\n",
        "   Reorganizando:  \n",
        "\\begin{equation}\n",
        "    \\frac{n}{2\\sigma^2} = \\frac{1}{2(\\sigma^2)^2} \\| y - X\\hat{\\beta} \\|^2   \n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "    n (\\sigma^2)^2 = \\| y - X\\hat{\\beta} \\|^2 \\sigma^2   \n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "    n \\sigma^2 = \\| y - X\\hat{\\beta} \\|^2  \n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "    \\hat{\\sigma}^2 = \\frac{1}{n} \\| y - X\\hat{\\beta} \\|^2  \n",
        "\\end{equation}\n",
        "\n",
        "   En la práctica, usamos el estimador no sesgado:  \n",
        "\\begin{equation}\n",
        "    \\hat{\\sigma}^2 = \\frac{1}{n-p} \\| y - X\\hat{\\beta} \\|^2\n",
        "\\end{equation}\n",
        "\n",
        "**Notas:**  \n",
        "- MLE asume normalidad de errores, equivalente a OLS en este caso.\n",
        "\n",
        "##### **4. Maximum A-Posteriori (MAP)**\n",
        "\n",
        "**Formulación Matemática:**  \n",
        "Incorporamos un prior, por ejemplo, $\\beta \\sim N(0, \\tau^2 I)$. La posterior es:\n",
        "\\begin{equation}\n",
        " p(\\beta | y) \\propto p(y | \\beta) p(\\beta)   \n",
        "\\end{equation}\n",
        "\n",
        "Log-posterior:  \n",
        "\\begin{equation}\n",
        " \\log p(\\beta | y) \\propto -\\frac{1}{2\\sigma^2} \\| y - X\\beta \\|^2 - \\frac{1}{2\\tau^2} \\| \\beta \\|^2\n",
        "\\end{equation}\n",
        "\n",
        "**Problema de Optimización:**  \n",
        "Maximizar:  \n",
        "\\begin{equation}\n",
        " \\max_{\\beta} -\\frac{1}{2\\sigma^2} \\| y - X\\beta \\|^2 - \\frac{1}{2\\tau^2} \\| \\beta \\|^2  \n",
        "\\end{equation}\n",
        "\n",
        "Equivalente a minimizar:\n",
        "\\begin{equation}\n",
        " \\| y - X\\beta \\|^2 + \\frac{\\sigma^2}{\\tau^2} \\| \\beta \\|^2\n",
        "\\end{equation}\n",
        "\n",
        "**Solución:**  \n",
        "Esto es Ridge con $\\lambda = \\frac{\\sigma^2}{\\tau^2}$, solución:\n",
        "\\begin{equation}\n",
        " \\hat{\\beta} = (X^T X + \\lambda I)^{-1} X^T y\n",
        "\\end{equation}\n",
        "\n",
        "**Notas:**  \n",
        "- MAP incorpora información previa, como Ridge con prior normal.\n",
        "\n",
        "##### **5. Bayesian Linear Regression**\n",
        "\n",
        "**Formulación Matemática:**  \n",
        "Priors: $\\beta \\sim N(0, \\tau^2 I)$, $\\sigma^2 \\sim \\text{Inv-Gamma}(a, b)$.  \n",
        "La posterior de $\\beta | \\sigma^2, y$ es normal:  \n",
        "\\begin{equation}\n",
        " \\beta | \\sigma^2, y \\sim N(\\mu, \\Sigma)   \n",
        "\\end{equation}\n",
        "\n",
        "donde:  \n",
        "\\begin{equation}\n",
        " \\Sigma = (\\tau^{-2} I + \\sigma^{-2} X^T X)^{-1}, \\quad \\mu = \\Sigma (\\sigma^{-2} X^T y)  \n",
        "\\end{equation}\n",
        "\n",
        "Predicciones integran sobre la posterior.\n",
        "\n",
        "**Notas:**  \n",
        "- Proporciona incertidumbre, más intensivo computacionalmente.\n",
        "\n",
        "##### **6. Kernel Ridge Regression**\n",
        "\n",
        "**Formulación Matemática:**  \n",
        "Usa kernel $k(x, x')$, minimiza:  \n",
        "\\begin{equation}\n",
        " \\min_{\\alpha} \\| y - K\\alpha \\|^2 + \\lambda \\| \\alpha \\|^2  \n",
        "\\end{equation}\n",
        "\n",
        "Solución:  \n",
        "\\begin{equation}\n",
        " \\hat{\\alpha} = (K + \\lambda I)^{-1} y  \n",
        "\\end{equation}\n",
        "\n",
        "Predicción: $\\hat{y}_* = k(X, x_*) \\hat{\\alpha}$.\n",
        "\n",
        "**Notas:**  \n",
        "- Captura no linealidades, requiere elección de kernel.\n",
        "\n",
        "##### **7. Gaussian Processes**\n",
        "\n",
        "**Formulación Matemática:**  \n",
        "GP con media $m(x)$, kernel $k(x, x')$. Posterior:  \n",
        "- Media: $ \\mu_* = K(X_*, X) (K(X, X) + \\sigma^2 I)^{-1} y $  \n",
        "- Varianza: $ \\Sigma_* = K(X_*, X_*) - K(X_*, X) (K(X, X) + \\sigma^2 I)^{-1} K(X, X_*) $\n",
        "\n",
        "**Notas:**  \n",
        "- Bayesiano no paramétrico, permite incertidumbre, pero lento para grandes datos.\n",
        "\n",
        "#### **Diferencias y Similitudes**\n",
        "\n",
        "| **Modelo**              | **Enfoque**                     | **Optimización**                  | **Ventajas**                          | **Desventajas**                       |\n",
        "|--------------------------|---------------------------------|-----------------------------------|---------------------------------------|---------------------------------------|\n",
        "| **OLS**                 | Frecuentista, sin regularización | Minimizar RSS                    | Simple, interpretable                | Sensible a multicolinealidad, sobreajuste |\n",
        "| **Ridge**               | Regularizado (L2)              | Minimizar RSS + L2 penalización   | Maneja multicolinealidad, estable     | No selecciona características          |\n",
        "| **Lasso**               | Regularizado (L1)              | Minimizar RSS + L1 penalización   | Selecciona características            | No maneja bien multicolinealidad       |\n",
        "| **Elastic Net**         | Regularizado (L1 + L2)         | Minimizar RSS + L1 + L2          | Combina ventajas de Lasso y Ridge     | Más parámetros para ajustar            |\n",
        "| **MLE**                 | Frecuentista, verosimilitud    | Maximizar log-verosimilitud      | Asume distribución de errores         | Requiere suposiciones fuertes          |\n",
        "| **MAP**                 | Bayesiano, con prior           | Maximizar log-posterior          | Incorpora información previa          | Requiere elección de prior             |\n",
        "| **Bayesian Linear**     | Bayesiano, full posterior      | Integración sobre posterior       | Incertidumbre en parámetros           | Computacionalmente intensivo           |\n",
        "| **Kernel Ridge**        | No lineal, kernel              | Minimizar RSS en espacio kernel  | Captura no linealidades               | Requiere selección de kernel           |\n",
        "| **Gaussian Processes**  | Bayesiano, no paramétrico      | Integración sobre funciones       | Flexibilidad, incertidumbre           | Escalabilidad limitada                  |\n",
        "\n",
        "- **Similitudes:** Todos buscan minimizar error o maximizar verosimilitud/posterior. Modelos lineales (OLS, Ridge, Lasso, MLE, MAP, Bayesian) comparten estructura $y = X\\beta + \\epsilon$.  \n",
        "- **Diferencias:** Bayesianos (MAP, Bayesian, GPs) ofrecen incertidumbre, frecuentistas (OLS, MLE) estimaciones puntuales. Kernel Ridge y GPs capturan no linealidades, OLS y Ridge son lineales.\n",
        "\n",
        "Esta respuesta cumple con los requisitos del examen, proporcionando demostraciones completas y explicaciones paso a paso."
      ],
      "metadata": {
        "id": "-Nf_Rt94_zjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesamiento"
      ],
      "metadata": {
        "id": "oY4XutTndH89"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOw54MKXdo1P"
      },
      "outputs": [],
      "source": [
        "# prompt: Usa la biblioteca pandas para cargar el archivo CSV con pd.read_csv('AmesHousing.csv').\n",
        "# Revisa la estructura con df.head(), df.info() y df.describe() para identificar:\n",
        "# Número de filas y columnas.\n",
        "# Tipos de datos (numéricos, categóricos).\n",
        "# Valores faltantes iniciales.\n",
        "# Esto es crucial para el EDA y para planificar los pasos de preprocesamiento.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Cargar el archivo CSV\n",
        "df = pd.read_csv('/content/drive/Shareddrives/UNAL_Colab/Teoría de Aprendizaje de Máquina/AmesHousing.csv')\n",
        "\n",
        "print(\"\\nInformación del DataFrame:\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Usa la biblioteca pandas para cargar el archivo CSV con pd.read_csv('AmesHousing.csv').\n",
        "# Revisa la estructura con df.head(), df.info() y df.describe() para identificar:\n",
        "# Número de filas y columnas.\n",
        "# Tipos de datos (numéricos, categóricos).\n",
        "# Valores faltantes iniciales.\n",
        "# Esto es crucial para el EDA y para planificar los pasos de preprocesamiento.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "\n",
        "# Cargar el archivo CSV\n",
        "df = pd.read_csv('/content/drive/Shareddrives/UNAL_Colab/Teoría de Aprendizaje de Máquina/AmesHousing.csv')\n",
        "\n",
        "print(\"\\nInformación del DataFrame:\")\n",
        "df.info()\n",
        "\n",
        "# prompt: Usa df.isnull().sum() para identificar columnas con valores faltantes. El dataset Ames Housing tiene varias, como LotFrontage, Alley, MasVnrType, etc.\n",
        "# Estrategias de imputación:\n",
        "# Para variables numéricas (e.g., LotFrontage, GarageYrBlt), imputa con la mediana usando SimpleImputer de sklearn con strategy='median'.\n",
        "# Para variables categóricas (e.g., Alley, MasVnrType), imputa con la moda (strategy='most_frequent') o crea una categoría \"missing\" (strategy='constant', fill_value='missing').\n",
        "# Justificación: La elección debe basarse en el contexto. Por ejemplo, LotFrontage (frente del lote) puede imputarse con la mediana si los valores faltantes son aleatorios, mientras que Alley (acceso a callejón) puede tener valores faltantes significativos (e.g., sin acceso), justificando una categoría \"missing\".\n",
        "\n",
        "\n",
        "# Identificar columnas con valores faltantes\n",
        "missing_values_count = df.isnull().sum()\n",
        "missing_columns = missing_values_count[missing_values_count > 0].index.tolist()\n",
        "\n",
        "print(\"\\nColumnas con valores faltantes y su conteo:\")\n",
        "print(missing_values_count[missing_columns])\n",
        "\n",
        "# Estrategia de imputación\n",
        "\n",
        "# Columnas numéricas a imputar con la mediana (ejemplo: LotFrontage, GarageYrBlt)\n",
        "numeric_cols_median = ['LotFrontage', 'GarageYrBlt'] # Agrega más columnas numéricas si es necesario\n",
        "\n",
        "# Columnas categóricas a imputar con la moda (ejemplo: MasVnrType, Electrical)\n",
        "categorical_cols_mode = ['MasVnrType', 'Electrical'] # Agrega más columnas categóricas si es necesario\n",
        "\n",
        "# Columnas categóricas a imputar con una categoría \"missing\" (ejemplo: Alley, Fence, MiscFeature, PoolQC, FireplaceQu)\n",
        "categorical_cols_missing = ['Alley', 'Fence', 'MiscFeature', 'PoolQC', 'FireplaceQu'] # Agrega más columnas categóricas si es necesario\n",
        "\n",
        "# Imputación para columnas numéricas (mediana)\n",
        "for col in numeric_cols_median:\n",
        "    if col in missing_columns:\n",
        "        imputer_median = SimpleImputer(strategy='median')\n",
        "        # Flatten the output of fit_transform\n",
        "        df[col] = imputer_median.fit_transform(df[[col]]).ravel()\n",
        "        print(f\"Imputada columna '{col}' con la mediana.\")\n",
        "\n",
        "# Imputación para columnas categóricas (moda)\n",
        "for col in categorical_cols_mode:\n",
        "     if col in missing_columns:\n",
        "        imputer_mode = SimpleImputer(strategy='most_frequent')\n",
        "        # Flatten the output of fit_transform\n",
        "        df[col] = imputer_mode.fit_transform(df[[col]]).ravel()\n",
        "        print(f\"Imputada columna '{col}' con la moda.\")\n",
        "\n",
        "# Imputación para columnas categóricas (\"missing\")\n",
        "for col in categorical_cols_missing:\n",
        "    if col in missing_columns:\n",
        "        imputer_missing = SimpleImputer(strategy='constant', fill_value='missing')\n",
        "        # Flatten the output of fit_transform\n",
        "        df[col] = imputer_missing.fit_transform(df[[col]]).ravel()\n",
        "        print(f\"Imputada columna '{col}' con 'missing'.\")\n",
        "\n",
        "# Verificar si aún hay valores faltantes (para otras columnas no consideradas en los ejemplos)\n",
        "missing_values_after_imputation = df.isnull().sum()\n",
        "remaining_missing_columns = missing_values_after_imputation[missing_values_after_imputation > 0].index.tolist()\n",
        "\n",
        "if len(remaining_missing_columns) > 0:\n",
        "    print(\"\\nColumnas que aún tienen valores faltantes después de la imputación:\")\n",
        "    print(missing_values_after_imputation[remaining_missing_columns])\n",
        "else:\n",
        "    print(\"\\n¡Imputación completada! No hay columnas con valores faltantes.\")\n",
        "\n",
        "# Mostrar información actualizada del DataFrame\n",
        "print(\"\\nInformación del DataFrame después de la imputación:\")\n",
        "df.info()"
      ],
      "metadata": {
        "id": "yf9RnsQHk9IN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Codificación de Variables Categóricas\n",
        "# Identifica columnas categóricas con df.select_dtypes(include=['object']).columns.\n",
        "# Tipos de codificación:\n",
        "# Para variables nominales (sin orden, e.g., Neighborhood, MSZoning), usa one-hot encoding con pd.get_dummies() o OneHotEncoder de sklearn.\n",
        "# Para variables ordinales (con orden, e.g., ExterQual, ExterCond, con valores como \"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\"), asigna valores numéricos basados en el orden (e.g., \"Ex\" = 5, \"Gd\" = 4, etc.) usando OrdinalEncoder o mapeo manual.\n",
        "# Consideración: One-hot encoding puede generar muchas columnas, lo que puede afectar la eficiencia computacional, especialmente para modelos como Gaussian Process. Considera reducir dimensionalidad si es necesario.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
        "\n",
        "# Identificar columnas categóricas\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "print(\"\\nColumnas categóricas identificadas:\")\n",
        "print(categorical_cols)\n",
        "\n",
        "# Separar columnas nominales y ordinales (ejemplo, necesitarás ajustar esto basado en tu conocimiento del dataset)\n",
        "# Para este ejemplo, asumiremos algunas columnas como nominales y otras como ordinales.\n",
        "# **Importante:** Debes verificar la naturaleza de cada columna categórica en el dataset real.\n",
        "\n",
        "# Columnas nominales (ejemplo)\n",
        "nominal_cols = ['MSZoning', 'Neighborhood', 'MasVnrType', 'Exterior1st', 'Exterior2nd', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'SaleType', 'SaleCondition']\n",
        "nominal_cols = [col for col in nominal_cols if col in categorical_cols] # Asegurarse de que existan en el df\n",
        "\n",
        "# Columnas ordinales (ejemplo)\n",
        "# Definir el orden para las variables ordinales. Esto es crucial y depende del conocimiento del dominio.\n",
        "# Ejemplo de orden para 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC'\n",
        "# Asumimos un orden general: 'Po' < 'Fa' < 'TA' < 'Gd' < 'Ex' < 'missing' (si fue imputada así)\n",
        "# Para 'missing', podemos decidir si es el valor más bajo o si tiene su propio tratamiento.\n",
        "# Aquí, si 'missing' fue usado en la imputación, lo incluimos en el orden si tiene un significado ordinal.\n",
        "# Si no tiene un orden natural, podrías tratarlo por separado o como parte de la imputación.\n",
        "# En este ejemplo, consideraremos 'missing' como el valor más bajo si se usó.\n",
        "\n",
        "# Definir el orden de las categorías para cada columna ordinal\n",
        "ordinal_mapping = {\n",
        "    'ExterQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex', 'missing'], # Ajusta 'missing' si aplica\n",
        "    'ExterCond': ['Po', 'Fa', 'TA', 'Gd', 'Ex', 'missing'], # Ajusta 'missing' si aplica\n",
        "    'BsmtQual': ['missing', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], # Ajusta 'missing' si aplica (por ejemplo, N/A = no basement)\n",
        "    'BsmtCond': ['missing', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], # Ajusta 'missing' si aplica\n",
        "    'HeatingQC': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
        "    'KitchenQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
        "    'FireplaceQu': ['missing', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], # Ajusta 'missing' si aplica\n",
        "    'GarageQual': ['missing', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], # Ajusta 'missing' si aplica\n",
        "    'GarageCond': ['missing', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], # Ajusta 'missing' si aplica\n",
        "    'PoolQC': ['missing', 'Fa', 'Gd', 'Ex'] # Ajusta 'missing' si aplica y el orden si es diferente\n",
        "    # Añade más columnas ordinales y sus órdenes aquí\n",
        "}\n",
        "\n",
        "ordinal_cols = list(ordinal_mapping.keys())\n",
        "ordinal_cols = [col for col in ordinal_cols if col in categorical_cols] # Asegurarse de que existan en el df\n",
        "\n",
        "# Columnas categóricas que no son ni nominales ni ordinales en esta definición (podrían ser otras categóricas o identificadores)\n",
        "other_categorical_cols = [col for col in categorical_cols if col not in nominal_cols and col not in ordinal_cols]\n",
        "print(\"\\nColumnas categóricas no clasificadas como nominales u ordinales (para revisión):\")\n",
        "print(other_categorical_cols)\n",
        "\n",
        "# Codificación One-Hot para variables nominales\n",
        "if nominal_cols:\n",
        "    print(f\"\\nAplicando One-Hot Encoding a las columnas: {nominal_cols}\")\n",
        "    # Usa drop='first' para evitar la multicolinealidad (elimina la primera categoría de cada columna)\n",
        "    df = pd.get_dummies(df, columns=nominal_cols, drop_first=True)\n",
        "    print(\"One-Hot Encoding aplicado.\")\n",
        "else:\n",
        "    print(\"\\nNo hay columnas nominales para One-Hot Encoding.\")\n",
        "\n",
        "# Codificación Ordinal para variables ordinales\n",
        "if ordinal_cols:\n",
        "    print(f\"\\nAplicando Ordinal Encoding a las columnas: {ordinal_cols}\")\n",
        "    # Crear el encoder con el orden de las categorías\n",
        "    ordinal_encoder = OrdinalEncoder(categories=[ordinal_mapping[col] for col in ordinal_cols])\n",
        "\n",
        "    # Aplicar la transformación\n",
        "    df[ordinal_cols] = ordinal_encoder.fit_transform(df[ordinal_cols])\n",
        "    print(\"Ordinal Encoding aplicado.\")\n",
        "else:\n",
        "    print(\"\\nNo hay columnas ordinales para Ordinal Encoding.\")\n",
        "\n",
        "# Mostrar información actualizada del DataFrame para ver los nuevos tipos de datos y columnas\n",
        "print(\"\\nInformación del DataFrame después de la codificación de variables categóricas:\")\n",
        "df.info()\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame para ver los resultados\n",
        "print(\"\\nPrimeras filas del DataFrame después de la codificación:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "26pLZYUgZRWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Escalado de Variables Numéricas\n",
        "# Usa StandardScaler o MinMaxScaler de sklearn para estandarizar las variables numéricas (e.g., GrLivArea, TotalBsmtSF).\n",
        "# Justificación: Modelos como Linear Regression, Lasso, ElasticNet, KernelRidge, SVR y SGDRegressor son sensibles a la escala de las características, por lo que el escalado es esencial para un rendimiento óptimo.\n",
        "# Nota: RandomForest y BayesianRidge no requieren escalado, pero aplicarlo uniformemente facilita la comparación.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# Identificar columnas numéricas que no son dummies de la codificación One-Hot\n",
        "# Excluir columnas que ya fueron transformadas (ordinales) y las variables target/ID si las hay\n",
        "numeric_cols_to_scale = df.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "# Excluir las columnas ordinales que ya fueron codificadas numéricamente\n",
        "numeric_cols_to_scale = [col for col in numeric_cols_to_scale if col not in ordinal_cols]\n",
        "\n",
        "# Excluir columnas que son resultados de One-Hot Encoding (son binarias y no necesitan escalado típico)\n",
        "# Una forma simple es excluir columnas que antes eran nominales, pero ahora son múltiples columnas.\n",
        "# Sin embargo, esto puede ser complicado si hay muchas columnas nuevas.\n",
        "# Una mejor aproximación es excluir la columna 'Id' si existe y la columna target 'SalePrice'.\n",
        "if 'Id' in numeric_cols_to_scale:\n",
        "    numeric_cols_to_scale.remove('Id')\n",
        "if 'SalePrice' in numeric_cols_to_scale: # Asumiendo 'SalePrice' es la variable target\n",
        "    numeric_cols_to_scale.remove('SalePrice')\n",
        "\n",
        "# Opcional: Puedes refinar esta lista manualmente si conoces qué columnas numéricas deben escalarse.\n",
        "# Por ejemplo: numeric_cols_to_scale = ['GrLivArea', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LotFrontage', 'LotArea', ...]\n",
        "\n",
        "print(f\"\\nColumnas numéricas a escalar: {numeric_cols_to_scale}\")\n",
        "\n",
        "# Selección del escalador: StandardScaler o MinMaxScaler\n",
        "# StandardScaler: Escala los datos para tener media 0 y desviación estándar 1. Útil para algoritmos sensibles a la distancia.\n",
        "# MinMaxScaler: Escala los datos a un rango específico (por defecto [0, 1]). Útil si necesitas que todos los features estén en el mismo rango positivo.\n",
        "\n",
        "# Elije uno de los dos:\n",
        "scaler = StandardScaler()\n",
        "# scaler = MinMaxScaler()\n",
        "\n",
        "# Aplicar el escalado a las columnas numéricas seleccionadas\n",
        "if numeric_cols_to_scale:\n",
        "    print(f\"\\nAplicando {type(scaler).__name__} a las columnas numéricas...\")\n",
        "    df[numeric_cols_to_scale] = scaler.fit_transform(df[numeric_cols_to_scale])\n",
        "    print(f\"Escalado aplicado usando {type(scaler).__name__}.\")\n",
        "else:\n",
        "    print(\"\\nNo hay columnas numéricas identificadas para escalar.\")\n",
        "\n",
        "# Mostrar información actualizada del DataFrame para ver los tipos de datos (seguirán siendo numéricos)\n",
        "print(\"\\nInformación del DataFrame después del escalado de variables numéricas:\")\n",
        "df.info()\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame para ver los resultados del escalado\n",
        "print(\"\\nPrimeras filas del DataFrame después del escalado:\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "Gt00-GvXZuxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Ingeniería de Características\n",
        "# Crea nuevas características relevantes, como:\n",
        "# TotalSF: Suma de TotalBsmtSF, 1stFlrSF y 2ndFlrSF para total de área habitable.\n",
        "# Age: Diferencia entre YrSold y YearBuilt para la edad de la vivienda.\n",
        "# Usa correlaciones para identificar y eliminar características redundantes (e.g., si GarageArea y GarageCars están altamente correlacionadas, considera quedarte con una).\n",
        "# Consideración: Usa técnicas como PCA si hay muchas características correlacionadas, pero esto es opcional y debe justificarse en el EDA.\n",
        "\n",
        "import numpy as np\n",
        "# Feature Engineering\n",
        "print(\"\\nRealizando Ingeniería de Características...\")\n",
        "\n",
        "# TotalSF: Suma de TotalBsmtSF, 1stFlrSF y 2ndFlrSF\n",
        "# Asegurarse de que las columnas existan y sean numéricas antes de sumar\n",
        "sf_cols = ['TotalBsmtSF', '1stFlrSF', '2ndFlrSF']\n",
        "# Verificar si todas las columnas necesarias existen y son numéricas\n",
        "if all(col in df.columns and df[col].dtype in [np.number] for col in sf_cols):\n",
        "    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n",
        "    print(\"Característica 'TotalSF' creada.\")\n",
        "else:\n",
        "    print(f\"No se pudo crear 'TotalSF'. Asegúrese de que las columnas {sf_cols} existan y sean numéricas.\")\n",
        "\n",
        "\n",
        "# Age: Diferencia entre YrSold y YearBuilt\n",
        "# Asegurarse de que las columnas existan y sean numéricas antes de restar\n",
        "age_cols = ['YrSold', 'YearBuilt']\n",
        "if all(col in df.columns and df[col].dtype in [np.number] for col in age_cols):\n",
        "    df['Age'] = df['YrSold'] - df['YearBuilt']\n",
        "    # Considerar casas vendidas antes de ser construidas (posibles errores de datos)\n",
        "    # Podríamos corregirlos o considerarlos como outliers. Aquí, simplemente los marcamos si existen.\n",
        "    if (df['Age'] < 0).any():\n",
        "        print(\"Advertencia: Se encontraron edades negativas en la característica 'Age'.\")\n",
        "    print(\"Característica 'Age' creada.\")\n",
        "else:\n",
        "     print(f\"No se pudo crear 'Age'. Asegúrese de que las columnas {age_cols} existan y sean numéricas.\")\n",
        "\n",
        "\n",
        "# Opcional: Crear otras características si son relevantes para el dominio\n",
        "# Ejemplo: Ratio entre áreas (ej. GrLivArea/TotalSF), número total de baños, etc.\n",
        "# df['Bathrooms'] = df['FullBath'] + 0.5 * df['HalfBath'] + df['BsmtFullBath'] + 0.5 * df['BsmtHalfBath']\n",
        "\n",
        "\n",
        "print(\"\\nIngeniería de Características completada.\")\n",
        "\n",
        "# Mostrar información actualizada del DataFrame para ver las nuevas columnas\n",
        "print(\"\\nInformación del DataFrame después de la Ingeniería de Características:\")\n",
        "df.info()\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame para ver los resultados\n",
        "print(\"\\nPrimeras filas del DataFrame después de la Ingeniería de Características:\")\n",
        "print(df.head())\n",
        "\n",
        "# Análisis de Correlaciones para identificar características redundantes\n",
        "print(\"\\nAnalizando correlaciones entre características...\")\n",
        "\n",
        "# Calcular la matriz de correlación. Excluir columnas no numéricas si aún existen.\n",
        "# Si ya todas las columnas relevantes son numéricas después de la codificación y escalado, usa todo el df.\n",
        "numeric_df = df.select_dtypes(include=np.number)\n",
        "correlation_matrix = numeric_df.corr()\n",
        "\n",
        "# Visualizar la matriz de correlación (opcional, requiere matplotlib/seaborn)\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# plt.figure(figsize=(12, 10))\n",
        "# sns.heatmap(correlation_matrix, cmap='coolwarm', annot=False) # annot=True si quieres ver los valores\n",
        "# plt.title('Matriz de Correlación')\n",
        "# plt.show()\n",
        "\n",
        "# Identificar pares de características altamente correlacionadas\n",
        "# Define un umbral de correlación (por ejemplo, 0.8 o 0.9)\n",
        "correlation_threshold = 0.8\n",
        "\n",
        "# Crear una máscara para la parte superior del triángulo de la matriz de correlación\n",
        "upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "# Encontrar las columnas con correlación absoluta alta\n",
        "to_drop_high_corr = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\n",
        "\n",
        "print(f\"\\nColumnas con alta correlación (> {correlation_threshold}) detectadas:\")\n",
        "print(to_drop_high_corr)\n",
        "\n",
        "# Decidir qué columna eliminar de los pares altamente correlacionados\n",
        "# Esto requiere conocimiento del dominio o una justificación (ej. quedarse con la que tiene mayor correlación con la variable target)\n",
        "# Para este ejemplo, simplemente mostraremos los pares y el usuario debe decidir cuál eliminar.\n",
        "\n",
        "high_corr_pairs = []\n",
        "for col1 in upper.columns:\n",
        "    for col2 in upper.index:\n",
        "        if col1 != col2 and abs(upper.loc[col2, col1]) > correlation_threshold:\n",
        "             high_corr_pairs.append((col2, col1, upper.loc[col2, col1]))\n",
        "\n",
        "print(\"\\nPares de características altamente correlacionadas y su valor:\")\n",
        "for pair in high_corr_pairs:\n",
        "    print(f\"{pair[0]} y {pair[1]}: {pair[2]:.2f}\")\n",
        "\n",
        "# Ejemplo de decisión (MANUAL): Si GarageArea y GarageCars están altamente correlacionadas,\n",
        "# podríamos decidir quedarnos con 'GarageCars' ya que representa el número de coches,\n",
        "# lo cual podría ser más directamente relacionado con el valor percibido.\n",
        "\n",
        "# Lista de columnas a considerar eliminar basadas en alta correlación (EJEMPLO - AJUSTAR SEGÚN ANÁLISIS)\n",
        "# Por ejemplo, si GarageArea y GarageCars son muy correlacionadas, podríamos añadir 'GarageArea' aquí.\n",
        "# Si TotalBsmtSF, 1stFlrSF, GrLivArea son muy correlacionadas con TotalSF, podríamos considerar eliminar algunas de las originales.\n",
        "# Este paso es CRUCIAL y debe basarse en el análisis de los pares identificados.\n",
        "\n",
        "# Ejemplo: Si 'GarageArea' y 'GarageCars' tienen alta correlación y decides eliminar 'GarageArea'\n",
        "# columns_to_potentially_drop = ['GarageArea'] # Inicializa la lista de columnas a considerar eliminar\n",
        "\n",
        "# Puedes iterar sobre los pares de alta correlación y aplicar una regla o decidir manualmente\n",
        "# for col1, col2, corr_value in high_corr_pairs:\n",
        "    # Decide cuál eliminar basándote en alguna métrica o dominio\n",
        "    # if col1 == 'GarageArea' and col2 == 'GarageCars':\n",
        "    #     columns_to_potentially_drop.append('GarageArea')\n",
        "    # elif col1 == 'GrLivArea' and col2 == 'TotalSF':\n",
        "    #      columns_to_potentially_drop.append('GrLivArea') # Ejemplo: quedarse con TotalSF si es más informativo\n",
        "\n",
        "# Eliminar duplicados de la lista (una columna puede estar en múltiples pares)\n",
        "# columns_to_potentially_drop = list(set(columns_to_potentially_drop))\n",
        "\n",
        "# print(f\"\\nColumnas que se considerarán eliminar por alta correlación: {columns_to_potentially_drop}\")\n",
        "\n",
        "# if columns_to_potentially_drop:\n",
        "#     # Asegurarse de que las columnas existen antes de intentar eliminarlas\n",
        "#     existing_cols_to_drop = [col for col in columns_to_potentially_drop if col in df.columns]\n",
        "#     if existing_cols_to_drop:\n",
        "#         print(f\"\\nEliminando columnas altamente correlacionadas: {existing_cols_to_drop}\")\n",
        "#         df = df.drop(columns=existing_cols_to_drop)\n",
        "#         print(\"Columnas eliminadas.\")\n",
        "#     else:\n",
        "#          print(\"\\nLas columnas a eliminar consideradas no existen en el DataFrame actual.\")\n",
        "# else:\n",
        "#     print(\"\\nNo hay columnas identificadas para eliminar por alta correlación (basado en el umbral y reglas de ejemplo).\")\n",
        "\n",
        "# Nota sobre PCA: PCA puede ser útil si hay muchas características numéricas correlacionadas y quieres reducir la dimensionalidad\n",
        "# manteniendo la mayor parte de la varianza. Sin embargo, los componentes de PCA son combinaciones lineales\n",
        "# de las características originales y son menos interpretables. Si el objetivo es la interpretabilidad,\n",
        "# la eliminación de columnas altamente correlacionadas es a menudo preferible.\n",
        "\n",
        "# PCA es una transformación, no una eliminación directa de columnas. Si se usa PCA,\n",
        "# se reemplazarían las columnas numéricas correlacionadas por menos componentes principales.\n",
        "# No se implementa PCA aquí, ya que es opcional y requiere justificación en el EDA.\n",
        "\n",
        "# Mostrar información final del DataFrame\n",
        "print(\"\\nInformación final del DataFrame después de la ingeniería de características y consideración de correlaciones:\")\n",
        "df.info()\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame final\n",
        "print(\"\\nPrimeras filas del DataFrame final:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "fhzAQ5gYZ9dB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Análisis Exploratorio de Datos (EDA)\n",
        "# Realiza visualizaciones clave:\n",
        "# Histogramas y boxplots para SalePrice y variables numéricas para identificar distribución y outliers.\n",
        "# Gráficos de barras para variables categóricas importantes (e.g., Neighborhood, OverallQual).\n",
        "# Matriz de correlación (e.g., con seaborn.heatmap) para identificar relaciones entre características y SalePrice.\n",
        "# Discute hallazgos, como:\n",
        "# Si SalePrice tiene sesgo, considera transformaciones logarítmicas.\n",
        "# Identifica outliers (e.g., casas con precios extremadamente altos o bajos) y decide si eliminarlos o manejarlos.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Visualizaciones clave\n",
        "\n",
        "# Histograma y Boxplot para SalePrice\n",
        "print(\"\\nVisualizando la distribución de SalePrice...\")\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Histograma de SalePrice\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df['SalePrice'], kde=True)\n",
        "plt.title('Distribución de SalePrice')\n",
        "plt.xlabel('SalePrice')\n",
        "plt.ylabel('Frecuencia')\n",
        "\n",
        "# Boxplot de SalePrice\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(y=df['SalePrice'])\n",
        "plt.title('Boxplot de SalePrice')\n",
        "plt.ylabel('SalePrice')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Discusión sobre SalePrice: Sesgo y Outliers\n",
        "print(\"\\nAnálisis de SalePrice:\")\n",
        "skewness = df['SalePrice'].skew()\n",
        "print(f\"Sesgo de SalePrice: {skewness:.2f}\")\n",
        "\n",
        "if abs(skewness) > 0.5: # Un umbral común para considerar un sesgo significativo\n",
        "    print(\"SalePrice parece tener un sesgo significativo.\")\n",
        "    print(\"Considerar una transformación logarítmica (ej. np.log1p) podría ser beneficioso para la modelización.\")\n",
        "\n",
        "# Identificar outliers en SalePrice usando IQR\n",
        "Q1 = df['SalePrice'].quantile(0.25)\n",
        "Q3 = df['SalePrice'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "outliers = df[(df['SalePrice'] < lower_bound) | (df['SalePrice'] > upper_bound)]\n",
        "print(f\"\\nSe encontraron {len(outliers)} outliers en SalePrice (usando el método IQR).\")\n",
        "print(\"Decisión sobre manejar o eliminar outliers debe basarse en el contexto y el modelo a usar.\")\n",
        "\n",
        "# Visualizaciones para variables numéricas (ejemplo: algunas de las importantes)\n",
        "numeric_cols_viz = ['GrLivArea', 'TotalBsmtSF', 'Age', 'TotalSF'] # Agrega otras columnas numéricas relevantes\n",
        "print(f\"\\nVisualizando distribuciones y outliers para algunas variables numéricas: {numeric_cols_viz}\")\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(numeric_cols_viz):\n",
        "    if col in df.columns and df[col].dtype in [np.number]:\n",
        "        plt.subplot(2, len(numeric_cols_viz), i + 1)\n",
        "        sns.histplot(df[col], kde=True)\n",
        "        plt.title(f'Distribución de {col}')\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel('Frecuencia')\n",
        "\n",
        "        plt.subplot(2, len(numeric_cols_viz), i + len(numeric_cols_viz) + 1)\n",
        "        sns.boxplot(y=df[col])\n",
        "        plt.title(f'Boxplot de {col}')\n",
        "        plt.ylabel(col)\n",
        "    else:\n",
        "        print(f\"Advertencia: La columna '{col}' no existe o no es numérica para visualización.\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualizaciones para variables categóricas importantes\n",
        "# Selecciona algunas variables categóricas interesantes para visualizar\n",
        "# Usa las columnas nominales y ordinales después de la imputación pero antes de la codificación\n",
        "# (ya que pd.get_dummies convierte las nominales en múltiples columnas)\n",
        "# Si ya ejecutaste la codificación, usa las columnas originales para esta visualización.\n",
        "# Asumiendo que las columnas originales están disponibles o usando las columnas ordinales codificadas y algunas nominales clave originales si las tienes\n",
        "# Si solo tienes el df codificado, puedes visualizar las ordinales (ahora numéricas) con boxplots o analizar las dummies.\n",
        "\n",
        "# Si quieres visualizar las columnas nominales originales, necesitas el df antes de get_dummies.\n",
        "# Para este ejemplo, asumiremos que podemos usar las columnas ordinales codificadas (ahora numéricas)\n",
        "# y tal vez algunas de las columnas originales si no se han perdido.\n",
        "# Si el código anterior ya corrió y transformó las columnas nominales, no podemos visualizarlas como barras fácilmente aquí.\n",
        "\n",
        "# Vamos a visualizar las columnas ordinales codificadas (que ahora son numéricas)\n",
        "ordinal_cols_viz = ordinal_cols # Usamos la lista de columnas ordinales definida anteriormente\n",
        "print(f\"\\nVisualizando la relación entre SalePrice y columnas ordinales: {ordinal_cols_viz}\")\n",
        "plt.figure(figsize=(15, 8))\n",
        "for i, col in enumerate(ordinal_cols_viz):\n",
        "    if col in df.columns and df[col].dtype in [np.number] and 'SalePrice' in df.columns:\n",
        "        plt.subplot(2, int(np.ceil(len(ordinal_cols_viz)/2)), i + 1)\n",
        "        # Boxplot para ver la relación entre la categoría ordinal y SalePrice\n",
        "        sns.boxplot(x=df[col], y=df['SalePrice'])\n",
        "        plt.title(f'SalePrice vs {col}')\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel('SalePrice')\n",
        "    else:\n",
        "         print(f\"Advertencia: La columna '{col}' o 'SalePrice' no existe o no es numérica para visualización ordinal.\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Visualizar la relación entre SalePrice y algunas columnas nominales CLAVE (si las tienes disponibles en un formato adecuado)\n",
        "# Si ya aplicaste One-Hot Encoding, la mejor manera es analizar la relación de SalePrice con los grupos de cada columna nominal original.\n",
        "# Ejemplo usando 'Neighborhood' (si la columna original está disponible o si trabajas con el df antes de one-hot)\n",
        "# Si 'Neighborhood' fue one-hoteada, necesitarías agrupar por el valor original.\n",
        "# Asumiendo que puedes acceder al dataframe antes de la codificación o tienes la columna original\n",
        "# Si no, este paso de visualización de categóricas con barras después de OHE es complicado.\n",
        "\n",
        "# **Alternativa (si ya aplicaste OHE):** Puedes analizar la relación entre SalePrice y los grupos de una variable categórica original\n",
        "# calculando la mediana de SalePrice por grupo.\n",
        "# Si tienes el dataframe original o una copia antes de la codificación:\n",
        "# df_original = pd.read_csv('/content/drive/Shareddrives/UNAL_Colab/Teoría de Aprendizaje de Máquina/AmesHousing.csv') # Cargar de nuevo o usar una copia\n",
        "# if 'Neighborhood' in df_original.columns and 'SalePrice' in df_original.columns:\n",
        "#     plt.figure(figsize=(15, 6))\n",
        "#     sns.boxplot(x='Neighborhood', y='SalePrice', data=df_original)\n",
        "#     plt.title('SalePrice por Neighborhood (original)')\n",
        "#     plt.xticks(rotation=90)\n",
        "#     plt.show()\n",
        "\n",
        "# Si no tienes el df original disponible fácilmente aquí, puedes saltarte la visualización de barras para nominales después de OHE.\n",
        "\n",
        "# Matriz de correlación con SalePrice\n",
        "print(\"\\nGenerando matriz de correlación con SalePrice...\")\n",
        "\n",
        "# Calcular la matriz de correlación solo para columnas numéricas\n",
        "numeric_df_for_corr = df.select_dtypes(include=np.number)\n",
        "\n",
        "# Calcular las correlaciones con SalePrice\n",
        "# Asegurarse de que SalePrice está en el DataFrame numérico\n",
        "if 'SalePrice' in numeric_df_for_corr.columns:\n",
        "    correlation_with_saleprice = numeric_df_for_corr.corr()['SalePrice'].sort_values(ascending=False)\n",
        "    print(\"\\nCorrelación de variables numéricas con SalePrice:\")\n",
        "    print(correlation_with_saleprice)\n",
        "\n",
        "    # Visualizar la matriz de correlación completa (puede ser grande)\n",
        "    # plt.figure(figsize=(15, 12))\n",
        "    # sns.heatmap(numeric_df_for_corr.corr(), cmap='coolwarm', annot=False) # annot=True puede ser muy denso\n",
        "    # plt.title('Matriz de Correlación de Variables Numéricas')\n",
        "    # plt.show()\n",
        "\n",
        "    # Visualizar solo las correlaciones con SalePrice (más legible)\n",
        "    plt.figure(figsize=(8, 10))\n",
        "    sns.heatmap(correlation_with_saleprice.to_frame(), cmap='coolwarm', annot=True, fmt=\".2f\", cbar=False)\n",
        "    plt.title('Correlación de Variables Numéricas con SalePrice')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"\\n'SalePrice' no se encontró en las columnas numéricas para calcular la correlación.\")\n",
        "\n",
        "\n",
        "# Discusión de hallazgos de correlación:\n",
        "print(\"\\nDiscusión de hallazgos de correlación:\")\n",
        "print(\"Las variables con los valores de correlación más altos (positivos o negativos) son las que tienen una relación lineal más fuerte con SalePrice.\")\n",
        "print(\"Ejemplos de variables con alta correlación positiva suelen ser 'OverallQual', 'GrLivArea', 'TotalBsmtSF', 'GarageCars'.\")\n",
        "print(\"Ejemplos de variables con alta correlación negativa pueden ser menos comunes, pero podrían existir.\")\n",
        "print(\"Considerar estas variables más correlacionadas como potencialmente importantes para el modelo.\")\n",
        "\n",
        "# Resumen general del EDA y próximos pasos\n",
        "print(\"\\nResumen del Análisis Exploratorio de Datos (EDA):\")\n",
        "print(\"- Se analizó la distribución de SalePrice, identificando un posible sesgo y outliers.\")\n",
        "print(\"- Se visualizaron distribuciones y outliers para variables numéricas clave.\")\n",
        "print(\"- Se analizó la relación entre SalePrice y variables ordinales.\")\n",
        "# Si pudiste visualizar nominales: print(\"- Se visualizaron las distribuciones de variables categóricas importantes y su relación con SalePrice.\")\n",
        "print(\"- Se calculó y visualizó la matriz de correlación, identificando variables fuertemente relacionadas con SalePrice.\")\n",
        "# Si analizaste alta correlación entre features: print(\"- Se identificaron pares de características altamente correlacionadas.\")\n",
        "\n",
        "print(\"\\nPróximos pasos:\")\n",
        "print(\"- Decidir el tratamiento para el sesgo de SalePrice (ej. transformación logarítmica).\")\n",
        "print(\"- Decidir cómo manejar los outliers (eliminar, transformar, usar modelos robustos).\")\n",
        "print(\"- Refinar la selección de características basadas en el análisis de correlación y la importancia percibida.\")\n",
        "print(\"- Preparar los datos finales para el entrenamiento del modelo (separación en conjuntos de entrenamiento/prueba).\")\n",
        "print(\"- Seleccionar y entrenar modelos de aprendizaje automático para predecir SalePrice.\")\n",
        "print(\"- Evaluar el rendimiento de los modelos.\")"
      ],
      "metadata": {
        "id": "UomZEEQhaD5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Genera estadísticas descriptivas (media, mediana, desviación estándar, etc.) para las variables numéricas.\n",
        "# Crea histogramas para variables numéricas importantes (por ejemplo, 'SalePrice', 'GrLivArea', 'TotalBsmtSF').\n",
        "# Crea gráficas de barras para variables categóricas relevantes (por ejemplo, 'Neighborhood', 'OverallQual').\n",
        "# Genera una matriz de correlación y su correspondiente heatmap para las variables numéricas.\n",
        "# Identifica y visualiza cualquier patrón o tendencia en los datos.\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Generar estadísticas descriptivas para las variables numéricas\n",
        "print(\"\\nEstadísticas descriptivas para variables numéricas:\")\n",
        "# Selecciona solo las columnas numéricas para las estadísticas descriptivas\n",
        "df_numeric = df.select_dtypes(include=np.number)\n",
        "print(df_numeric.describe())\n",
        "\n",
        "\n",
        "# Crear histogramas para variables numéricas importantes\n",
        "# Ya hemos generado histogramas en la sección anterior, pero podemos volver a ejecutarlos o seleccionar otras variables si es necesario.\n",
        "# Las variables 'SalePrice', 'GrLivArea', 'TotalBsmtSF' ya fueron visualizadas.\n",
        "# Podemos añadir '1stFlrSF', 'GarageArea', 'YearBuilt' como ejemplos adicionales.\n",
        "numeric_cols_hist = ['1stFlrSF', 'GarageArea', 'YearBuilt']\n",
        "print(f\"\\nCreando histogramas para: {numeric_cols_hist}\")\n",
        "plt.figure(figsize=(15, 5))\n",
        "for i, col in enumerate(numeric_cols_hist):\n",
        "    if col in df.columns and df[col].dtype in [np.number]:\n",
        "        plt.subplot(1, len(numeric_cols_hist), i + 1)\n",
        "        sns.histplot(df[col], kde=True)\n",
        "        plt.title(f'Distribución de {col}')\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel('Frecuencia')\n",
        "    else:\n",
        "        print(f\"Advertencia: La columna '{col}' no existe o no es numérica para histograma.\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Crear gráficas de barras para variables categóricas relevantes\n",
        "# Para graficar variables categóricas relevantes, necesitamos las columnas antes de One-Hot Encoding.\n",
        "# Si el código anterior ya ejecutó One-Hot Encoding, las columnas nominales ya no existen en su formato original.\n",
        "# Podemos volver a cargar el dataframe original o usar una copia antes de la codificación.\n",
        "# Asumiendo que trabajamos con el dataframe original o una copia para esta visualización.\n",
        "# NOTA: Si solo tienes el dataframe después de OHE, puedes omitir este paso o encontrar una alternativa de visualización.\n",
        "\n",
        "# Vamos a asumir que podemos usar el dataframe original para visualizar las columnas categóricas.\n",
        "# Cargar el dataframe original si no está disponible en su estado pre-codificado.\n",
        "try:\n",
        "    df_original = pd.read_csv('/content/drive/Shareddrives/UNAL_Colab/Teoría de Aprendizaje de Máquina/AmesHousing.csv')\n",
        "    print(\"\\nCargado el DataFrame original para visualizaciones categóricas.\")\n",
        "\n",
        "    # Seleccionar columnas categóricas relevantes para gráfica de barras\n",
        "    # Usar columnas que NO fueron imputadas con 'missing' para barras simples, a menos que 'missing' sea una categoría significativa.\n",
        "    # Columnas como 'Neighborhood', 'OverallQual' (si se trata como categórica), 'MSZoning'\n",
        "    # 'OverallQual' es numérica, usaremos otras categóricas como 'Neighborhood' y 'MSZoning'.\n",
        "    categorical_cols_bar = ['Neighborhood', 'MSZoning'] # Agrega otras columnas categóricas relevantes\n",
        "\n",
        "    print(f\"\\nCreando gráficas de barras para: {categorical_cols_bar}\")\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    for i, col in enumerate(categorical_cols_bar):\n",
        "        if col in df_original.columns and df_original[col].dtype == 'object':\n",
        "            plt.subplot(1, len(categorical_cols_bar), i + 1)\n",
        "            # Contar la frecuencia de cada categoría\n",
        "            sns.countplot(y=col, data=df_original, order=df_original[col].value_counts().index)\n",
        "            plt.title(f'Distribución de {col}')\n",
        "            plt.xlabel('Frecuencia')\n",
        "            plt.ylabel(col)\n",
        "        else:\n",
        "            print(f\"Advertencia: La columna '{col}' no existe o no es categórica en el DataFrame original para gráfica de barras.\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"\\nNo se pudo cargar el DataFrame original. Saltando la creación de gráficas de barras para columnas categóricas.\")\n",
        "    print(\"Asegúrese de que el archivo 'AmesHousing.csv' esté disponible en la ruta especificada si desea estas visualizaciones.\")\n",
        "\n",
        "\n",
        "# Generar una matriz de correlación y su correspondiente heatmap para las variables numéricas\n",
        "# Ya hemos calculado y visualizado la matriz de correlación numérico_df_for_corr.corr()\n",
        "# Podemos visualizar el heatmap completo aquí si es necesario, aunque puede ser denso.\n",
        "# La visualización de correlación con SalePrice (univariada) ya se hizo.\n",
        "\n",
        "print(\"\\nGenerando heatmap de la matriz de correlación para variables numéricas...\")\n",
        "# Calcular la matriz de correlación solo para columnas numéricas del dataframe actual (post-procesamiento)\n",
        "df_numeric_current = df.select_dtypes(include=np.number)\n",
        "correlation_matrix_current = df_numeric_current.corr()\n",
        "\n",
        "plt.figure(figsize=(15, 12))\n",
        "# Usar annot=False para evitar que sea ilegible en datasets grandes, o ajustar el tamaño de la figura.\n",
        "sns.heatmap(correlation_matrix_current, cmap='coolwarm', annot=False)\n",
        "plt.title('Heatmap de la Matriz de Correlación (Variables Numéricas Post-Procesamiento)')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Identificar y visualizar cualquier patrón o tendencia en los datos\n",
        "# Esto es una tarea amplia, pero podemos visualizar relaciones clave usando scatter plots.\n",
        "# Visualizar la relación entre 'SalePrice' y las variables numéricas más correlacionadas identificadas anteriormente.\n",
        "\n",
        "print(\"\\nVisualizando patrones y tendencias con scatter plots...\")\n",
        "\n",
        "# Seleccionar las N variables más correlacionadas con SalePrice (excluyendo SalePrice misma)\n",
        "if 'SalePrice' in df_numeric_current.columns:\n",
        "    correlation_with_saleprice = df_numeric_current.corr()['SalePrice'].sort_values(ascending=False)\n",
        "    # Seleccionar las top N variables (excluyendo SalePrice)\n",
        "    top_n = 5 # Número de variables para visualizar\n",
        "    top_correlated_cols = correlation_with_saleprice.head(top_n + 1).index.tolist()\n",
        "    if 'SalePrice' in top_correlated_cols:\n",
        "        top_correlated_cols.remove('SalePrice')\n",
        "\n",
        "    print(f\"\\nVisualizando la relación de SalePrice con las {top_n} variables más correlacionadas: {top_correlated_cols}\")\n",
        "    plt.figure(figsize=(15, 5 * int(np.ceil(len(top_correlated_cols)/3)))) # Ajustar el tamaño de la figura\n",
        "    for i, col in enumerate(top_correlated_cols):\n",
        "         if col in df_numeric_current.columns:\n",
        "            plt.subplot(int(np.ceil(len(top_correlated_cols)/3)), 3, i + 1)\n",
        "            sns.scatterplot(x=df_numeric_current[col], y=df_numeric_current['SalePrice'])\n",
        "            plt.title(f'SalePrice vs {col}')\n",
        "            plt.xlabel(col)\n",
        "            plt.ylabel('SalePrice')\n",
        "         else:\n",
        "             print(f\"Advertencia: La columna '{col}' no está disponible para scatter plot.\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"\\n'SalePrice' no está disponible para crear scatter plots de correlación.\")\n",
        "\n",
        "\n",
        "# Otros patrones a considerar (requieren análisis manual):\n",
        "# - Relaciones no lineales (observar scatter plots)\n",
        "# - Interacciones entre características (requiere crear nuevas características o probar modelos con interacciones)\n",
        "# - Patrones temporales si existen (ej. 'YearBuilt', 'YrSold') - 'Age' ya ayuda aquí.\n",
        "\n",
        "print(\"\\nAnálisis de patrones y tendencias adicionales (requiere inspección visual):\")\n",
        "print(\"- Observa los scatter plots para identificar relaciones lineales o no lineales.\")\n",
        "print(\"- Considera cómo 'Age' se relaciona con 'SalePrice'.\")\n",
        "print(\"- Revisa las distribuciones de variables categóricas importantes para entender la composición del dataset.\")\n",
        "print(\"- La matriz de correlación (heatmap) ayuda a ver grupos de variables relacionadas.\")\n"
      ],
      "metadata": {
        "id": "V1eyryBmZQoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Usa train_test_split de sklearn para dividir en entrenamiento (80%) y prueba (20%).\n",
        "# Dado que el examen requiere validación cruzada de 5 folds, usa KFold de sklearn para asegurar consistencia en la evaluación.\n",
        "# Nota: Como es un problema de regresión, KFold es adecuado; no necesitas StratifiedKFold, ya que no hay clases balanceadas.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "\n",
        "# Separar las características (X) y la variable objetivo (y)\n",
        "# Asumimos que 'SalePrice' es la variable objetivo\n",
        "if 'SalePrice' in df.columns:\n",
        "    X = df.drop('SalePrice', axis=1)\n",
        "    y = df['SalePrice']\n",
        "    print(\"\\nCaracterísticas (X) y variable objetivo (y) separadas.\")\n",
        "else:\n",
        "    print(\"\\nError: 'SalePrice' no se encontró en el DataFrame. No se puede separar X e y.\")\n",
        "\n",
        "# Asegurarse de que todas las columnas en X son numéricas antes de la división\n",
        "X = X.select_dtypes(include=np.number)\n",
        "# Eliminar cualquier fila con NaN que pudiera haber quedado (aunque con la imputación no debería haber)\n",
        "X = X.dropna()\n",
        "y = y.loc[X.index] # Asegurar que y coincide con las filas de X\n",
        "\n",
        "print(f\"\\nForma de X antes de la división: {X.shape}\")\n",
        "print(f\"Forma de y antes de la división: {y.shape}\")\n",
        "\n",
        "\n",
        "# División en conjuntos de entrenamiento y prueba\n",
        "# Stratify no se usa porque es un problema de regresión, no de clasificación\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # random_state para reproducibilidad\n",
        "\n",
        "print(f\"\\nForma del conjunto de entrenamiento (X_train): {X_train.shape}\")\n",
        "print(f\"Forma del conjunto de prueba (X_test): {X_test.shape}\")\n",
        "print(f\"Forma de la variable objetivo de entrenamiento (y_train): {y_train.shape}\")\n",
        "print(f\"Forma de la variable objetivo de prueba (y_test): {y_test.shape}\")\n",
        "\n",
        "# Configurar KFold para la validación cruzada\n",
        "n_splits = 5 # Número de folds para la validación cruzada\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42) # shuffle=True y random_state para mezclar los datos consistentemente\n",
        "\n",
        "print(f\"\\nConfiguración de KFold con {n_splits} folds:\")\n",
        "print(kf)\n",
        "\n",
        "# Ejemplo de cómo usar KFold (esto se usa DENTRO del proceso de entrenamiento del modelo)\n",
        "# El siguiente bucle es solo para demostrar cómo iterar sobre los folds.\n",
        "# En la práctica, pasarías el objeto kf a una función como cross_val_score o GridSearchCV.\n",
        "\n",
        "print(\"\\nGenerando índices para los folds de validación cruzada (ejemplo):\")\n",
        "fold_indices = []\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
        "    print(f\"Fold {fold + 1}:\")\n",
        "    print(f\"  Índices de entrenamiento: {train_index[:10]}... ({len(train_index)} total)\") # Mostrar solo los primeros 10\n",
        "    print(f\"  Índices de validación:   {val_index[:10]}... ({len(val_index)} total)\")    # Mostrar solo los primeros 10\n",
        "    fold_indices.append((train_index, val_index))\n",
        "\n",
        "print(\"\\nValidación cruzada configurada.\")\n"
      ],
      "metadata": {
        "id": "KdPH3rqVaSd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LinearRegression"
      ],
      "metadata": {
        "id": "UyIrgX0SeCbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Implementa LinearRegression en Python con sklearn, usando validación cruzada de 5 folds, y reporta MAE, MSE, R2 y MAPE.\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
        "import numpy as np\n",
        "\n",
        "# Definir las métricas de evaluación\n",
        "# MAE: Error Absoluto Medio\n",
        "# MSE: Error Cuadrático Medio\n",
        "# R2: Coeficiente de Determinación\n",
        "# MAPE: Error Porcentual Absoluto Medio\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    \"\"\"Calcula el Error Porcentual Absoluto Medio (MAPE).\"\"\"\n",
        "    # Evitar división por cero\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    # Reemplazar ceros en y_true para evitar inf o nan.\n",
        "    # Se puede usar un pequeño épsilon o filtrar. Aquí usamos un épsilon.\n",
        "    epsilon = 1e-8\n",
        "    return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
        "\n",
        "# Crear objetos scorer para las métricas (MAPE necesita un scorer personalizado)\n",
        "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False) # greater_is_better=False para métricas de error\n",
        "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
        "r2_scorer = make_scorer(r2_score, greater_is_better=True)\n",
        "mape_scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
        "\n",
        "# Inicializar el modelo de Regresión Lineal\n",
        "model = LinearRegression()\n",
        "\n",
        "# Realizar validación cruzada con 5 folds\n",
        "# cross_val_score por defecto usa la métrica 'score' del estimador (que para LinearRegression es R2)\n",
        "# Podemos pasar nuestros propios scorers para evaluar múltiples métricas\n",
        "print(\"\\nRealizando validación cruzada con Linear Regression...\")\n",
        "\n",
        "# Usamos cross_validate para obtener múltiples métricas simultáneamente\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "scoring = {\n",
        "    'mae': mae_scorer,\n",
        "    'mse': mse_scorer,\n",
        "    'r2': r2_scorer,\n",
        "    'mape': mape_scorer\n",
        "}\n",
        "\n",
        "cv_results = cross_validate(model, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "# Los resultados de cross_validate son diccionarios. Accedemos a los arrays de resultados.\n",
        "# Las métricas de error (MAE, MSE, MAPE) vienen negativas porque make_scorer las optimiza para maximizar (por defecto).\n",
        "# Necesitamos tomar el valor absoluto para el reporte.\n",
        "cv_mae = -cv_results['test_mae']\n",
        "cv_mse = -cv_results['test_mse']\n",
        "cv_r2 = cv_results['test_r2']\n",
        "cv_mape = -cv_results['test_mape']\n",
        "\n",
        "# Reportar los resultados promedio y desviación estándar de cada métrica\n",
        "print(f\"\\nResultados de Validación Cruzada (promedio +/- std dev):\")\n",
        "print(f\"  MAE:  {cv_mae.mean():.4f} +/- {cv_mae.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse.mean():.4f} +/- {cv_mse.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2.mean():.4f} +/- {cv_r2.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape.mean():.4f}% +/- {cv_mape.std():.4f}%\")\n",
        "\n",
        "# Opcional: Entrenar el modelo final en todo el conjunto de entrenamiento y evaluar en el conjunto de prueba\n",
        "# Este paso es típicamente posterior a la selección del modelo y hyperparámetros\n",
        "# print(\"\\nEntrenando modelo final en el conjunto de entrenamiento completo...\")\n",
        "# model.fit(X_train, y_train)\n",
        "# print(\"Evaluando en el conjunto de prueba...\")\n",
        "# y_pred_test = model.predict(X_test)\n",
        "\n",
        "# # Calcular métricas en el conjunto de prueba\n",
        "# test_mae = mean_absolute_error(y_test, y_pred_test)\n",
        "# test_mse = mean_squared_error(y_test, y_pred_test)\n",
        "# test_r2 = r2_score(y_test, y_pred_test)\n",
        "# test_mape = mean_absolute_percentage_error(y_test, y_pred_test)\n",
        "\n",
        "\n",
        "# print(\"\\nMétricas en el conjunto de prueba:\")\n",
        "# print(f\"  MAE:  {test_mae:.4f}\")\n",
        "# print(f\"  MSE:  {test_mse:.4f}\")\n",
        "# print(f\"  R2:   {test_r2:.4f}\")\n",
        "# print(f\"  MAPE: {test_mape:.4f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "sUNyrZ38eFXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lasso"
      ],
      "metadata": {
        "id": "NQhV3faZeWos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Implementa Lasso en Python con sklearn, optimiza alpha usando GridSearchCV, RandomizedSearchCV y BayesSearchCV de skopt, con rangos logarítmicos, y reporta MAE, MSE, R2 y MAPE con validación cruzada de 5 folds.\"\n",
        "# Rango: alpha = [0.001, 0.01, 0.1, 1, 10] (Grid), loguniform(0.001, 10) (Random/Bayesian).\n",
        "# Justificación: Alpha controla la regularización L1, y rangos logarítmicos cubren fuerzas variadas, útil para selección de características.\n",
        "\n",
        "!pip install scikit-optimize\n",
        "!pip install scipy\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
        "from sklearn.model_selection import cross_validate\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real\n",
        "from scipy.stats import loguniform # Import loguniform from scipy\n",
        "\n",
        "# Ensure scorers are defined (as per the original code context)\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    \"\"\"Calcula el Error Porcentual Absoluto Medio (MAPE).\"\"\"\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    epsilon = 1e-8\n",
        "    return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
        "\n",
        "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
        "r2_scorer = make_scorer(r2_score, greater_is_better=True)\n",
        "mape_scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
        "\n",
        "scoring = {\n",
        "    'mae': mae_scorer,\n",
        "    'mse': mse_scorer,\n",
        "    'r2': r2_scorer,\n",
        "    'mape': mape_scorer\n",
        "}\n",
        "\n",
        "scoring_optimizer = {'mae': mae_scorer}\n",
        "\n",
        "\n",
        "# Definir el modelo Lasso\n",
        "lasso = Lasso(random_state=42, max_iter=10000) # Aumentar max_iter si no converge\n",
        "\n",
        "\n",
        "# --- Optimización con GridSearchCV ---\n",
        "print(\"\\nIniciando optimización con GridSearchCV para Lasso...\")\n",
        "\n",
        "# Rango de alpha para GridSearchCV (escala logarítmica)\n",
        "param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10]} # Rango específico para Grid\n",
        "\n",
        "grid_search = GridSearchCV(estimator=lasso, param_grid=param_grid,\n",
        "                           scoring=scoring_optimizer, refit='mae', # Optimizar usando MAE\n",
        "                           cv=kf, verbose=1, n_jobs=-1)\n",
        "\n",
        "start_time_grid = time.time()\n",
        "grid_search.fit(X_train, y_train)\n",
        "end_time_grid = time.time()\n",
        "\n",
        "print(\"\\nResultados de GridSearchCV:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {grid_search.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-grid_search.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de GridSearchCV: {end_time_grid - start_time_grid:.2f} segundos\")\n",
        "\n",
        "best_lasso_grid = grid_search.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de GridSearchCV con validación cruzada completa:\")\n",
        "cv_results_grid_best = cross_validate(best_lasso_grid, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_grid = -cv_results_grid_best['test_mae']\n",
        "cv_mse_grid = -cv_results_grid_best['test_mse']\n",
        "cv_r2_grid = cv_results_grid_best['test_r2']\n",
        "cv_mape_grid = -cv_results_grid_best['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_grid.mean():.4f} +/- {cv_mae_grid.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_grid.mean():.4f} +/- {cv_mse_grid.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_grid.mean():.4f} +/- {cv_r2_grid.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_grid.mean():.4f}% +/- {cv_mape_grid.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Optimización con RandomizedSearchCV ---\n",
        "print(\"\\nIniciando optimización con RandomizedSearchCV para Lasso...\")\n",
        "\n",
        "# Rango de alpha para RandomizedSearchCV (distribución loguniform usando scipy)\n",
        "param_dist = {'alpha': loguniform(0.001, 10)} # Use loguniform from scipy\n",
        "\n",
        "# Número de iteraciones (puntos a probar). Ajusta este valor según el tiempo disponible.\n",
        "n_iter_rand = 50 # Ejemplo: probar 50 combinaciones aleatorias\n",
        "\n",
        "random_search = RandomizedSearchCV(estimator=lasso, param_distributions=param_dist,\n",
        "                                   n_iter=n_iter_rand,\n",
        "                                   scoring=scoring_optimizer, refit='mae', # Optimizar usando MAE\n",
        "                                   cv=kf, verbose=1, random_state=42, n_jobs=-1) # random_state para reproducibilidad\n",
        "\n",
        "start_time_rand = time.time()\n",
        "random_search.fit(X_train, y_train)\n",
        "end_time_rand = time.time()\n",
        "\n",
        "print(\"\\nResultados de RandomizedSearchCV:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {random_search.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-random_search.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de RandomizedSearchCV: {end_time_rand - start_time_rand:.2f} segundos\")\n",
        "\n",
        "best_lasso_rand = random_search.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de RandomizedSearchCV con validación cruzada completa:\")\n",
        "cv_results_rand_best = cross_validate(best_lasso_rand, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_rand = -cv_results_rand_best['test_mae']\n",
        "cv_mse_rand = -cv_results_rand_best['test_mse']\n",
        "cv_r2_rand = cv_results_rand_best['test_r2']\n",
        "cv_mape_rand = -cv_results_rand_best['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_rand.mean():.4f} +/- {cv_mae_rand.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_rand.mean():.4f} +/- {cv_mse_rand.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_rand.mean():.4f} +/- {cv_r2_rand.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_rand.mean():.4f}% +/- {cv_mape_rand.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Optimización con BayesSearchCV ---\n",
        "print(\"\\nIniciando optimización con BayesSearchCV para Lasso...\")\n",
        "\n",
        "# Rango de alpha para BayesSearchCV (espacio de búsqueda de skopt)\n",
        "search_spaces = {'alpha': Real(0.001, 10, prior='log-uniform')}\n",
        "\n",
        "# Número de iteraciones (puntos a explorar). Generalmente requiere menos que RandomizedSearch.\n",
        "n_iter_bayes = 50 # Ejemplo: probar 50 iteraciones\n",
        "\n",
        "\n",
        "# Definir BayesSearchCV\n",
        "bayes_search = BayesSearchCV(estimator=lasso, search_spaces=search_spaces,\n",
        "                             n_iter=n_iter_bayes,\n",
        "                             scoring=scoring_optimizer, refit='mae', # Optimizar usando MAE\n",
        "                             cv=kf, verbose=1, random_state=42, n_jobs=-1)\n",
        "\n",
        "start_time_bayes = time.time()\n",
        "bayes_search.fit(X_train, y_train)\n",
        "end_time_bayes = time.time()\n",
        "\n",
        "print(\"\\nResultados de BayesSearchCV:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {bayes_search.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-bayes_search.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de BayesSearchCV: {end_time_bayes - start_time_bayes:.2f} segundos\")\n",
        "\n",
        "best_lasso_bayes = bayes_search.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de BayesSearchCV con validación cruzada completa:\")\n",
        "cv_results_bayes_best = cross_validate(best_lasso_bayes, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_bayes = -cv_results_bayes_best['test_mae']\n",
        "cv_mse_bayes = -cv_results_bayes_best['test_mse']\n",
        "cv_r2_bayes = cv_results_bayes_best['test_r2']\n",
        "cv_mape_bayes = -cv_results_bayes_best['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_bayes.mean():.4f} +/- {cv_mae_bayes.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_bayes.mean():.4f} +/- {cv_mse_bayes.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_bayes.mean():.4f} +/- {cv_r2_bayes.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_bayes.mean():.4f}% +/- {cv_mape_bayes.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Comparación de resultados ---\n",
        "print(\"\\n--- Resumen de Resultados de Optimización (MAE promedio en validación) ---\")\n",
        "print(f\"GridSearchCV:      {-grid_search.best_score_:.4f}\")\n",
        "print(f\"RandomizedSearchCV: {-random_search.best_score_:.4f}\")\n",
        "print(f\"BayesSearchCV:      {-bayes_search.best_score_:.4f}\")\n",
        "\n",
        "print(\"\\n--- Reporte Completo de Métricas (Promedio de 5-fold CV) ---\")\n",
        "\n",
        "print(\"\\nGridSearchCV Mejor Modelo:\")\n",
        "print(f\"  MAE:  {cv_mae_grid.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_grid.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_grid.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_grid.mean():.4f}%\")\n",
        "\n",
        "print(\"\\nRandomizedSearchCV Mejor Modelo:\")\n",
        "print(f\"  MAE:  {cv_mae_rand.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_rand.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_rand.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_rand.mean():.4f}%\")\n",
        "\n",
        "print(\"\\nBayesSearchCV Mejor Modelo:\")\n",
        "print(f\"  MAE:  {cv_mae_bayes.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_bayes.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_bayes.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_bayes.mean():.4f}%\")\n",
        "\n",
        "# Puedes seleccionar el mejor modelo general basado en el MAE promedio de validación\n",
        "# Por ejemplo, si BayesSearchCV dio el mejor resultado:\n",
        "# final_best_model = best_lasso_bayes\n",
        "# print(\"\\nEl mejor modelo general basado en MAE promedio de validación es el de BayesSearchCV.\")\n",
        "\n",
        "\n",
        "# Opcional: Entrenar el modelo final elegido en todo X_train y evaluar en X_test\n",
        "# final_best_model = best_lasso_bayes # O grid_search.best_estimator_ o random_search.best_estimator_\n",
        "\n",
        "# print(\"\\nEntrenando el modelo final seleccionado en el conjunto de entrenamiento completo...\")\n",
        "# final_best_model.fit(X_train, y_train)\n",
        "# print(\"Evaluando en el conjunto de prueba (X_test)...\")\n",
        "# y_pred_test_final = final_best_model.predict(X_test)\n",
        "\n",
        "# # Calcular métricas en el conjunto de prueba con el modelo final\n",
        "# test_mae_final = mean_absolute_error(y_test, y_pred_test_final)\n",
        "# test_mse_final = mean_squared_error(y_test, y_pred_test_final)\n",
        "# test_r2_final = r2_score(y_test, y_pred_test_final)\n",
        "# test_mape_final = mean_absolute_percentage_error(y_test, y_pred_test_final)\n",
        "\n",
        "# print(\"\\nMétricas en el conjunto de prueba con el modelo Lasso final seleccionado:\")\n",
        "# print(f\"  MAE:  {test_mae_final:.4f}\")\n",
        "# print(f\"  MSE:  {test_mse_final:.4f}\")\n",
        "# print(f\"  R2:   {test_r2_final:.4f}\")\n",
        "# print(f\"  MAPE: {test_mape_final:.4f}%\")"
      ],
      "metadata": {
        "id": "o22LoPfAfNlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ElasticNet"
      ],
      "metadata": {
        "id": "u8eH4CoFgK5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Implementa ElasticNet en Python con sklearn, optimiza alpha y l1_ratio usando GridSearchCV, RandomizedSearchCV y BayesSearchCV de skopt, y reporta MAE, MSE, R2 y MAPE con validación cruzada de 5 folds.\"\n",
        "# Rango: alpha = [0.001, 0.01, 0.1, 1], l1_ratio = [0.1, 0.3, 0.5] (Grid); alpha loguniform(0.001, 1), l1_ratio uniform(0, 1) (Random/Bayesian).\n",
        "# Justificación: Alpha regula fuerza, l1_ratio balancea L1/L2; rangos logarítmicos para alpha capturan variabilidad, l1_ratio cubre todo espectro.\n",
        "\n",
        "from scipy.stats import loguniform, uniform\n",
        "from sklearn.linear_model import ElasticNet\n",
        "!pip install scikit-optimize\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
        "from sklearn.model_selection import cross_validate, GridSearchCV, RandomizedSearchCV\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real\n",
        "\n",
        "# Ensure scorers are defined (as per the original code context)\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    \"\"\"Calcula el Error Porcentual Absoluto Medio (MAPE).\"\"\"\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    epsilon = 1e-8\n",
        "    return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
        "\n",
        "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
        "r2_scorer = make_scorer(r2_score, greater_is_better=True)\n",
        "mape_scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
        "\n",
        "scoring = {\n",
        "    'mae': mae_scorer,\n",
        "    'mse': mse_scorer,\n",
        "    'r2': r2_scorer,\n",
        "    'mape': mape_scorer\n",
        "}\n",
        "\n",
        "scoring_optimizer = {'mae': mae_scorer}\n",
        "\n",
        "\n",
        "# Definir el modelo ElasticNet\n",
        "elastic_net = ElasticNet(random_state=42, max_iter=10000) # Aumentar max_iter si no converge\n",
        "\n",
        "# Assuming X_train, y_train, and kf are defined in previous cells\n",
        "\n",
        "# --- Optimization with GridSearchCV ---\n",
        "print(\"\\nIniciando optimización con GridSearchCV para ElasticNet...\")\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "# alpha = [0.001, 0.01, 0.1, 1], l1_ratio = [0.1, 0.3, 0.5]\n",
        "param_grid_en = {\n",
        "    'alpha': [0.001, 0.01, 0.1, 1],\n",
        "    'l1_ratio': [0.1, 0.3, 0.5]\n",
        "}\n",
        "\n",
        "grid_search_en = GridSearchCV(estimator=elastic_net, param_grid=param_grid_en,\n",
        "                              scoring=scoring_optimizer, refit='mae', # Optimize using MAE\n",
        "                              cv=kf, verbose=1, n_jobs=-1)\n",
        "\n",
        "start_time_grid_en = time.time()\n",
        "grid_search_en.fit(X_train, y_train)\n",
        "end_time_grid_en = time.time()\n",
        "\n",
        "print(\"\\nResultados de GridSearchCV para ElasticNet:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {grid_search_en.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-grid_search_en.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de GridSearchCV: {end_time_grid_en - start_time_grid_en:.2f} segundos\")\n",
        "\n",
        "best_elastic_net_grid = grid_search_en.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de GridSearchCV para ElasticNet con validación cruzada completa:\")\n",
        "cv_results_grid_best_en = cross_validate(best_elastic_net_grid, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_grid_en = -cv_results_grid_best_en['test_mae']\n",
        "cv_mse_grid_en = -cv_results_grid_best_en['test_mse']\n",
        "cv_r2_grid_en = cv_results_grid_best_en['test_r2']\n",
        "cv_mape_grid_en = -cv_results_grid_best_en['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_grid_en.mean():.4f} +/- {cv_mae_grid_en.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_grid_en.mean():.4f} +/- {cv_mse_grid_en.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_grid_en.mean():.4f} +/- {cv_r2_grid_en.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_grid_en.mean():.4f}% +/- {cv_mape_grid_en.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Optimization with RandomizedSearchCV ---\n",
        "print(\"\\nIniciando optimización con RandomizedSearchCV para ElasticNet...\")\n",
        "\n",
        "# Define parameter distribution for RandomizedSearchCV\n",
        "# alpha loguniform(0.001, 1), l1_ratio uniform(0, 1)\n",
        "param_dist_en = {\n",
        "    'alpha': loguniform(0.001, 1),\n",
        "    'l1_ratio': uniform(0, 1)  # Use scipy.stats.uniform for RandomizedSearchCV\n",
        "}\n",
        "\n",
        "# Number of iterations (adjust as needed)\n",
        "n_iter_rand_en = 50\n",
        "\n",
        "random_search_en = RandomizedSearchCV(estimator=elastic_net, param_distributions=param_dist_en,\n",
        "                                      n_iter=n_iter_rand_en,\n",
        "                                      scoring=scoring_optimizer, refit='mae', # Optimize using MAE\n",
        "                                      cv=kf, verbose=1, random_state=42, n_jobs=-1) # random_state para reproducibilidad\n",
        "\n",
        "start_time_rand_en = time.time()\n",
        "random_search_en.fit(X_train, y_train)\n",
        "end_time_rand_en = time.time()\n",
        "\n",
        "print(\"\\nResultados de RandomizedSearchCV para ElasticNet:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {random_search_en.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-random_search_en.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de RandomizedSearchCV: {end_time_rand_en - start_time_rand_en:.2f} segundos\")\n",
        "\n",
        "best_elastic_net_rand = random_search_en.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de RandomizedSearchCV para ElasticNet con validación cruzada completa:\")\n",
        "cv_results_rand_best_en = cross_validate(best_elastic_net_rand, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_rand_en = -cv_results_rand_best_en['test_mae']\n",
        "cv_mse_rand_en = -cv_results_rand_best_en['test_mse']\n",
        "cv_r2_rand_en = cv_results_rand_best_en['test_r2']\n",
        "cv_mape_rand_en = -cv_results_rand_best_en['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_rand_en.mean():.4f} +/- {cv_mae_rand_en.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_rand_en.mean():.4f} +/- {cv_mse_rand_en.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_rand_en.mean():.4f} +/- {cv_r2_rand_en.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_rand_en.mean():.4f}% +/- {cv_mape_rand_en.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Optimization with BayesSearchCV ---\n",
        "print(\"\\nIniciando optimización con BayesSearchCV para ElasticNet...\")\n",
        "\n",
        "# Define search spaces for BayesSearchCV\n",
        "# alpha loguniform(0.001, 1), l1_ratio uniform(0, 1)\n",
        "search_spaces_en = {\n",
        "    'alpha': Real(0.001, 1, prior='log-uniform'),\n",
        "    'l1_ratio': Real(0, 1, prior='uniform') # Use skopt's Real for BayesSearchCV\n",
        "}\n",
        "\n",
        "# Number of iterations (adjust as needed)\n",
        "n_iter_bayes_en = 50\n",
        "\n",
        "bayes_search_en = BayesSearchCV(estimator=elastic_net, search_spaces=search_spaces_en,\n",
        "                                n_iter=n_iter_bayes_en,\n",
        "                                scoring=scoring_optimizer, refit='mae', # Optimize using MAE\n",
        "                                cv=kf, verbose=1, random_state=42, n_jobs=-1)\n",
        "\n",
        "start_time_bayes_en = time.time()\n",
        "bayes_search_en.fit(X_train, y_train)\n",
        "end_time_bayes_en = time.time()\n",
        "\n",
        "print(\"\\nResultados de BayesSearchCV para ElasticNet:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {bayes_search_en.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-bayes_search_en.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de BayesSearchCV: {end_time_bayes_en - start_time_bayes_en:.2f} segundos\")\n",
        "\n",
        "best_elastic_net_bayes = bayes_search_en.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de BayesSearchCV para ElasticNet con validación cruzada completa:\")\n",
        "cv_results_bayes_best_en = cross_validate(best_elastic_net_bayes, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_bayes_en = -cv_results_bayes_best_en['test_mae']\n",
        "cv_mse_bayes_en = -cv_results_bayes_best_en['test_mse']\n",
        "cv_r2_bayes_en = cv_results_bayes_best_en['test_r2']\n",
        "cv_mape_bayes_en = -cv_results_bayes_best_en['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_bayes_en.mean():.4f} +/- {cv_mae_bayes_en.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_bayes_en.mean():.4f} +/- {cv_mse_bayes_en.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_bayes_en.mean():.4f} +/- {cv_r2_bayes_en.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_bayes_en.mean():.4f}% +/- {cv_mape_bayes_en.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Comparación de resultados de ElasticNet ---\n",
        "print(\"\\n--- Resumen de Resultados de Optimización para ElasticNet (MAE promedio en validación) ---\")\n",
        "print(f\"GridSearchCV:      {-grid_search_en.best_score_:.4f}\")\n",
        "print(f\"RandomizedSearchCV: {-random_search_en.best_score_:.4f}\")\n",
        "print(f\"BayesSearchCV:      {-bayes_search_en.best_score_:.4f}\")\n",
        "\n",
        "print(\"\\n--- Reporte Completo de Métricas para ElasticNet (Promedio de 5-fold CV) ---\")\n",
        "\n",
        "print(\"\\nGridSearchCV Mejor Modelo (ElasticNet):\")\n",
        "print(f\"  MAE:  {cv_mae_grid_en.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_grid_en.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_grid_en.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_grid_en.mean():.4f}%\")\n",
        "\n",
        "print(\"\\nRandomizedSearchCV Mejor Modelo (ElasticNet):\")\n",
        "print(f\"  MAE:  {cv_mae_rand_en.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_rand_en.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_rand_en.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_rand_en.mean():.4f}%\")\n",
        "\n",
        "print(\"\\nBayesSearchCV Mejor Modelo (ElasticNet):\")\n",
        "print(f\"  MAE:  {cv_mae_bayes_en.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_bayes_en.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_bayes_en.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_bayes_en.mean():.4f}%\")\n",
        "\n",
        "# You can select the overall best ElasticNet model based on validation MAE\n",
        "# For instance, if BayesSearchCV gave the best result:\n",
        "# final_best_elastic_net_model = best_elastic_net_bayes\n",
        "# print(\"\\nEl mejor modelo ElasticNet general basado en MAE promedio de validación es el de BayesSearchCV.\")\n",
        "\n",
        "# Optional: Train the chosen best ElasticNet model on the entire X_train and evaluate on X_test\n",
        "# final_best_elastic_net_model = best_elastic_net_bayes # Or grid_search_en.best_estimator_ or random_search_en.best_estimator_\n",
        "\n",
        "# print(\"\\nEntrenando el modelo ElasticNet final seleccionado en el conjunto de entrenamiento completo...\")\n",
        "# final_best_elastic_net_model.fit(X_train, y_train)\n",
        "# print(\"Evaluando en el conjunto de prueba (X_test) con el modelo ElasticNet final...\")\n",
        "# y_pred_test_final_en = final_best_elastic_net_model.predict(X_test)\n",
        "\n",
        "# # Calculate metrics on the test set with the final ElasticNet model\n",
        "# test_mae_final_en = mean_absolute_error(y_test, y_pred_test_final_en)\n",
        "# test_mse_final_en = mean_squared_error(y_test, y_pred_test_final_en)\n",
        "# test_r2_final_en = r2_score(y_test, y_pred_test_final_en)\n",
        "# test_mape_final_en = mean_absolute_percentage_error(y_test, y_pred_test_final_en)\n",
        "\n",
        "# print(\"\\nMétricas en el conjunto de prueba con el modelo ElasticNet final seleccionado:\")\n",
        "# print(f\"  MAE:  {test_mae_final_en:.4f}\")\n",
        "# print(f\"  MSE:  {test_mse_final_en:.4f}\")\n",
        "# print(f\"  R2:   {test_r2_final_en:.4f}\")\n",
        "# print(f\"  MAPE: {test_mape_final_en:.4f}%\")"
      ],
      "metadata": {
        "id": "b-9lMkjMhoEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KernelRidge"
      ],
      "metadata": {
        "id": "HJWqwv_XiQ0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Implementa KernelRidge en Python con sklearn, optimiza alpha y gamma para kernel RBF usando GridSearchCV, RandomizedSearchCV y BayesSearchCV de skopt, y reporta MAE, MSE, R2 y MAPE con validación cruzada de 5 folds.\"\n",
        "# Rango: alpha = [0.001, 0.01, 0.1], gamma = [0.001, 0.01] (Grid); alpha loguniform(0.001, 1), gamma loguniform(0.001, 1) (Random/Bayesian).\n",
        "# Justificación: Alpha controla regularización, gamma ajusta escala del kernel RBF; rangos logarítmicos capturan no linealidades.\n",
        "\n",
        "!pip install scikit-optimize\n",
        "\n",
        "# Ensure scorers and kf are defined (as per the original code context)\n",
        "# (assuming the previous cells defining these have been run)\n",
        "\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "\n",
        "\n",
        "# Definir el modelo KernelRidge con kernel RBF\n",
        "# Aumentar max_iter si es necesario para la convergencia (aunque KRR no tiene un solver iterativo como las lineales)\n",
        "# kernel='rbf' especifica el tipo de kernel\n",
        "kernel_ridge = KernelRidge(kernel='rbf')\n",
        "\n",
        "\n",
        "# --- Optimization with GridSearchCV ---\n",
        "print(\"\\nIniciando optimización con GridSearchCV para KernelRidge (kernel RBF)...\")\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "# alpha = [0.001, 0.01, 0.1], gamma = [0.001, 0.01]\n",
        "param_grid_krr = {\n",
        "    'alpha': [0.001, 0.01, 0.1],\n",
        "    'gamma': [0.001, 0.01]\n",
        "}\n",
        "\n",
        "grid_search_krr = GridSearchCV(estimator=kernel_ridge, param_grid=param_grid_krr,\n",
        "                               scoring=scoring_optimizer, refit='mae', # Optimize using MAE\n",
        "                               cv=kf, verbose=1, n_jobs=-1)\n",
        "\n",
        "start_time_grid_krr = time.time()\n",
        "grid_search_krr.fit(X_train, y_train)\n",
        "end_time_grid_krr = time.time()\n",
        "\n",
        "print(\"\\nResultados de GridSearchCV para KernelRidge:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {grid_search_krr.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-grid_search_krr.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de GridSearchCV: {end_time_grid_krr - start_time_grid_krr:.2f} segundos\")\n",
        "\n",
        "best_kernel_ridge_grid = grid_search_krr.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de GridSearchCV para KernelRidge con validación cruzada completa:\")\n",
        "cv_results_grid_best_krr = cross_validate(best_kernel_ridge_grid, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_grid_krr = -cv_results_grid_best_krr['test_mae']\n",
        "cv_mse_grid_krr = -cv_results_grid_best_krr['test_mse']\n",
        "cv_r2_grid_krr = cv_results_grid_best_krr['test_r2']\n",
        "cv_mape_grid_krr = -cv_results_grid_best_krr['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_grid_krr.mean():.4f} +/- {cv_mae_grid_krr.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_grid_krr.mean():.4f} +/- {cv_mse_grid_krr.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_grid_krr.mean():.4f} +/- {cv_r2_grid_krr.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_grid_krr.mean():.4f}% +/- {cv_mape_grid_krr.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Optimization with RandomizedSearchCV ---\n",
        "print(\"\\nIniciando optimización con RandomizedSearchCV para KernelRidge (kernel RBF)...\")\n",
        "\n",
        "# Define parameter distribution for RandomizedSearchCV\n",
        "# alpha loguniform(0.001, 1), gamma loguniform(0.001, 1)\n",
        "param_dist_krr = {\n",
        "    'alpha': loguniform(0.001, 1),\n",
        "    'gamma': loguniform(0.001, 1)\n",
        "}\n",
        "\n",
        "# Number of iterations (adjust as needed)\n",
        "n_iter_rand_krr = 50 # Example: try 50 random combinations\n",
        "\n",
        "random_search_krr = RandomizedSearchCV(estimator=kernel_ridge, param_distributions=param_dist_krr,\n",
        "                                       n_iter=n_iter_rand_krr,\n",
        "                                       scoring=scoring_optimizer, refit='mae', # Optimize using MAE\n",
        "                                       cv=kf, verbose=1, random_state=42, n_jobs=-1) # random_state para reproducibilidad\n",
        "\n",
        "start_time_rand_krr = time.time()\n",
        "random_search_krr.fit(X_train, y_train)\n",
        "end_time_rand_krr = time.time()\n",
        "\n",
        "print(\"\\nResultados de RandomizedSearchCV para KernelRidge:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {random_search_krr.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-random_search_krr.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de RandomizedSearchCV: {end_time_rand_krr - start_time_rand_krr:.2f} segundos\")\n",
        "\n",
        "best_kernel_ridge_rand = random_search_krr.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de RandomizedSearchCV para KernelRidge con validación cruzada completa:\")\n",
        "cv_results_rand_best_krr = cross_validate(best_kernel_ridge_rand, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_rand_krr = -cv_results_rand_best_krr['test_mae']\n",
        "cv_mse_rand_krr = -cv_results_rand_best_krr['test_mse']\n",
        "cv_r2_rand_krr = cv_results_rand_best_krr['test_r2']\n",
        "cv_mape_rand_krr = -cv_results_rand_best_krr['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_rand_krr.mean():.4f} +/- {cv_mae_rand_krr.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_rand_krr.mean():.4f} +/- {cv_mse_rand_krr.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_rand_krr.mean():.4f} +/- {cv_r2_rand_krr.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_rand_krr.mean():.4f}% +/- {cv_mape_rand_krr.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Optimization with BayesSearchCV ---\n",
        "print(\"\\nIniciando optimización con BayesSearchCV para KernelRidge (kernel RBF)...\")\n",
        "\n",
        "# Define search spaces for BayesSearchCV\n",
        "# alpha loguniform(0.001, 1), gamma loguniform(0.001, 1)\n",
        "search_spaces_krr = {\n",
        "    'alpha': Real(0.001, 1, prior='log-uniform'),\n",
        "    'gamma': Real(0.001, 1, prior='log-uniform')\n",
        "}\n",
        "\n",
        "# Number of iterations (adjust as needed)\n",
        "n_iter_bayes_krr = 50 # Example: try 50 iterations\n",
        "\n",
        "bayes_search_krr = BayesSearchCV(estimator=kernel_ridge, search_spaces=search_spaces_krr,\n",
        "                                 n_iter=n_iter_bayes_krr,\n",
        "                                 scoring=scoring_optimizer, refit='mae', # Optimize using MAE\n",
        "                                 cv=kf, verbose=1, random_state=42, n_jobs=-1)\n",
        "\n",
        "start_time_bayes_krr = time.time()\n",
        "bayes_search_krr.fit(X_train, y_train)\n",
        "end_time_bayes_krr = time.time()\n",
        "\n",
        "print(\"\\nResultados de BayesSearchCV para KernelRidge:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {bayes_search_krr.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-bayes_search_krr.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de BayesSearchCV: {end_time_bayes_krr - start_time_bayes_krr:.2f} segundos\")\n",
        "\n",
        "best_kernel_ridge_bayes = bayes_search_krr.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de BayesSearchCV para KernelRidge con validación cruzada completa:\")\n",
        "cv_results_bayes_best_krr = cross_validate(best_kernel_ridge_bayes, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_bayes_krr = -cv_results_bayes_best_krr['test_mae']\n",
        "cv_mse_bayes_krr = -cv_results_bayes_best_krr['test_mse']\n",
        "cv_r2_bayes_krr = cv_results_bayes_best_krr['test_r2']\n",
        "cv_mape_bayes_krr = -cv_results_bayes_best_krr['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_bayes_krr.mean():.4f} +/- {cv_mae_bayes_krr.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_bayes_krr.mean():.4f} +/- {cv_mse_bayes_krr.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_bayes_krr.mean():.4f} +/- {cv_r2_bayes_krr.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_bayes_krr.mean():.4f}% +/- {cv_mape_bayes_krr.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Comparación de resultados de KernelRidge ---\n",
        "print(\"\\n--- Resumen de Resultados de Optimización para KernelRidge (MAE promedio en validación) ---\")\n",
        "print(f\"GridSearchCV:      {-grid_search_krr.best_score_:.4f}\")\n",
        "print(f\"RandomizedSearchCV: {-random_search_krr.best_score_:.4f}\")\n",
        "print(f\"BayesSearchCV:      {-bayes_search_krr.best_score_:.4f}\")\n",
        "\n",
        "print(\"\\n--- Reporte Completo de Métricas para KernelRidge (Promedio de 5-fold CV) ---\")\n",
        "\n",
        "print(\"\\nGridSearchCV Mejor Modelo (KernelRidge):\")\n",
        "print(f\"  MAE:  {cv_mae_grid_krr.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_grid_krr.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_grid_krr.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_grid_krr.mean():.4f}%\")\n",
        "\n",
        "print(\"\\nRandomizedSearchCV Mejor Modelo (KernelRidge):\")\n",
        "print(f\"  MAE:  {cv_mae_rand_krr.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_rand_krr.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_rand_krr.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_rand_krr.mean():.4f}%\")\n",
        "\n",
        "print(\"\\nBayesSearchCV Mejor Modelo (KernelRidge):\")\n",
        "print(f\"  MAE:  {cv_mae_bayes_krr.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_bayes_krr.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_bayes_krr.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_bayes_krr.mean():.4f}%\")\n",
        "\n",
        "# You can select the overall best KernelRidge model based on validation MAE\n",
        "# For instance, if BayesSearchCV gave the best result:\n",
        "# final_best_kernel_ridge_model = best_kernel_ridge_bayes\n",
        "# print(\"\\nEl mejor modelo KernelRidge general basado en MAE promedio de validación es el de BayesSearchCV.\")\n",
        "\n",
        "# Optional: Train the chosen best KernelRidge model on the entire X_train and evaluate on X_test\n",
        "# final_best_kernel_ridge_model = best_kernel_ridge_bayes # Or grid_search_krr.best_estimator_ or random_search_krr.best_estimator_\n",
        "\n",
        "# print(\"\\nEntrenando el modelo KernelRidge final seleccionado en el conjunto de entrenamiento completo...\")\n",
        "# final_best_kernel_ridge_model.fit(X_train, y_train)\n",
        "# print(\"Evaluando en el conjunto de prueba (X_test) con el modelo KernelRidge final...\")\n",
        "# y_pred_test_final_krr = final_best_kernel_ridge_model.predict(X_test)\n",
        "\n",
        "# # Calculate metrics on the test set with the final KernelRidge model\n",
        "# test_mae_final_krr = mean_absolute_error(y_test, y_pred_test_final_krr)\n",
        "# test_mse_final_krr = mean_squared_error(y_test, y_pred_test_final_krr)\n",
        "# test_r2_final_krr = r2_score(y_test, y_pred_test_final_krr)\n",
        "# test_mape_final_krr = mean_absolute_percentage_error(y_test, y_pred_test_final_krr)\n",
        "\n",
        "# print(\"\\nMétricas en el conjunto de prueba con el modelo KernelRidge final seleccionado:\")\n",
        "# print(f\"  MAE:  {test_mae_final_krr:.4f}\")\n",
        "# print(f\"  MSE:  {test_mse_final_krr:.4f}\")\n",
        "# print(f\"  R2:   {test_r2_final_krr:.4f}\")\n",
        "# print(f\"  MAPE: {test_mape_final_krr:.4f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "dZyvVT5xiX5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SGDRegressor"
      ],
      "metadata": {
        "id": "uTj3I8f6jOGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Implementa SGDRegressor en Python con sklearn, optimiza alpha con learning_rate='invscaling' usando GridSearchCV, RandomizedSearchCV y BayesSearchCV de skopt, y reporta MAE, MSE, R2 y MAPE con validación cruzada de 5 folds.\"\n",
        "# Rango: alpha = [0.0001, 0.001] (Grid); alpha loguniform(0.0001, 0.01) (Random/Bayesian).\n",
        "# Justificación: Alpha regula regularización, learning_rate fijo para estabilidad; rangos pequeños para evitar overfitting.\n",
        "\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "# Ensure scorers and kf are defined (as per the original code context)\n",
        "# (assuming the previous cells defining these have been run)\n",
        "\n",
        "# Define the SGDRegressor model\n",
        "# Use learning_rate='invscaling' and set early_stopping=True for better performance and stability\n",
        "sgd_regressor = SGDRegressor(learning_rate='invscaling', early_stopping=True,\n",
        "                             random_state=42, max_iter=10000) # Increased max_iter\n",
        "\n",
        "# --- Optimization with GridSearchCV ---\n",
        "print(\"\\nIniciando optimización con GridSearchCV para SGDRegressor...\")\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "# Range: alpha = [0.0001, 0.001]\n",
        "param_grid_sgd = {\n",
        "    'alpha': [0.0001, 0.001]\n",
        "}\n",
        "\n",
        "grid_search_sgd = GridSearchCV(estimator=sgd_regressor, param_grid=param_grid_sgd,\n",
        "                               scoring=scoring_optimizer, refit='mae', # Optimize using MAE\n",
        "                               cv=kf, verbose=1, n_jobs=-1)\n",
        "\n",
        "start_time_grid_sgd = time.time()\n",
        "grid_search_sgd.fit(X_train, y_train)\n",
        "end_time_grid_sgd = time.time()\n",
        "\n",
        "print(\"\\nResultados de GridSearchCV para SGDRegressor:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {grid_search_sgd.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-grid_search_sgd.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de GridSearchCV: {end_time_grid_sgd - start_time_grid_sgd:.2f} segundos\")\n",
        "\n",
        "best_sgd_grid = grid_search_sgd.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de GridSearchCV para SGDRegressor con validación cruzada completa:\")\n",
        "cv_results_grid_best_sgd = cross_validate(best_sgd_grid, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_grid_sgd = -cv_results_grid_best_sgd['test_mae']\n",
        "cv_mse_grid_sgd = -cv_results_grid_best_sgd['test_mse']\n",
        "cv_r2_grid_sgd = cv_results_grid_best_sgd['test_r2']\n",
        "cv_mape_grid_sgd = -cv_results_grid_best_sgd['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_grid_sgd.mean():.4f} +/- {cv_mae_grid_sgd.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_grid_sgd.mean():.4f} +/- {cv_mse_grid_sgd.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_grid_sgd.mean():.4f} +/- {cv_r2_grid_sgd.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_grid_sgd.mean():.4f}% +/- {cv_mape_grid_sgd.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Optimization with RandomizedSearchCV ---\n",
        "print(\"\\nIniciando optimización con RandomizedSearchCV para SGDRegressor...\")\n",
        "\n",
        "# Define parameter distribution for RandomizedSearchCV\n",
        "# Range: alpha loguniform(0.0001, 0.01)\n",
        "param_dist_sgd = {\n",
        "    'alpha': loguniform(0.0001, 0.01)\n",
        "}\n",
        "\n",
        "# Number of iterations (adjust as needed)\n",
        "n_iter_rand_sgd = 50 # Example: try 50 random combinations\n",
        "\n",
        "random_search_sgd = RandomizedSearchCV(estimator=sgd_regressor, param_distributions=param_dist_sgd,\n",
        "                                       n_iter=n_iter_rand_sgd,\n",
        "                                       scoring=scoring_optimizer, refit='mae', # Optimize using MAE\n",
        "                                       cv=kf, verbose=1, random_state=42, n_jobs=-1) # random_state para reproducibilidad\n",
        "\n",
        "start_time_rand_sgd = time.time()\n",
        "random_search_sgd.fit(X_train, y_train)\n",
        "end_time_rand_sgd = time.time()\n",
        "\n",
        "print(\"\\nResultados de RandomizedSearchCV para SGDRegressor:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {random_search_sgd.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-random_search_sgd.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de RandomizedSearchCV: {end_time_rand_sgd - start_time_rand_sgd:.2f} segundos\")\n",
        "\n",
        "best_sgd_rand = random_search_sgd.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de RandomizedSearchCV para SGDRegressor con validación cruzada completa:\")\n",
        "cv_results_rand_best_sgd = cross_validate(best_sgd_rand, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_rand_sgd = -cv_results_rand_best_sgd['test_mae']\n",
        "cv_mse_rand_sgd = -cv_results_rand_best_sgd['test_mse']\n",
        "cv_r2_rand_sgd = cv_results_rand_best_sgd['test_r2']\n",
        "cv_mape_rand_sgd = -cv_results_rand_best_sgd['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_rand_sgd.mean():.4f} +/- {cv_mae_rand_sgd.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_rand_sgd.mean():.4f} +/- {cv_mse_rand_sgd.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_rand_sgd.mean():.4f} +/- {cv_r2_rand_sgd.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_rand_sgd.mean():.4f}% +/- {cv_mape_rand_sgd.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Optimization with BayesSearchCV ---\n",
        "print(\"\\nIniciando optimización con BayesSearchCV para SGDRegressor...\")\n",
        "\n",
        "# Define search spaces for BayesSearchCV\n",
        "# Range: alpha loguniform(0.0001, 0.01)\n",
        "search_spaces_sgd = {\n",
        "    'alpha': Real(0.0001, 0.01, prior='log-uniform')\n",
        "}\n",
        "\n",
        "# Number of iterations (adjust as needed)\n",
        "n_iter_bayes_sgd = 50 # Example: try 50 iterations\n",
        "\n",
        "bayes_search_sgd = BayesSearchCV(estimator=sgd_regressor, search_spaces=search_spaces_sgd,\n",
        "                                 n_iter=n_iter_bayes_sgd,\n",
        "                                 scoring=scoring_optimizer, refit='mae', # Optimize using MAE\n",
        "                                 cv=kf, verbose=1, random_state=42, n_jobs=-1)\n",
        "\n",
        "start_time_bayes_sgd = time.time()\n",
        "bayes_search_sgd.fit(X_train, y_train)\n",
        "end_time_bayes_sgd = time.time()\n",
        "\n",
        "print(\"\\nResultados de BayesSearchCV para SGDRegressor:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {bayes_search_sgd.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-bayes_search_sgd.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de BayesSearchCV: {end_time_bayes_sgd - start_time_bayes_sgd:.2f} segundos\")\n",
        "\n",
        "best_sgd_bayes = bayes_search_sgd.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de BayesSearchCV para SGDRegressor con validación cruzada completa:\")\n",
        "cv_results_bayes_best_sgd = cross_validate(best_sgd_bayes, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_bayes_sgd = -cv_results_bayes_best_sgd['test_mae']\n",
        "cv_mse_bayes_sgd = -cv_results_bayes_best_sgd['test_mse']\n",
        "cv_r2_bayes_sgd = cv_results_bayes_best_sgd['test_r2']\n",
        "cv_mape_bayes_sgd = -cv_results_bayes_best_sgd['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_bayes_sgd.mean():.4f} +/- {cv_mae_bayes_sgd.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_bayes_sgd.mean():.4f} +/- {cv_mse_bayes_sgd.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_bayes_sgd.mean():.4f} +/- {cv_r2_bayes_sgd.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_bayes_sgd.mean():.4f}% +/- {cv_mape_bayes_sgd.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Comparación de resultados de SGDRegressor ---\n",
        "print(\"\\n--- Resumen de Resultados de Optimización para SGDRegressor (MAE promedio en validación) ---\")\n",
        "print(f\"GridSearchCV:      {-grid_search_sgd.best_score_:.4f}\")\n",
        "print(f\"RandomizedSearchCV: {-random_search_sgd.best_score_:.4f}\")\n",
        "print(f\"BayesSearchCV:      {-bayes_search_sgd.best_score_:.4f}\")\n",
        "\n",
        "print(\"\\n--- Reporte Completo de Métricas para SGDRegressor (Promedio de 5-fold CV) ---\")\n",
        "\n",
        "print(\"\\nGridSearchCV Mejor Modelo (SGDRegressor):\")\n",
        "print(f\"  MAE:  {cv_mae_grid_sgd.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_grid_sgd.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_grid_sgd.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_grid_sgd.mean():.4f}%\")\n",
        "\n",
        "print(\"\\nRandomizedSearchCV Mejor Modelo (SGDRegressor):\")\n",
        "print(f\"  MAE:  {cv_mae_rand_sgd.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_rand_sgd.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_rand_sgd.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_rand_sgd.mean():.4f}%\")\n",
        "\n",
        "print(\"\\nBayesSearchCV Mejor Modelo (SGDRegressor):\")\n",
        "print(f\"  MAE:  {cv_mae_bayes_sgd.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_bayes_sgd.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_bayes_sgd.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_bayes_sgd.mean():.4f}%\")\n",
        "\n",
        "# You can select the overall best SGDRegressor model based on validation MAE\n",
        "# For instance, if BayesSearchCV gave the best result:\n",
        "# final_best_sgd_model = best_sgd_bayes\n",
        "# print(\"\\nEl mejor modelo SGDRegressor general basado en MAE promedio de validación es el de BayesSearchCV.\")\n",
        "\n",
        "# Optional: Train the chosen best SGDRegressor model on the entire X_train and evaluate on X_test\n",
        "# final_best_sgd_model = best_sgd_bayes # Or grid_search_sgd.best_estimator_ or random_search_sgd.best_estimator_\n",
        "\n",
        "# print(\"\\nEntrenando el modelo SGDRegressor final seleccionado en el conjunto de entrenamiento completo...\")\n",
        "# final_best_sgd_model.fit(X_train, y_train)\n",
        "# print(\"Evaluando en el conjunto de prueba (X_test) con el modelo SGDRegressor final...\")\n",
        "# y_pred_test_final_sgd = final_best_sgd_model.predict(X_test)\n",
        "\n",
        "# # Calculate metrics on the test set with the final SGDRegressor model\n",
        "# test_mae_final_sgd = mean_absolute_error(y_test, y_pred_test_final_sgd)\n",
        "# test_mse_final_sgd = mean_squared_error(y_test, y_pred_test_final_sgd)\n",
        "# test_r2_final_sgd = r2_score(y_test, y_pred_test_final_sgd)\n",
        "# test_mape_final_sgd = mean_absolute_percentage_error(y_test, y_pred_test_final_sgd)\n",
        "\n",
        "# print(\"\\nMétricas en el conjunto de prueba con el modelo SGDRegressor final seleccionado:\")\n",
        "# print(f\"  MAE:  {test_mae_final_sgd:.4f}\")\n",
        "# print(f\"  MSE:  {test_mse_final_sgd:.4f}\")\n",
        "# print(f\"  R2:   {test_r2_final_sgd:.4f}\")\n",
        "# print(f\"  MAPE: {test_mape_final_sgd:.4f}%\")\n"
      ],
      "metadata": {
        "id": "-OFHH83SjW3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BayesianRidge"
      ],
      "metadata": {
        "id": "nMwIZ9S_jxa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Implementa BayesianRidge en Python con sklearn, usa validación cruzada de 5 folds, y reporta MAE, MSE, R2 y MAPE, sin optimización de hiperparámetros.\"\n",
        "# Rango: n_iter = [300] (Grid); no aplica para Random/Bayesian.\n",
        "# Justificación: Robustez a multicolinealidad, parámetros predeterminados son suficientes.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "\n",
        "# Ensure scorers and kf are defined (as per the original code context)\n",
        "# (assuming the previous cells defining these have been run)\n",
        "\n",
        "# Redefine the custom MAPE scorer if it wasn't in the preceding code or is needed again\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    \"\"\"Calcula el Error Porcentual Absoluto Medio (MAPE).\"\"\"\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    epsilon = 1e-8\n",
        "    return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
        "\n",
        "# Create scorer objects for the metrics\n",
        "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
        "r2_scorer = make_scorer(r2_score, greater_is_better=True)\n",
        "mape_scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
        "\n",
        "# Define the scoring dictionary for cross_validate\n",
        "scoring = {\n",
        "    'mae': mae_scorer,\n",
        "    'mse': mse_scorer,\n",
        "    'r2': r2_scorer,\n",
        "    'mape': mape_scorer\n",
        "}\n",
        "\n",
        "# Define the BayesianRidge model with default parameters\n",
        "# n_iter=300 as specified, but the prompt says \"sin optimización de hiperparámetros\",\n",
        "# and n_iter doesn't apply to default sklearn BayesianRidge without tuning.\n",
        "# Let's use the default parameters as requested by \"sin optimización de hiperparámetros\".\n",
        "# If n_iter was a requirement, we'd clarify or set it, but the prompt suggests default.\n",
        "# The default max_iter for BayesianRidge is 300, so it aligns if we were to set it,\n",
        "# but sticking to \"sin optimización\" implies using the model as is.\n",
        "model = BayesianRidge()\n",
        "\n",
        "# Perform cross-validation with 5 folds (kf)\n",
        "print(\"\\nRealizando validación cruzada con BayesianRidge (parámetros predeterminados)...\")\n",
        "\n",
        "# Use cross_validate to get multiple metrics simultaneously\n",
        "cv_results = cross_validate(model, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "# Access the results arrays\n",
        "# Error metrics (MAE, MSE, MAPE) are negative because make_scorer optimizes by maximizing.\n",
        "# Take the absolute value for reporting.\n",
        "cv_mae = -cv_results['test_mae']\n",
        "cv_mse = -cv_results['test_mse']\n",
        "cv_r2 = cv_results['test_r2']\n",
        "cv_mape = -cv_results['test_mape']\n",
        "\n",
        "# Report the average and standard deviation of each metric\n",
        "print(f\"\\nResultados de Validación Cruzada para BayesianRidge (promedio +/- std dev):\")\n",
        "print(f\"  MAE:  {cv_mae.mean():.4f} +/- {cv_mae.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse.mean():.4f} +/- {cv_mse.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2.mean():.4f} +/- {cv_r2.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape.mean():.4f}% +/- {cv_mape.std():.4f}%\")\n",
        "\n",
        "# Optional: Train the model on the entire training set and evaluate on the test set\n",
        "# This step is typically performed after model selection and hyperparameter tuning,\n",
        "# but can be done here to show performance on unseen data with default parameters.\n",
        "# print(\"\\nEntrenando modelo final BayesianRidge en el conjunto de entrenamiento completo...\")\n",
        "# model.fit(X_train, y_train)\n",
        "# print(\"Evaluando en el conjunto de prueba...\")\n",
        "# y_pred_test = model.predict(X_test)\n",
        "\n",
        "# # Calculate metrics on the test set\n",
        "# test_mae = mean_absolute_error(y_test, y_pred_test)\n",
        "# test_mse = mean_squared_error(y_test, y_pred_test)\n",
        "# test_r2 = r2_score(y_test, y_pred_test)\n",
        "# test_mape = mean_absolute_percentage_error(y_test, y_pred_test)\n",
        "\n",
        "# print(\"\\nMétricas en el conjunto de prueba (BayesianRidge con parámetros predeterminados):\")\n",
        "# print(f\"  MAE:  {test_mae:.4f}\")\n",
        "# print(f\"  MSE:  {test_mse:.4f}\")\n",
        "# print(f\"  R2:   {test_r2:.4f}\")\n",
        "# print(f\"  MAPE: {test_mape:.4f}%\")"
      ],
      "metadata": {
        "id": "KmjvCGJ3j4Ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GaussianProcessRegressor"
      ],
      "metadata": {
        "id": "TNsIVmcKkF8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Implementa GaussianProcessRegressor en Python con sklearn, optimiza length_scale y alpha para kernel RBF usando GridSearchCV, RandomizedSearchCV y BayesSearchCV de skopt, y reporta MAE, MSE, R2 y MAPE con validación cruzada de 5 folds.\"\n",
        "# # # Rango: length_scale = [0.1, 1], alpha = [0.001] (Grid); length_scale loguniform(0.1, 2), alpha loguniform(0.001, 0.1) (Random/Bayesian).\n",
        "# # # Justificación: Length_scale ajusta correlación, alpha controla ruido; rangos logarítmicos para capturar variabilidad, alpha fijo pequeño para estabilidad.\n",
        "# # Organízame el código para que al momento realizar la optimización de los hiperparámetros, no use tantos fits y no se demore\n",
        "\n",
        "# Instalar scikit-optimize si no está instalado\n",
        "!pip install scikit-optimize\n",
        "\n",
        "import time\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "from sklearn.model_selection import cross_validate, KFold\n",
        "from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real\n",
        "from scipy.stats import loguniform\n",
        "\n",
        "# Suponiendo que X_train, y_train, X_test, y_test están definidos\n",
        "# Definir KFold para validación cruzada\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Definir métricas para evaluación\n",
        "scoring_optimizer = {'mae': make_scorer(mean_absolute_error, greater_is_better=False)}\n",
        "scoring = {\n",
        "    'mae': make_scorer(mean_absolute_error, greater_is_better=False),\n",
        "    'mse': make_scorer(mean_squared_error, greater_is_better=False),\n",
        "    'r2': make_scorer(r2_score, greater_is_better=True),\n",
        "    'mape': make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
        "}\n",
        "\n",
        "# Definir el modelo con menos reinicios\n",
        "kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (0.1, 2.0))\n",
        "gpr = GaussianProcessRegressor(kernel=kernel, random_state=42, n_restarts_optimizer=5)\n",
        "\n",
        "# --- Optimización con GridSearchCV ---\n",
        "print(\"\\nIniciando optimización con GridSearchCV...\")\n",
        "param_grid_gpr = {\n",
        "    'kernel__k2__length_scale': [0.1, 1.0],\n",
        "    'alpha': [0.001]\n",
        "}\n",
        "grid_search_gpr = GridSearchCV(estimator=gpr, param_grid=param_grid_gpr,\n",
        "                               scoring=scoring_optimizer, refit='mae',\n",
        "                               cv=kf, verbose=1, n_jobs=-1)\n",
        "start_time_grid_gpr = time.time()\n",
        "grid_search_gpr.fit(X_train, y_train)\n",
        "end_time_grid_gpr = time.time()\n",
        "\n",
        "print(\"\\nResultados de GridSearchCV:\")\n",
        "print(f\"Mejores hiperparámetros: {grid_search_gpr.best_params_}\")\n",
        "print(f\"Mejor MAE promedio: {-grid_search_gpr.best_score_:.4f}\")\n",
        "print(f\"Tiempo: {end_time_grid_gpr - start_time_grid_gpr:.2f} segundos\")\n",
        "\n",
        "# --- Optimización con RandomizedSearchCV ---\n",
        "print(\"\\nIniciando optimización con RandomizedSearchCV...\")\n",
        "param_dist_gpr = {\n",
        "    'kernel__k2__length_scale': loguniform(0.1, 2.0),\n",
        "    'alpha': loguniform(0.001, 0.1)\n",
        "}\n",
        "random_search_gpr = RandomizedSearchCV(estimator=gpr, param_distributions=param_dist_gpr,\n",
        "                                      n_iter=10, scoring=scoring_optimizer, refit='mae',\n",
        "                                      cv=kf, verbose=1, random_state=42, n_jobs=-1)\n",
        "start_time_rand_gpr = time.time()\n",
        "random_search_gpr.fit(X_train, y_train)\n",
        "end_time_rand_gpr = time.time()\n",
        "\n",
        "print(\"\\nResultados de RandomizedSearchCV:\")\n",
        "print(f\"Mejores hiperparámetros: {random_search_gpr.best_params_}\")\n",
        "print(f\"Mejor MAE promedio: {-random_search_gpr.best_score_:.4f}\")\n",
        "print(f\"Tiempo: {end_time_rand_gpr - start_time_rand_gpr:.2f} segundos\")\n",
        "\n",
        "# --- Optimización con BayesSearchCV ---\n",
        "print(\"\\nIniciando optimización con BayesSearchCV...\")\n",
        "search_spaces_gpr = {\n",
        "    'kernel__k2__length_scale': Real(0.1, 2.0, prior='log-uniform'),\n",
        "    'alpha': Real(0.001, 0.1, prior='log-uniform')\n",
        "}\n",
        "bayes_search_gpr = BayesSearchCV(estimator=gpr, search_spaces=search_spaces_gpr,\n",
        "                                 n_iter=10, scoring=scoring_optimizer, refit='mae',\n",
        "                                 cv=kf, verbose=1, random_state=42, n_jobs=-1)\n",
        "start_time_bayes_gpr = time.time()\n",
        "bayes_search_gpr.fit(X_train, y_train)\n",
        "end_time_bayes_gpr = time.time()\n",
        "\n",
        "print(\"\\nResultados de BayesSearchCV:\")\n",
        "print(f\"Mejores hiperparámetros: {bayes_search_gpr.best_params_}\")\n",
        "print(f\"Mejor MAE promedio: {-bayes_search_gpr.best_score_:.4f}\")\n",
        "print(f\"Tiempo: {end_time_bayes_gpr - start_time_bayes_gpr:.2f} segundos\")\n",
        "\n",
        "# Evaluación final del mejor modelo (BayesSearchCV) en el conjunto completo\n",
        "best_gpr = bayes_search_gpr.best_estimator_\n",
        "cv_results_best_gpr = cross_validate(best_gpr, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "print(\"\\nMétricas del mejor modelo (BayesSearchCV) con validación cruzada:\")\n",
        "print(f\"  MAE:  {-cv_results_best_gpr['test_mae'].mean():.4f} +/- {cv_results_best_gpr['test_mae'].std():.4f}\")\n",
        "print(f\"  MSE:  {-cv_results_best_gpr['test_mse'].mean():.4f} +/- {cv_results_best_gpr['test_mse'].std():.4f}\")\n",
        "print(f\"  R2:   {cv_results_best_gpr['test_r2'].mean():.4f} +/- {cv_results_best_gpr['test_r2'].std():.4f}\")\n",
        "print(f\"  MAPE: {-cv_results_best_gpr['test_mape'].mean():.4f}% +/- {cv_results_best_gpr['test_mape'].std():.4f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OXKwdmI0qIaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RandomForestRegressor"
      ],
      "metadata": {
        "id": "kDTO6-FMmd1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Implementa RandomForestRegressor en Python con sklearn, optimiza n_estimators, max_depth y min_samples_split usando GridSearchCV, RandomizedSearchCV y BayesSearchCV de skopt, y reporta MAE, MSE, R2 y MAPE con validación cruzada de 5 folds.\"\n",
        "# Rango: n_estimators = [50, 256], max_depth = [None, 16], min_samples_split = [2, 10] (Grid); n_estimators randint(56, 256), max_depth [None] + list(range(5, 30)), min_samples_split randint(2, 25) (Random); n_estimators Integer(50, 256), max_depth Integer(5, 35), min_samples_split Integer(2, 25) (Bayesian).\n",
        "# Justificación: n_estimators controla ensamblado, max_depth y min_samples_split evitan overfitting; rangos amplios para explorar complejidad.\n",
        "\n",
        "# Importaciones necesarias\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Integer\n",
        "from scipy.stats import randint\n",
        "import time\n",
        "from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
        "\n",
        "# Suponiendo que X_train, y_train están definidos\n",
        "# Definir KFold para validación cruzada\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Definir métricas para evaluación\n",
        "scoring_optimizer = {'mae': make_scorer(mean_absolute_error, greater_is_better=False)}\n",
        "scoring = {\n",
        "    'mae': make_scorer(mean_absolute_error, greater_is_better=False),\n",
        "    'mse': make_scorer(mean_squared_error, greater_is_better=False),\n",
        "    'r2': make_scorer(r2_score, greater_is_better=True),\n",
        "    'mape': make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
        "}\n",
        "\n",
        "# Definir el modelo RandomForestRegressor\n",
        "rf_regressor = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "\n",
        "# --- Optimización con GridSearchCV ---\n",
        "print(\"\\nIniciando optimización con GridSearchCV para RandomForestRegressor...\")\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100],  # Reducido para acelerar\n",
        "    'max_depth': [None, 10],    # Reducido para acelerar\n",
        "    'min_samples_split': [2, 5] # Reducido para acelerar\n",
        "}\n",
        "grid_search_rf = GridSearchCV(estimator=rf_regressor, param_grid=param_grid_rf,\n",
        "                              scoring=scoring_optimizer, refit='mae',\n",
        "                              cv=kf, verbose=1, n_jobs=-1)\n",
        "start_time_grid_rf = time.time()\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "end_time_grid_rf = time.time()\n",
        "\n",
        "print(\"\\nResultados de GridSearchCV para RandomForestRegressor:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {grid_search_rf.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-grid_search_rf.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de GridSearchCV: {end_time_grid_rf - start_time_grid_rf:.2f} segundos\")\n",
        "\n",
        "# --- Optimización con RandomizedSearchCV ---\n",
        "print(\"\\nIniciando optimización con RandomizedSearchCV para RandomForestRegressor...\")\n",
        "param_dist_rf = {\n",
        "    'n_estimators': randint(50, 150),  # Rango reducido\n",
        "    'max_depth': [None] + list(range(5, 15)),  # Rango reducido\n",
        "    'min_samples_split': randint(2, 10)  # Rango reducido\n",
        "}\n",
        "random_search_rf = RandomizedSearchCV(estimator=rf_regressor, param_distributions=param_dist_rf,\n",
        "                                      n_iter=10,  # Reducido de 50 a 10\n",
        "                                      scoring=scoring_optimizer, refit='mae',\n",
        "                                      cv=kf, verbose=1, random_state=42, n_jobs=-1)\n",
        "start_time_rand_rf = time.time()\n",
        "random_search_rf.fit(X_train, y_train)\n",
        "end_time_rand_rf = time.time()\n",
        "\n",
        "print(\"\\nResultados de RandomizedSearchCV para RandomForestRegressor:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {random_search_rf.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-random_search_rf.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de RandomizedSearchCV: {end_time_rand_rf - start_time_rand_rf:.2f} segundos\")\n",
        "\n",
        "# --- Optimización con BayesSearchCV ---\n",
        "print(\"\\nIniciando optimización con BayesSearchCV para RandomForestRegressor...\")\n",
        "search_spaces_rf = {\n",
        "    'n_estimators': Integer(50, 150),  # Rango reducido\n",
        "    'max_depth': Integer(5, 15),       # Rango reducido\n",
        "    'min_samples_split': Integer(2, 10)  # Rango reducido\n",
        "}\n",
        "bayes_search_rf = BayesSearchCV(estimator=rf_regressor, search_spaces=search_spaces_rf,\n",
        "                                n_iter=10,  # Reducido de 50 a 10\n",
        "                                scoring=scoring_optimizer, refit='mae',\n",
        "                                cv=kf, verbose=1, random_state=42, n_jobs=-1)\n",
        "start_time_bayes_rf = time.time()\n",
        "bayes_search_rf.fit(X_train, y_train)\n",
        "end_time_bayes_rf = time.time()\n",
        "\n",
        "print(\"\\nResultados de BayesSearchCV para RandomForestRegressor:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {bayes_search_rf.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-bayes_search_rf.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de BayesSearchCV: {end_time_bayes_rf - start_time_bayes_rf:.2f} segundos\")\n",
        "\n",
        "# Evaluación final del mejor modelo (BayesSearchCV) en el conjunto completo\n",
        "best_rf = bayes_search_rf.best_estimator_\n",
        "cv_results_best_rf = cross_validate(best_rf, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "print(\"\\nMétricas del mejor modelo (BayesSearchCV) con validación cruzada:\")\n",
        "print(f\"  MAE:  {-cv_results_best_rf['test_mae'].mean():.4f} +/- {cv_results_best_rf['test_mae'].std():.4f}\")\n",
        "print(f\"  MSE:  {-cv_results_best_rf['test_mse'].mean():.4f} +/- {cv_results_best_rf['test_mse'].std():.4f}\")\n",
        "print(f\"  R2:   {cv_results_best_rf['test_r2'].mean():.4f} +/- {cv_results_best_rf['test_r2'].std():.4f}\")\n",
        "print(f\"  MAPE: {-cv_results_best_rf['test_mape'].mean():.4f}% +/- {cv_results_best_rf['test_mape'].std():.4f}%\")\n"
      ],
      "metadata": {
        "id": "2DcaxUFomjBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVR"
      ],
      "metadata": {
        "id": "TWYOIGvhmj6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: \"Implementa SVR en Python con sklearn, optimiza C, epsilon y gamma para kernel RBF usando GridSearchCV, RandomizedSearchCV y BayesSearchCV de skopt, y reporta MAE, MSE, R2 y MAPE con validación cruzada de 5 folds.\"\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "from scipy.stats import uniform, randint # Ensure randint is imported for Random Forest later if needed\n",
        "from skopt.space import Real, Integer, Categorical # Ensure Integer and Categorical are imported if used later\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "\n",
        "# Ensure scorers and kf are defined (as per the original code context)\n",
        "# (assuming the previous cells defining these have been run)\n",
        "\n",
        "# Redefine the custom MAPE scorer if it wasn't in the preceding code or is needed again\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    \"\"\"Calcula el Error Porcentual Absoluto Medio (MAPE).\"\"\"\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    epsilon = 1e-8\n",
        "    return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
        "\n",
        "# Create scorer objects for the metrics\n",
        "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
        "r2_scorer = make_scorer(r2_score, greater_is_better=True)\n",
        "mape_scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
        "\n",
        "# Define the scoring dictionary for cross_validate and optimizers\n",
        "# We will optimize based on MAE, so define a single scorer for the optimizers\n",
        "scoring_optimizer = {'mae': mae_scorer}\n",
        "\n",
        "# Define the full scoring dictionary for reporting results\n",
        "scoring = {\n",
        "    'mae': mae_scorer,\n",
        "    'mse': mse_scorer,\n",
        "    'r2': r2_scorer,\n",
        "    'mape': mape_scorer\n",
        "}\n",
        "\n",
        "\n",
        "# Definir el modelo SVR con kernel RBF\n",
        "svr = SVR(kernel='rbf')\n",
        "\n",
        "# Assuming X_train, y_train, and kf are defined in previous cells\n",
        "\n",
        "# --- Optimization with GridSearchCV ---\n",
        "print(\"\\nIniciando optimización con GridSearchCV para SVR (kernel RBF)...\")\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "# C = [1, 10, 100], epsilon = [0.01, 0.1, 0.5], gamma = ['scale', 'auto', 0.001]\n",
        "param_grid_svr = {\n",
        "    'C': [1, 10, 100],\n",
        "    'epsilon': [0.01, 0.1, 0.5],\n",
        "    'gamma': ['scale', 'auto', 0.001] # 'scale' and 'auto' are also valid values\n",
        "}\n",
        "\n",
        "grid_search_svr = GridSearchCV(estimator=svr, param_grid=param_grid_svr,\n",
        "                               scoring=scoring_optimizer, refit='mae', # Optimize using MAE\n",
        "                               cv=kf, verbose=1, n_jobs=-1)\n",
        "\n",
        "start_time_grid_svr = time.time()\n",
        "grid_search_svr.fit(X_train, y_train)\n",
        "end_time_grid_svr = time.time()\n",
        "\n",
        "print(\"\\nResultados de GridSearchCV para SVR:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {grid_search_svr.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-grid_search_svr.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de GridSearchCV: {end_time_grid_svr - start_time_grid_svr:.2f} segundos\")\n",
        "\n",
        "best_svr_grid = grid_search_svr.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de GridSearchCV para SVR con validación cruzada completa:\")\n",
        "cv_results_grid_best_svr = cross_validate(best_svr_grid, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_grid_svr = -cv_results_grid_best_svr['test_mae']\n",
        "cv_mse_grid_svr = -cv_results_grid_best_svr['test_mse']\n",
        "cv_r2_grid_svr = cv_results_grid_best_svr['test_r2']\n",
        "cv_mape_grid_svr = -cv_results_grid_best_svr['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_grid_svr.mean():.4f} +/- {cv_mae_grid_svr.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_grid_svr.mean():.4f} +/- {cv_mse_grid_svr.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_grid_svr.mean():.4f} +/- {cv_r2_grid_svr.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_grid_svr.mean():.4f}% +/- {cv_mape_grid_svr.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Optimization with RandomizedSearchCV ---\n",
        "print(\"\\nIniciando optimización con RandomizedSearchCV para SVR (kernel RBF)...\")\n",
        "\n",
        "# Define parameter distribution for RandomizedSearchCV\n",
        "# C loguniform(1, 1000), epsilon uniform(0, 1), gamma loguniform(0.0001, 1)\n",
        "param_dist_svr = {\n",
        "    'C': loguniform(1, 1000),\n",
        "    'epsilon': uniform(0, 1),\n",
        "    'gamma': loguniform(0.0001, 1)\n",
        "}\n",
        "\n",
        "# Number of iterations (puntos a probar). Ajusta este valor según el tiempo disponible.\n",
        "n_iter_rand_svr = 100 # Example: try 100 random combinations\n",
        "\n",
        "random_search_svr = RandomizedSearchCV(estimator=svr, param_distributions=param_dist_svr,\n",
        "                                       n_iter=n_iter_rand_svr,\n",
        "                                       scoring=scoring_optimizer, refit='mae', # Optimize using MAE\n",
        "                                       cv=kf, verbose=1, random_state=42, n_jobs=-1) # random_state para reproducibilidad\n",
        "\n",
        "start_time_rand_svr = time.time()\n",
        "random_search_svr.fit(X_train, y_train)\n",
        "end_time_rand_svr = time.time()\n",
        "\n",
        "print(\"\\nResultados de RandomizedSearchCV para SVR:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {random_search_svr.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-random_search_svr.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de RandomizedSearchCV: {end_time_rand_svr - start_time_rand_svr:.2f} segundos\")\n",
        "\n",
        "best_svr_rand = random_search_svr.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de RandomizedSearchCV para SVR con validación cruzada completa:\")\n",
        "cv_results_rand_best_svr = cross_validate(best_svr_rand, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_rand_svr = -cv_results_rand_best_svr['test_mae']\n",
        "cv_mse_rand_svr = -cv_results_rand_best_svr['test_mse']\n",
        "cv_r2_rand_svr = cv_results_rand_best_svr['test_r2']\n",
        "cv_mape_rand_svr = -cv_results_rand_best_svr['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_rand_svr.mean():.4f} +/- {cv_mae_rand_svr.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_rand_svr.mean():.4f} +/- {cv_mse_rand_svr.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_rand_svr.mean():.4f} +/- {cv_r2_rand_svr.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_rand_svr.mean():.4f}% +/- {cv_mape_rand_svr.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Optimization with BayesSearchCV ---\n",
        "print(\"\\nIniciando optimización con BayesSearchCV para SVR (kernel RBF)...\")\n",
        "\n",
        "# Define search spaces for BayesSearchCV\n",
        "# C Real(1e-1, 1e3, prior='log-uniform'), epsilon Real(1e-3, 1e0, prior='uniform'), gamma Real(1e-4, 1e0, prior='log-uniform')\n",
        "search_spaces_svr = {\n",
        "    'C': Real(1e-1, 1e3, prior='log-uniform'),\n",
        "    'epsilon': Real(1e-3, 1e0, prior='uniform'),\n",
        "    'gamma': Real(1e-4, 1e0, prior='log-uniform')\n",
        "}\n",
        "\n",
        "# Number of iterations (puntos a explorar). Generalmente requiere menos que RandomizedSearch.\n",
        "n_iter_bayes_svr = 100 # Example: try 100 iterations. Adjust based on computational resources.\n",
        "\n",
        "bayes_search_svr = BayesSearchCV(estimator=svr, search_spaces=search_spaces_svr,\n",
        "                                 n_iter=n_iter_bayes_svr,\n",
        "                                 scoring=scoring_optimizer, refit='mae', # Optimize using MAE\n",
        "                                 cv=kf, verbose=1, random_state=42, n_jobs=-1)\n",
        "\n",
        "start_time_bayes_svr = time.time()\n",
        "bayes_search_svr.fit(X_train, y_train)\n",
        "end_time_bayes_svr = time.time()\n",
        "\n",
        "print(\"\\nResultados de BayesSearchCV para SVR:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {bayes_search_svr.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-bayes_search_svr.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de BayesSearchCV: {end_time_bayes_svr - start_time_bayes_svr:.2f} segundos\")\n",
        "\n",
        "best_svr_bayes = bayes_search_svr.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de BayesSearchCV para SVR con validación cruzada completa:\")\n",
        "cv_results_bayes_best_svr = cross_validate(best_svr_bayes, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_bayes_svr = -cv_results_bayes_best_svr['test_mae']\n",
        "cv_mse_bayes_svr = -cv_results_bayes_best_svr['test_mse']\n",
        "cv_r2_bayes_svr = cv_results_bayes_best_svr['test_r2']\n",
        "cv_mape_bayes_svr = -cv_results_bayes_best_svr['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_bayes_svr.mean():.4f} +/- {cv_mae_bayes_svr.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_bayes_svr.mean():.4f} +/- {cv_mse_bayes_svr.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_bayes_svr.mean():.4f} +/- {cv_r2_bayes_svr.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_bayes_svr.mean():.4f}% +/- {cv_mape_bayes_svr.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Comparación de resultados de SVR ---\n",
        "print(\"\\n--- Resumen de Resultados de Optimización para SVR (MAE promedio en validación) ---\")\n",
        "print(f\"GridSearchCV:      {-grid_search_svr.best_score_:.4f}\")\n",
        "print(f\"RandomizedSearchCV: {-random_search_svr.best_score_:.4f}\")\n",
        "print(f\"BayesSearchCV:      {-bayes_search_svr.best_score_:.4f}\")\n",
        "\n",
        "print(\"\\n--- Reporte Completo de Métricas para SVR (Promedio de 5-fold CV) ---\")\n",
        "\n",
        "print(\"\\nGridSearchCV Mejor Modelo (SVR):\")\n",
        "print(f\"  MAE:  {cv_mae_grid_svr.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_grid_svr.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_grid_svr.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_grid_svr.mean():.4f}%\")\n",
        "\n",
        "print(\"\\nRandomizedSearchCV Mejor Modelo (SVR):\")\n",
        "print(f\"  MAE:  {cv_mae_rand_svr.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_rand_svr.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_rand_svr.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_rand_svr.mean():.4f}%\")\n",
        "\n",
        "print(\"\\nBayesSearchCV Mejor Modelo (SVR):\")\n",
        "print(f\"  MAE:  {cv_mae_bayes_svr.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_bayes_svr.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_bayes_svr.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_bayes_svr.mean():.4f}%\")\n",
        "\n",
        "# You can select the overall best SVR model based on validation MAE\n",
        "# For instance, if BayesSearchCV gave the best result:\n",
        "# final_best_svr_model = best_svr_bayes\n",
        "# print(\"\\nEl mejor modelo SVR general basado en MAE promedio de validación es el de BayesSearchCV.\")\n",
        "\n",
        "# Optional: Train the chosen best SVR model on the entire X_train and evaluate on X_test\n",
        "# final_best_svr_model = best_svr_bayes # Or grid_search_svr.best_estimator_ or random_search_svr.best_estimator_\n",
        "\n",
        "# print(\"\\nEntrenando el modelo SVR final seleccionado en el conjunto de entrenamiento completo...\")\n",
        "# final_best_svr_model.fit(X_train, y_train)\n",
        "# print(\"Evaluando en el conjunto de prueba (X_test) con el modelo SVR final...\")\n",
        "# y_pred_test_final_svr = final_best_svr_model.predict(X_test)\n",
        "\n",
        "# # Calculate metrics on the test set with the final SVR model\n",
        "# test_mae_final_svr = mean_absolute_error(y_test, y_pred_test_final_svr)\n",
        "# test_mse_final_svr = mean_squared_error(y_test, y_pred_test_final_svr)\n",
        "# test_r2_final_svr = r2_score(y_test, y_pred_test_final_svr)\n",
        "# test_mape_final_svr = mean_absolute_percentage_error(y_test, y_pred_test_final_svr)\n",
        "\n",
        "# print(\"\\nMétricas en el conjunto de prueba con el modelo SVR final seleccionado:\")\n",
        "# print(f\"  MAE:  {test_mae_final_svr:.4f}\")\n",
        "# print(f\"  MSE:  {test_mse_final_svr:.4f}\")\n",
        "# print(f\"  R2:   {test_r2_final_svr:.4f}\")\n",
        "# print(f\"  MAPE: {test_mape_final_svr:.4f}%\")"
      ],
      "metadata": {
        "id": "fe6MWsg3mnrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resultados"
      ],
      "metadata": {
        "id": "YwY69B0RaQE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Crea gráficas de barras que comparen las métricas MAE, MSE, R2 y MAPE de cada uno de los modelos.\n",
        "# Indica los tres mejores modelos según los rendimientos de cada uno y para los tres mejores modelos, genera gráficas de importancia de características.\n",
        "# Asegúrate de que todas las gráficas sean claras, bien etiquetadas y adecuadas para un dashboard de Streamlit.\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import BayesianRidge, SGDRegressor\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from scipy.stats import randint\n",
        "from sklearn.linear_model import LinearRegression # Import LinearRegression for coefficient plotting\n",
        "\n",
        "# Dictionary to store the best results for each model (optimized)\n",
        "model_results = {\n",
        "    'Linear Regression (Default)': {\n",
        "        'MAE': cv_mae.mean(),\n",
        "        'MSE': cv_mse.mean(),\n",
        "        'R2': cv_r2.mean(),\n",
        "        'MAPE': cv_mape.mean()\n",
        "    },\n",
        "    'Lasso (Bayes)': { # Using BayesSearchCV as it often finds better solutions\n",
        "        'MAE': cv_mae_bayes.mean(),\n",
        "        'MSE': cv_mse_bayes.mean(),\n",
        "        'R2': cv_r2_bayes.mean(),\n",
        "        'MAPE': cv_mape_bayes.mean()\n",
        "    },\n",
        "    'ElasticNet (Bayes)': { # Using BayesSearchCV\n",
        "        'MAE': cv_mae_bayes_en.mean(),\n",
        "        'MSE': cv_mse_bayes_en.mean(),\n",
        "        'R2': cv_r2_bayes_en.mean(),\n",
        "        'MAPE': cv_mape_bayes_en.mean()\n",
        "    },\n",
        "    'KernelRidge (Bayes)': { # Using BayesSearchCV\n",
        "        'MAE': cv_mae_bayes_krr.mean(),\n",
        "        'MSE': cv_mse_bayes_krr.mean(),\n",
        "        'R2': cv_r2_bayes_krr.mean(),\n",
        "        'MAPE': cv_mape_bayes_krr.mean()\n",
        "    },\n",
        "    'SGDRegressor (Bayes)': { # Using BayesSearchCV\n",
        "        'MAE': cv_mae_bayes_sgd.mean(),\n",
        "        'MSE': cv_mse_bayes_sgd.mean(),\n",
        "        'R2': cv_r2_bayes_sgd.mean(),\n",
        "        'MAPE': cv_mape_bayes_sgd.mean()\n",
        "    },\n",
        "     'BayesianRidge (Default)': { # As this model was run without optimization\n",
        "        'MAE': cv_mae.mean(), # These variables should hold the results from the last BayesianRidge run\n",
        "        'MSE': cv_mse.mean(),\n",
        "        'R2': cv_r2.mean(),\n",
        "        'MAPE': cv_mape.mean()\n",
        "    },\n",
        "    'GaussianProcessRegressor (Bayes)': { # Using BayesSearchCV\n",
        "        'MAE': -cv_results_best_gpr['test_mae'].mean(),\n",
        "        'MSE': -cv_results_best_gpr['test_mse'].mean(),\n",
        "        'R2': cv_results_best_gpr['test_r2'].mean(),\n",
        "        'MAPE': -cv_results_best_gpr['test_mape'].mean()\n",
        "    },\n",
        "    'RandomForestRegressor (Bayes)': { # Using BayesSearchCV\n",
        "        'MAE': -cv_results_best_rf['test_mae'].mean(),\n",
        "        'MSE': -cv_results_best_rf['test_mse'].mean(),\n",
        "        'R2': cv_results_best_rf['test_r2'].mean(),\n",
        "        'MAPE': -cv_results_best_rf['test_mape'].mean()\n",
        "    },\n",
        "    'SVR (Bayes)': { # Using BayesSearchCV\n",
        "        'MAE': -cv_results_bayes_best_svr['test_mae'].mean(),\n",
        "        'MSE': -cv_results_bayes_best_svr['test_mse'].mean(),\n",
        "        # Corrected: Access 'test_r2' from the dictionary\n",
        "        'R2': cv_results_bayes_best_svr['test_r2'].mean(),\n",
        "        'MAPE': cv_mape_bayes_svr.mean() # The variable name was slightly different\n",
        "    }\n",
        "}\n",
        "\n",
        "# Convert results to DataFrame for easy plotting\n",
        "results_df = pd.DataFrame.from_dict(model_results, orient='index')\n",
        "\n",
        "# Sort models by MAE (lower is better)\n",
        "results_df_sorted_mae = results_df.sort_values(by='MAE')\n",
        "print(\"\\nRanking de Modelos por MAE:\")\n",
        "print(results_df_sorted_mae[['MAE']])\n",
        "\n",
        "# Sort models by MSE (lower is better)\n",
        "results_df_sorted_mse = results_df.sort_values(by='MSE')\n",
        "print(\"\\nRanking de Modelos por MSE:\")\n",
        "print(results_df_sorted_mse[['MSE']])\n",
        "\n",
        "# Sort models by R2 (higher is better)\n",
        "results_df_sorted_r2 = results_df.sort_values(by='R2', ascending=False)\n",
        "print(\"\\nRanking de Modelos por R2:\")\n",
        "print(results_df_sorted_r2[['R2']])\n",
        "\n",
        "# Sort models by MAPE (lower is better)\n",
        "results_df_sorted_mape = results_df.sort_values(by='MAPE')\n",
        "print(\"\\nRanking de Modelos por MAPE:\")\n",
        "print(results_df_sorted_mape[['MAPE']])\n",
        "\n",
        "\n",
        "# Define the best models based on MAE (example, adjust if criteria changes)\n",
        "best_mae_models = results_df_sorted_mae.head(3).index.tolist()\n",
        "print(f\"\\nLos 3 mejores modelos según MAE son: {best_mae_models}\")\n",
        "\n",
        "# Define the best models based on MSE (example, adjust if criteria changes)\n",
        "best_mse_models = results_df_sorted_mse.head(3).index.tolist()\n",
        "print(f\"\\nLos 3 mejores modelos según MSE son: {best_mse_models}\")\n",
        "\n",
        "# Define the best models based on R2 (example, adjust if criteria changes)\n",
        "best_r2_models = results_df_sorted_r2.head(3).index.tolist()\n",
        "print(f\"\\nLos 3 mejores modelos según R2 son: {best_r2_models}\")\n",
        "\n",
        "# Define the best models based on MAPE (example, adjust if criteria changes)\n",
        "best_mape_models = results_df_sorted_mape.head(3).index.tolist()\n",
        "print(f\"\\nLos 3 mejores modelos según MAPE son: {best_mape_models}\")\n",
        "\n",
        "\n",
        "# Combine the top models from each metric (optional, to get a broader view)\n",
        "all_top_models = list(set(best_mae_models + best_mse_models + best_r2_models + best_mape_models))\n",
        "print(f\"\\nModelos que aparecieron en el top 3 de al menos una métrica: {all_top_models}\")\n",
        "\n",
        "# For the purpose of feature importance visualization, let's select the top 3 models based on MAE\n",
        "top_3_models_for_importance = best_mae_models\n",
        "print(f\"\\nSe generarán gráficas de importancia de características para los 3 mejores modelos según MAE: {top_3_models_for_importance}\")\n",
        "\n",
        "\n",
        "# Plotting function for metrics\n",
        "def plot_metrics_bar(results_df, metric, title, ylabel, ascending=True):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(x=results_df.index, y=metric, data=results_df.sort_values(by=metric, ascending=ascending))\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.xlabel(\"Modelo\", fontsize=12)\n",
        "    plt.ylabel(ylabel, fontsize=12)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Generate bar plots for each metric\n",
        "plot_metrics_bar(results_df, 'MAE', 'Comparación de Modelos por MAE', 'MAE', ascending=True)\n",
        "plot_metrics_bar(results_df, 'MSE', 'Comparación de Modelos por MSE', 'MSE', ascending=True)\n",
        "plot_metrics_bar(results_df, 'R2', 'Comparación de Modelos por R2', 'R2 Score', ascending=False)\n",
        "plot_metrics_bar(results_df, 'MAPE', 'Comparación de Modelos por MAPE', 'MAPE (%)', ascending=True)\n",
        "\n",
        "# Function to get the best estimator for a given model name (optimized via BayesSearchCV if available)\n",
        "def get_best_estimator(model_name):\n",
        "    # Make sure to have the optimizer objects defined in the environment if needed\n",
        "    # (e.g., bayes_search, bayes_search_en, etc.)\n",
        "    if model_name == 'Linear Regression (Default)':\n",
        "        return LinearRegression().fit(X_train, y_train) # Train default Linear Regression for coefficients\n",
        "    elif model_name == 'Lasso (Bayes)':\n",
        "        # Assuming bayes_search is the BayesSearchCV object for Lasso\n",
        "        return bayes_search.best_estimator_\n",
        "    elif model_name == 'ElasticNet (Bayes)':\n",
        "         # Assuming bayes_search_en is the BayesSearchCV object for ElasticNet\n",
        "        return bayes_search_en.best_estimator_\n",
        "    elif model_name == 'KernelRidge (Bayes)':\n",
        "         # KRR does not have feature coefficients like linear models or tree-based models\n",
        "         print(f\"Advertencia: {model_name} no proporciona importancia de características directa (coeficientes).\")\n",
        "         return None\n",
        "    elif model_name == 'SGDRegressor (Bayes)':\n",
        "         # Assuming bayes_search_sgd is the BayesSearchCV object for SGDRegressor\n",
        "        return bayes_search_sgd.best_estimator_\n",
        "    elif model_name == 'BayesianRidge (Default)':\n",
        "        # Train a default BayesianRidge model for coefficients\n",
        "        return BayesianRidge().fit(X_train, y_train)\n",
        "    elif model_name == 'GaussianProcessRegressor (Bayes)':\n",
        "        # GPR does not have feature importance based on coefficients or tree structure\n",
        "        print(f\"Advertencia: {model_name} no proporciona importancia de características directa.\")\n",
        "        return None\n",
        "    elif model_name == 'RandomForestRegressor (Bayes)':\n",
        "        # Assuming bayes_search_rf is the BayesSearchCV object for RandomForestRegressor\n",
        "        return bayes_search_rf.best_estimator_\n",
        "    elif model_name == 'SVR (Bayes)':\n",
        "        # SVR with RBF kernel does not provide linear coefficients\n",
        "        print(f\"Advertencia: {model_name} no proporciona importancia de características directa.\")\n",
        "        return None\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "# Generate Feature Importance plots for the top 3 models (based on MAE)\n",
        "print(\"\\nGenerando gráficas de importancia de características para los 3 mejores modelos (según MAE):\")\n",
        "\n",
        "for model_name in top_3_models_for_importance:\n",
        "    print(f\"\\nImportancia de Características para: {model_name}\")\n",
        "    best_model = get_best_estimator(model_name)\n",
        "\n",
        "    if best_model:\n",
        "        if hasattr(best_model, 'coef_'): # For linear models (LinearRegression, Lasso, ElasticNet, SGDRegressor, BayesianRidge)\n",
        "            # Ensure X_train is a pandas DataFrame with named columns for plotting\n",
        "            if isinstance(X_train, pd.DataFrame):\n",
        "                coefficients = pd.Series(best_model.coef_, index=X_train.columns)\n",
        "                # Sort coefficients by absolute value for importance\n",
        "                sorted_coefficients = coefficients.abs().sort_values(ascending=False)\n",
        "\n",
        "                plt.figure(figsize=(10, min(20, len(sorted_coefficients) * 0.3))) # Adjust figure size based on num features\n",
        "                sns.barplot(x=sorted_coefficients.values, y=sorted_coefficients.index)\n",
        "                plt.title(f'Importancia de Características (Coeficientes Absolutos) - {model_name}', fontsize=14)\n",
        "                plt.xlabel('Magnitud del Coeficiente', fontsize=12)\n",
        "                plt.ylabel('Característica', fontsize=12)\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "            else:\n",
        "                 print(f\"Advertencia: X_train no es un DataFrame. No se puede generar la gráfica de importancia de características para {model_name}.\")\n",
        "\n",
        "\n",
        "        elif hasattr(best_model, 'feature_importances_'): # For tree-based models (RandomForestRegressor)\n",
        "             # Ensure X_train is a pandas DataFrame with named columns for plotting\n",
        "            if isinstance(X_train, pd.DataFrame):\n",
        "                importances = pd.Series(best_model.feature_importances_, index=X_train.columns)\n",
        "                # Sort feature importances\n",
        "                sorted_importances = importances.sort_values(ascending=False)\n",
        "\n",
        "                plt.figure(figsize=(10, min(20, len(sorted_importances) * 0.3))) # Adjust figure size\n",
        "                sns.barplot(x=sorted_importances.values, y=sorted_importances.index)\n",
        "                plt.title(f'Importancia de Características - {model_name}', fontsize=14)\n",
        "                plt.xlabel('Importancia', fontsize=12)\n",
        "                plt.ylabel('Característica', fontsize=12)\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "            else:\n",
        "                 print(f\"Advertencia: X_train no es un DataFrame. No se puede generar la gráfica de importancia de características para {model_name}.\")\n",
        "        else:\n",
        "             print(f\"El modelo {model_name} no tiene atributos 'coef_' o 'feature_importances_'.\")\n",
        "    else:\n",
        "        print(f\"No se pudo obtener el mejor estimador para {model_name} o no aplica para importancia de características.\")\n",
        "\n",
        "\n",
        "# Print the top 3 models based on each metric explicitly again\n",
        "print(\"\\n--- Resumen de los 3 Mejores Modelos por Métrica ---\")\n",
        "print(\"Según MAE (menor es mejor):\")\n",
        "print(results_df_sorted_mae.head(3)[['MAE']])\n",
        "print(\"\\nSegún MSE (menor es mejor):\")\n",
        "print(results_df_sorted_mse.head(3)[['MSE']])\n",
        "print(\"\\nSegún R2 (mayor es mejor):\")\n",
        "print(results_df_sorted_r2.head(3)[['R2']])\n",
        "print(\"\\nSegún MAPE (menor es mejor):\")\n",
        "print(results_df_sorted_mape.head(3)[['MAPE']])"
      ],
      "metadata": {
        "id": "P_Gsj2pjb8h4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DashBoard"
      ],
      "metadata": {
        "id": "SFUbGc-hlZFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize\n",
        "!pip install streamlit -q #instalación de librerías\n",
        "!pip install pyngrok\n",
        "!pip install optuna\n",
        "!pip install streamlit pandas matplotlib seaborn scikit-learn pyngrok kagglehub\n",
        "!pip install pyngrok streamlit --quiet"
      ],
      "metadata": {
        "id": "wWvvtNi_lc34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir pages"
      ],
      "metadata": {
        "id": "tbOtl_LxlkRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile TAM.py\n",
        "import streamlit as st\n",
        "from pyngrok import ngrok\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import joblib\n",
        "import shap\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import ElasticNet, SGDRegressor\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "from scipy.stats import loguniform, uniform\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
      ],
      "metadata": {
        "id": "00w9pas6lq02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Histograma de \"SalePrice\" con sns.histplot(df['SalePrice'], kde=True).\n",
        "# Mapa de calor de correlación con sns.heatmap(df.corr(), annot=True, cmap='coolwarm').\n",
        "# Gráficos de barras para variables categóricas, como sns.countplot(x='Neighborhood', data=df).\n",
        "# Usa st.pyplot() para mostrar estas gráficas en el dashboard.\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_validate\n",
        "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet, SGDRegressor, BayesianRidge\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Integer\n",
        "from scipy.stats import loguniform, uniform, randint\n",
        "\n",
        "# Configuración de la página de Streamlit\n",
        "st.set_page_config(layout=\"wide\", page_title=\"Análisis y Modelización de Precios de Viviendas\")\n",
        "\n",
        "st.title('Análisis y Modelización de Precios de Viviendas')\n",
        "\n",
        "# Cargar los datos\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    try:\n",
        "        df = pd.read_csv('/content/drive/Shareddrives/UNAL_Colab/Teoría de Aprendizaje de Máquina/AmesHousing.csv')\n",
        "        # Limpiar nombres de columnas (quitar espacios y caracteres especiales)\n",
        "        df.columns = df.columns.str.replace(' ', '_').str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        st.error(\"Error: Asegúrese de que el archivo 'AmesHousing.csv' esté en la ruta especificada.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "df = load_data()\n",
        "\n",
        "if df.empty:\n",
        "    st.stop()\n",
        "\n",
        "st.subheader(\"Datos Originales (primeras 5 filas)\")\n",
        "st.dataframe(df.head())\n",
        "\n",
        "# Procesamiento de Datos (Basado en tu código anterior)\n",
        "@st.cache_data\n",
        "def preprocess_data(df):\n",
        "    # Copia del dataframe para evitar modificar el original en caché\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    # Eliminar columnas con muchos valores nulos\n",
        "    threshold = len(df_processed) * 0.5 # Eliminar columnas con más del 50% de nulos\n",
        "    cols_to_drop_nulls = df_processed.columns[df_processed.isnull().sum() > threshold].tolist()\n",
        "    df_processed = df_processed.drop(columns=cols_to_drop_nulls)\n",
        "    st.sidebar.write(f\"Columnas eliminadas por exceso de nulos (>50%): {cols_to_drop_nulls}\")\n",
        "\n",
        "    # Imputación de valores nulos\n",
        "    # Identificar columnas por tipo de dato después de eliminar las columnas con muchos nulos\n",
        "    numerical_cols = df_processed.select_dtypes(include=np.number).columns.tolist()\n",
        "    categorical_cols = df_processed.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "    # Imputar numéricas con la mediana\n",
        "    for col in numerical_cols:\n",
        "        if df_processed[col].isnull().any():\n",
        "            median_val = df_processed[col].median()\n",
        "            df_processed[col].fillna(median_val, inplace=True)\n",
        "\n",
        "    # Imputar categóricas con la moda o una categoría 'missing'\n",
        "    for col in categorical_cols:\n",
        "        if df_processed[col].isnull().any():\n",
        "            # For simplicity, impute with a placeholder 'Missing' or the mode\n",
        "            # Here using 'Missing' as it often preserves info about the missingness\n",
        "            df_processed[col].fillna('Missing', inplace=True)\n",
        "\n",
        "    # Crear características de edad (ejemplo: edad de la casa y edad de remodelación)\n",
        "    if 'YearBuilt' in df_processed.columns:\n",
        "         current_year = 2023 # O el año actual relevante\n",
        "         df_processed['Age'] = current_year - df_processed['YearBuilt']\n",
        "         numerical_cols.append('Age') # Añadir la nueva columna numérica\n",
        "         # Eliminar YearBuilt para evitar colinealidad\n",
        "         if 'YearBuilt' in df_processed.columns:\n",
        "             df_processed = df_processed.drop(columns=['YearBuilt'])\n",
        "             if 'YearBuilt' in numerical_cols:\n",
        "                 numerical_cols.remove('YearBuilt')\n",
        "\n",
        "    if 'YearRemodAdd' in df_processed.columns:\n",
        "         df_processed['YearsSinceRemodel'] = current_year - df_processed['YearRemodAdd']\n",
        "         numerical_cols.append('YearsSinceRemodel') # Añadir la nueva columna numérica\n",
        "         # Eliminar YearRemodAdd para evitar colinealidad\n",
        "         if 'YearRemodAdd' in df_processed.columns:\n",
        "             df_processed = df_processed.drop(columns=['YearRemodAdd'])\n",
        "             if 'YearRemodAdd' in numerical_cols:\n",
        "                 numerical_cols.remove('YearRemodAdd')\n",
        "\n",
        "\n",
        "    # Identificar variables categóricas (nominales y ordinales si aplica)\n",
        "    # Aquí, para simplificar, trataremos todas las 'object' como nominales para One-Hot Encoding\n",
        "    # Si tuvieras un diccionario de mapeo para ordinales, lo aplicarías aquí antes de One-Hot Encoding\n",
        "\n",
        "    # Separar SalePrice antes de la codificación\n",
        "    if 'SalePrice' in df_processed.columns:\n",
        "        y = df_processed['SalePrice']\n",
        "        X = df_processed.drop('SalePrice', axis=1)\n",
        "    else:\n",
        "        st.error(\"La columna 'SalePrice' no se encontró en el DataFrame procesado.\")\n",
        "        return None, None, None # Retorna None si SalePrice no está presente\n",
        "\n",
        "\n",
        "    # Aplicar One-Hot Encoding a las columnas categóricas restantes en X\n",
        "    X = pd.get_dummies(X, columns=categorical_cols, dummy_na=False) # dummy_na=False si ya imputaste nulos\n",
        "\n",
        "    # Asegurarse de que X solo contenga columnas numéricas después del get_dummies\n",
        "    X = X.select_dtypes(include=np.number)\n",
        "\n",
        "    # Asegurarse de que y coincida con las filas de X después de cualquier posible eliminación de filas (si hubo dropna)\n",
        "    # Aunque con la imputación no debería haber NaNs en X, es una buena práctica.\n",
        "    # Si hubo alguna eliminación de filas por NaNs en X antes, necesitarías re-indexar y.\n",
        "    # Dado que imputamos, no esperamos NaNs restantes, pero si los hubiera, se podría añadir X = X.dropna()\n",
        "\n",
        "    # Retornar X y y\n",
        "    return X, y, df_processed # También retornamos el df_processed por si se necesita para visualizaciones pre-split\n",
        "\n",
        "X, y, df_processed = preprocess_data(df.copy()) # Usar una copia para el procesamiento\n",
        "\n",
        "if X is None or y is None:\n",
        "     st.stop()\n",
        "\n",
        "st.sidebar.subheader(\"Opciones de Visualización\")\n",
        "viz_option = st.sidebar.selectbox(\"Selecciona una visualización:\",\n",
        "                                   [\"Distribución de SalePrice\",\n",
        "                                    \"Mapa de Calor de Correlación\",\n",
        "                                    \"Relación con SalePrice (Scatter plots)\",\n",
        "                                    \"Distribución de Variables Categóricas (Original)\",\n",
        "                                    \"Distribución de Variables Numéricas\"])\n",
        "\n",
        "# Visualizaciones\n",
        "if viz_option == \"Distribución de SalePrice\":\n",
        "    st.subheader(\"Distribución de SalePrice\")\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # Histograma de SalePrice\n",
        "    sns.histplot(df_processed['SalePrice'], kde=True, ax=axes[0])\n",
        "    axes[0].set_title('Distribución de SalePrice')\n",
        "    axes[0].set_xlabel('SalePrice')\n",
        "    axes[0].set_ylabel('Frecuencia')\n",
        "\n",
        "    # Boxplot de SalePrice\n",
        "    sns.boxplot(y=df_processed['SalePrice'], ax=axes[1])\n",
        "    axes[1].set_title('Boxplot de SalePrice')\n",
        "    axes[1].set_ylabel('SalePrice')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    st.pyplot(fig)\n",
        "\n",
        "elif viz_option == \"Mapa de Calor de Correlación\":\n",
        "    st.subheader(\"Mapa de Calor de Correlación\")\n",
        "    numeric_df_for_corr = df_processed.select_dtypes(include=np.number)\n",
        "\n",
        "    if 'SalePrice' in numeric_df_for_corr.columns:\n",
        "        st.write(\"Correlación de variables numéricas con SalePrice:\")\n",
        "        correlation_with_saleprice = numeric_df_for_corr.corr()['SalePrice'].sort_values(ascending=False)\n",
        "        st.dataframe(correlation_with_saleprice) # Mostrar la tabla de correlación\n",
        "\n",
        "        # Visualizar el heatmap completo (puede ser muy denso, opcional)\n",
        "        # st.write(\"Heatmap de Matriz de Correlación Completa (Variables Numéricas):\")\n",
        "        # fig_full_corr, ax_full_corr = plt.subplots(figsize=(15, 12))\n",
        "        # sns.heatmap(numeric_df_for_corr.corr(), cmap='coolwarm', annot=False, ax=ax_full_corr)\n",
        "        # ax_full_corr.set_title('Matriz de Correlación de Variables Numéricas')\n",
        "        # st.pyplot(fig_full_corr)\n",
        "\n",
        "        # Visualizar solo las correlaciones con SalePrice (más legible)\n",
        "        st.write(\"Heatmap de Correlación con SalePrice:\")\n",
        "        fig_saleprice_corr, ax_saleprice_corr = plt.subplots(figsize=(8, 10))\n",
        "        sns.heatmap(correlation_with_saleprice.to_frame(), cmap='coolwarm', annot=True, fmt=\".2f\", cbar=False, ax=ax_saleprice_corr)\n",
        "        ax_saleprice_corr.set_title('Correlación de Variables Numéricas con SalePrice')\n",
        "        ax_saleprice_corr.tick_params(axis='y', rotation=0)\n",
        "        st.pyplot(fig_saleprice_corr)\n",
        "\n",
        "    else:\n",
        "        st.warning(\"La columna 'SalePrice' no está disponible para calcular la correlación.\")\n",
        "\n",
        "elif viz_option == \"Relación con SalePrice (Scatter plots)\":\n",
        "    st.subheader(\"Relación entre SalePrice y Variables Numéricas Clave\")\n",
        "    numeric_df_current = df_processed.select_dtypes(include=np.number)\n",
        "\n",
        "    if 'SalePrice' in numeric_df_current.columns:\n",
        "        correlation_with_saleprice = numeric_df_current.corr()['SalePrice'].sort_values(ascending=False)\n",
        "        # Seleccionar las N variables más correlacionadas (excluyendo SalePrice)\n",
        "        top_n = st.slider(\"Número de variables más correlacionadas a mostrar:\", 3, 10, 5)\n",
        "        top_correlated_cols = correlation_with_saleprice.head(top_n + 1).index.tolist()\n",
        "        if 'SalePrice' in top_correlated_cols:\n",
        "            top_correlated_cols.remove('SalePrice')\n",
        "\n",
        "        st.write(f\"Mostrando la relación de SalePrice con las {len(top_correlated_cols)} variables más correlacionadas:\")\n",
        "\n",
        "        # Calcular el número de filas y columnas para la cuadrícula de gráficos\n",
        "        num_cols_plot = 3\n",
        "        num_rows_plot = int(np.ceil(len(top_correlated_cols) / num_cols_plot))\n",
        "\n",
        "        if len(top_correlated_cols) > 0:\n",
        "            fig_scatter, axes_scatter = plt.subplots(num_rows_plot, num_cols_plot, figsize=(15, 5 * num_rows_plot))\n",
        "            # Aplanar el array de axes si hay más de 1 fila\n",
        "            axes_scatter = axes_scatter.flatten() if num_rows_plot > 1 else [axes_scatter]\n",
        "\n",
        "            for i, col in enumerate(top_correlated_cols):\n",
        "                 if col in numeric_df_current.columns:\n",
        "                    sns.scatterplot(x=numeric_df_current[col], y=numeric_df_current['SalePrice'], ax=axes_scatter[i])\n",
        "                    axes_scatter[i].set_title(f'SalePrice vs {col}')\n",
        "                    axes_scatter[i].set_xlabel(col)\n",
        "                    axes_scatter[i].set_ylabel('SalePrice')\n",
        "                 else:\n",
        "                     st.warning(f\"La columna '{col}' no está disponible para scatter plot.\")\n",
        "                     axes_scatter[i].set_visible(False) # Ocultar el subplot vacío\n",
        "\n",
        "            # Ocultar subplots vacíos si el número total de gráficos no llena la cuadrícula\n",
        "            for j in range(i + 1, len(axes_scatter)):\n",
        "                axes_scatter[j].set_visible(False)\n",
        "\n",
        "\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig_scatter)\n",
        "        else:\n",
        "            st.info(\"No hay variables numéricas para graficar scatter plots con SalePrice.\")\n",
        "\n",
        "    else:\n",
        "         st.warning(\"La columna 'SalePrice' no está disponible para crear scatter plots de correlación.\")\n",
        "\n",
        "\n",
        "elif viz_option == \"Distribución de Variables Categóricas (Original)\":\n",
        "    st.subheader(\"Distribución de Variables Categóricas\")\n",
        "    # Para visualizar las variables categóricas con countplots, es mejor usar el df original\n",
        "    # o al menos el df antes de One-Hot Encoding.\n",
        "    # Asumiendo que df_processed mantiene las columnas categóricas originales antes de get_dummies\n",
        "    # Opcional: Cargar el dataframe original si el df_processed ya ha sido OHEado\n",
        "    try:\n",
        "         # Intentar usar el df_processed antes de la codificación si está disponible\n",
        "         # Si no, cargar el original.\n",
        "         if not any(df_processed.select_dtypes(include='object').columns):\n",
        "             df_original_for_viz = load_data() # Cargar el original de nuevo si df_processed no tiene categóricas\n",
        "\n",
        "             # Aplicar solo la limpieza de nombres de columnas al original\n",
        "             df_original_for_viz.columns = df_original_for_viz.columns.str.replace(' ', '_').str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
        "\n",
        "             # Aplicar imputación a categóricas si es necesario (para countplot)\n",
        "             categorical_cols_original = df_original_for_viz.select_dtypes(include='object').columns.tolist()\n",
        "             for col in categorical_cols_original:\n",
        "                 if df_original_for_viz[col].isnull().any():\n",
        "                    df_original_for_viz[col].fillna('Missing', inplace=True)\n",
        "\n",
        "         else:\n",
        "            # Si df_processed aún tiene columnas 'object', úsalo (asumiendo que ya fueron imputadas)\n",
        "            df_original_for_viz = df_processed # Renombrar para claridad en esta sección\n",
        "\n",
        "\n",
        "         categorical_cols_to_viz = df_original_for_viz.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "         if not categorical_cols_to_viz:\n",
        "             st.info(\"No hay columnas categóricas (tipo 'object') disponibles para graficar.\")\n",
        "         else:\n",
        "            # Permitir al usuario seleccionar qué columnas categóricas visualizar\n",
        "            selected_categorical_cols = st.multiselect(\"Selecciona variables categóricas para visualizar:\",\n",
        "                                                       categorical_cols_to_viz,\n",
        "                                                       default=categorical_cols_to_viz[:5]) # Seleccionar las primeras 5 por defecto\n",
        "\n",
        "            if selected_categorical_cols:\n",
        "                # Determinar el número de filas y columnas para la cuadrícula de gráficos\n",
        "                num_cols_cat_plot = 2\n",
        "                num_rows_cat_plot = int(np.ceil(len(selected_categorical_cols) / num_cols_cat_plot))\n",
        "\n",
        "                fig_cat, axes_cat = plt.subplots(num_rows_cat_plot, num_cols_cat_plot, figsize=(15, 5 * num_rows_cat_plot))\n",
        "                axes_cat = axes_cat.flatten() if num_rows_cat_plot > 1 else [axes_cat]\n",
        "\n",
        "                for i, col in enumerate(selected_categorical_cols):\n",
        "                    if col in df_original_for_viz.columns and df_original_for_viz[col].dtype == 'object':\n",
        "                        # Usar countplot para mostrar la frecuencia de cada categoría\n",
        "                        sns.countplot(y=col, data=df_original_for_viz, order=df_original_for_viz[col].value_counts().index, ax=axes_cat[i])\n",
        "                        axes_cat[i].set_title(f'Distribución de {col}')\n",
        "                        axes_cat[i].set_xlabel('Frecuencia')\n",
        "                        axes_cat[i].set_ylabel(col)\n",
        "                    else:\n",
        "                        st.warning(f\"La columna '{col}' no es categórica o no existe en el DataFrame para gráfica de barras.\")\n",
        "                        axes_cat[i].set_visible(False) # Ocultar el subplot vacío\n",
        "\n",
        "                # Ocultar subplots vacíos\n",
        "                for j in range(i + 1, len(axes_cat)):\n",
        "                    axes_cat[j].set_visible(False)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                st.pyplot(fig_cat)\n",
        "            else:\n",
        "                st.info(\"Por favor, selecciona al menos una columna categórica para visualizar.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        st.warning(\"No se pudo cargar el DataFrame original para visualizaciones categóricas.\")\n",
        "\n",
        "elif viz_option == \"Distribución de Variables Numéricas\":\n",
        "    st.subheader(\"Distribución de Variables Numéricas\")\n",
        "    numeric_cols_to_viz = df_processed.select_dtypes(include=np.number).columns.tolist()\n",
        "    # Excluir SalePrice para esta visualización si se muestra por separado\n",
        "    if 'SalePrice' in numeric_cols_to_viz:\n",
        "        numeric_cols_to_viz.remove('SalePrice')\n",
        "\n",
        "    if not numeric_cols_to_viz:\n",
        "        st.info(\"No hay columnas numéricas disponibles para graficar (excluyendo SalePrice).\")\n",
        "    else:\n",
        "         selected_numeric_cols = st.multiselect(\"Selecciona variables numéricas para visualizar:\",\n",
        "                                               numeric_cols_to_viz,\n",
        "                                               default=numeric_cols_to_viz[:5]) # Seleccionar las primeras 5 por defecto\n",
        "\n",
        "         if selected_numeric_cols:\n",
        "             # Determinar el número de filas y columnas para la cuadrícula de gráficos\n",
        "             num_cols_num_plot = 3\n",
        "             num_rows_num_plot = int(np.ceil(len(selected_numeric_cols) / num_cols_num_plot)) * 2 # Dos gráficos por columna\n",
        "\n",
        "             fig_num, axes_num = plt.subplots(num_rows_num_plot, num_cols_num_plot, figsize=(15, 5 * num_rows_num_plot/2))\n",
        "             axes_num = axes_num.flatten() if num_rows_num_plot > 1 else [axes_num]\n",
        "\n",
        "             plot_index = 0\n",
        "             for i, col in enumerate(selected_numeric_cols):\n",
        "                 if col in df_processed.columns and df_processed[col].dtype in [np.number]:\n",
        "                     # Histograma\n",
        "                     sns.histplot(df_processed[col], kde=True, ax=axes_num[plot_index])\n",
        "                     axes_num[plot_index].set_title(f'Distribución de {col}')\n",
        "                     axes_num[plot_index].set_xlabel(col)\n",
        "                     axes_num[plot_index].set_ylabel('Frecuencia')\n",
        "                     plot_index += 1\n",
        "\n",
        "                     # Boxplot\n",
        "                     sns.boxplot(y=df_processed[col], ax=axes_num[plot_index])\n",
        "                     axes_num[plot_index].set_title(f'Boxplot de {col}')\n",
        "                     axes_num[plot_index].set_ylabel(col)\n",
        "                     plot_index += 1\n",
        "                 else:\n",
        "                     st.warning(f\"La columna '{col}' no es numérica o no existe en el DataFrame.\")\n",
        "                     # Ocultar los dos subplots correspondientes\n",
        "                     axes_num[plot_index].set_visible(False)\n",
        "                     axes_num[plot_index+1].set_visible(False)\n",
        "                     plot_index += 2\n",
        "\n",
        "\n",
        "             # Ocultar subplots vacíos\n",
        "             for j in range(plot_index, len(axes_num)):\n",
        "                 axes_num[j].set_visible(False)\n",
        "\n",
        "             plt.tight_layout()\n",
        "             st.pyplot(fig_num)\n",
        "         else:\n",
        "             st.info(\"Por favor, selecciona al menos una columna numérica para visualizar.\")\n",
        "\n",
        "\n",
        "\n",
        "# Separar datos para modelado (si aún no se ha hecho en preprocess_data)\n",
        "if 'SalePrice' in df_processed.columns:\n",
        "     X = df_processed.drop('SalePrice', axis=1)\n",
        "     y = df_processed['SalePrice']\n",
        "     # Asegurarse de que X es numérico después del get_dummies si preprocess_data no lo garantizó\n",
        "     X = X.select_dtypes(include=np.number)\n",
        "     # Asegurarse de que las filas coinciden (esto debería estar cubierto por la imputación sin NaNs)\n",
        "     # X = X.dropna()\n",
        "     # y = y.loc[X.index]\n",
        "else:\n",
        "    st.error(\"La columna 'SalePrice' es necesaria para el modelado.\")\n",
        "    st.stop()\n",
        "\n",
        "\n",
        "# División en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # random_state para reproducibilidad\n",
        "\n",
        "st.sidebar.subheader(\"Preparación de Datos\")\n",
        "st.sidebar.write(f\"Dimensiones de X_train: {X_train.shape}\")\n",
        "st.sidebar.write(f\"Dimensiones de X_test: {X_test.shape}\")\n",
        "st.sidebar.write(f\"Dimensiones de y_train: {y_train.shape}\")\n",
        "st.sidebar.write(f\"Dimensiones de y_test: {y_test.shape}\")\n",
        "\n",
        "\n",
        "# Configurar KFold para la validación cruzada\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42) # shuffle=True y random_state para mezclar los datos consistentemente\n",
        "\n",
        "# Definir métricas de evaluación\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    epsilon = 1e-8\n",
        "    return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
        "\n",
        "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
        "r2_scorer = make_scorer(r2_score, greater_is_better=True)\n",
        "mape_scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
        "\n",
        "scoring = {\n",
        "    'mae': mae_scorer,\n",
        "    'mse': mse_scorer,\n",
        "    'r2': r2_scorer,\n",
        "    'mape': mape_scorer\n",
        "}\n",
        "scoring_optimizer = {'mae': mae_scorer} # Usar MAE para la optimización\n",
        "\n",
        "\n",
        "st.sidebar.subheader(\"Modelización\")\n",
        "selected_models = st.sidebar.multiselect(\"Selecciona modelos para entrenar y evaluar:\",\n",
        "                                         ['Linear Regression', 'Lasso', 'ElasticNet',\n",
        "                                          'KernelRidge', 'SGDRegressor', 'BayesianRidge',\n",
        "                                          'RandomForestRegressor', 'SVR'],\n",
        "                                         default=['Linear Regression']) # Modelo por defecto\n",
        "\n",
        "run_model_training = st.sidebar.button(\"Entrenar Modelos Seleccionados\")\n",
        "\n",
        "model_results = {}\n",
        "\n",
        "if run_model_training and selected_models:\n",
        "    st.subheader(\"Entrenamiento y Evaluación de Modelos\")\n",
        "    progress_bar = st.progress(0)\n",
        "    status_text = st.empty()\n",
        "\n",
        "    total_models = len(selected_models)\n",
        "    model_count = 0\n",
        "\n",
        "    for model_name in selected_models:\n",
        "        model_count += 1\n",
        "        status_text.text(f\"Entrenando modelo: {model_name} ({model_count}/{total_models})\")\n",
        "        progress_bar.progress(model_count / total_models)\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Inicializar modelo y definir espacios de búsqueda/parámetros si se usa optimización\n",
        "        if model_name == 'Linear Regression':\n",
        "            model = LinearRegression()\n",
        "            # Sin optimización de hiperparámetros para LR base\n",
        "            cv_results = cross_validate(model, X_train, y_train, cv=kf, scoring=scoring)\n",
        "            best_estimator = model # No hay un \"mejor estimador\" de una búsqueda, solo el modelo entrenado\n",
        "\n",
        "        elif model_name == 'Lasso':\n",
        "             model = Lasso(random_state=42, max_iter=10000)\n",
        "             # Usar BayesSearchCV para optimización (ejemplo)\n",
        "             search_spaces = {'alpha': Real(0.001, 10, prior='log-uniform')}\n",
        "             optimizer = BayesSearchCV(estimator=model, search_spaces=search_spaces,\n",
        "                                      n_iter=50, scoring=scoring_optimizer, refit='mae',\n",
        "                                      cv=kf, n_jobs=-1, random_state=42)\n",
        "             optimizer.fit(X_train, y_train)\n",
        "             best_estimator = optimizer.best_estimator_\n",
        "             cv_results = cross_validate(best_estimator, X_train, y_train, cv=kf, scoring=scoring) # Evaluar el mejor con CV\n",
        "\n",
        "        elif model_name == 'ElasticNet':\n",
        "             model = ElasticNet(random_state=42, max_iter=10000)\n",
        "             # Usar BayesSearchCV para optimización (ejemplo)\n",
        "             search_spaces = {\n",
        "                 'alpha': Real(0.001, 1, prior='log-uniform'),\n",
        "                 'l1_ratio': Real(0, 1, prior='uniform')\n",
        "             }\n",
        "             optimizer = BayesSearchCV(estimator=model, search_spaces=search_spaces,\n",
        "                                      n_iter=50, scoring=scoring_optimizer, refit='mae',\n",
        "                                      cv=kf, n_jobs=-1, random_state=42)\n",
        "             optimizer.fit(X_train, y_train)\n",
        "             best_estimator = optimizer.best_estimator_\n",
        "             cv_results = cross_validate(best_estimator, X_train, y_train, cv=kf, scoring=scoring) # Evaluar el mejor con CV\n",
        "\n",
        "\n",
        "        elif model_name == 'KernelRidge':\n",
        "             model = KernelRidge(kernel='rbf')\n",
        "             # Usar BayesSearchCV para optimización (ejemplo)\n",
        "             search_spaces = {\n",
        "                 'alpha': Real(0.001, 1, prior='log-uniform'),\n",
        "                 'gamma': Real(0.001, 1, prior='log-uniform')\n",
        "             }\n",
        "             optimizer = BayesSearchCV(estimator=model, search_spaces=search_spaces,\n",
        "                                      n_iter=50, scoring=scoring_optimizer, refit='mae',\n",
        "                                      cv=kf, n_jobs=-1, random_state=42)\n",
        "             optimizer.fit(X_train, y_train)\n",
        "             best_estimator = optimizer.best_estimator_\n",
        "             cv_results = cross_validate(best_estimator, X_train, y_train, cv=kf, scoring=scoring) # Evaluar el mejor con CV\n",
        "\n",
        "        elif model_name == 'SGDRegressor':\n",
        "             model = SGDRegressor(learning_rate='invscaling', early_stopping=True,\n",
        "                                 random_state=42, max_iter=10000)\n",
        "             # Usar BayesSearchCV para optimización (ejemplo)\n",
        "             search_spaces = {'alpha': Real(0.0001, 0.01, prior='log-uniform')}\n",
        "             optimizer = BayesSearchCV(estimator=model, search_spaces=search_spaces,\n",
        "                                      n_iter=50, scoring=scoring_optimizer, refit='mae',\n",
        "                                      cv=kf, n_jobs=-1, random_state=42)\n",
        "             optimizer.fit(X_train, y_train)\n",
        "             best_estimator = optimizer.best_estimator_\n",
        "             cv_results = cross_validate(best_estimator, X_train, y_train, cv=kf, scoring=scoring) # Evaluar el mejor con CV\n",
        "\n",
        "        elif model_name == 'BayesianRidge':\n",
        "            model = BayesianRidge()\n",
        "             # Sin optimización de hiperparámetros para BR base\n",
        "            cv_results = cross_validate(model, X_train, y_train, cv=kf, scoring=scoring)\n",
        "            best_estimator = model # No hay un \"mejor estimador\" de una búsqueda, solo el modelo entrenado\n",
        "\n",
        "\n",
        "        elif model_name == 'RandomForestRegressor':\n",
        "             model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "             # Usar BayesSearchCV para optimización (ejemplo)\n",
        "             search_spaces = {\n",
        "                 'n_estimators': Integer(50, 150),\n",
        "                 'max_depth': Integer(5, 15),\n",
        "                 'min_samples_split': Integer(2, 10)\n",
        "             }\n",
        "             optimizer = BayesSearchCV(estimator=model, search_spaces=search_spaces,\n",
        "                                      n_iter=10, # Menos iteraciones para acelerar\n",
        "                                      scoring=scoring_optimizer, refit='mae',\n",
        "                                      cv=kf, n_jobs=-1, random_state=42)\n",
        "             optimizer.fit(X_train, y_train)\n",
        "             best_estimator = optimizer.best_estimator_\n",
        "             cv_results = cross_validate(best_estimator, X_train, y_train, cv=kf, scoring=scoring) # Evaluar el mejor con CV\n",
        "\n",
        "        elif model_name == 'SVR':\n",
        "             model = SVR(kernel='rbf')\n",
        "             # Usar BayesSearchCV para optimización (ejemplo)\n",
        "             search_spaces = {\n",
        "                 'C': Real(1e-1, 1e3, prior='log-uniform'),\n",
        "                 'epsilon': Real(1e-3, 1e0, prior='uniform'),\n",
        "                 'gamma': Real(1e-4, 1e0, prior='log-uniform')\n",
        "             }\n",
        "             optimizer = BayesSearchCV(estimator=model, search_spaces=search_spaces,\n",
        "                                      n_iter=50, # Menos iteraciones para acelerar\n",
        "                                      scoring=scoring_optimizer, refit='mae',\n",
        "                                      cv=kf, n_jobs=-1, random_state=42)\n",
        "             optimizer.fit(X_train, y_train)\n",
        "             best_estimator = optimizer.best_estimator_\n",
        "             cv_results = cross_validate(best_estimator, X_train, y_train, cv=kf, scoring=scoring) # Evaluar el mejor con CV\n",
        "\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "\n",
        "        # Guardar resultados\n",
        "        # MAE, MSE, MAPE son negativas por make_scorer, tomar el valor absoluto\n",
        "        model_results[model_name] = {\n",
        "            'MAE': -cv_results['test_mae'].mean(),\n",
        "            'MSE': -cv_results['test_mse'].mean(),\n",
        "            'R2': cv_results['test_r2'].mean(),\n",
        "            'MAPE': -cv_results['test_mape'].mean(),\n",
        "            'Time (s)': elapsed_time,\n",
        "            'Best Estimator': best_estimator # Guardar el mejor estimador encontrado\n",
        "        }\n",
        "        st.write(f\"--- Resultados para {model_name} ---\")\n",
        "        st.write(f\"  MAE Promedio: {model_results[model_name]['MAE']:.4f}\")\n",
        "        st.write(f\"  MSE Promedio: {model_results[model_name]['MSE']:.4f}\")\n",
        "        st.write(f\"  R2 Promedio: {model_results[model_name]['R2']:.4f}\")\n",
        "        st.write(f\"  MAPE Promedio: {model_results[model_name]['MAPE']:.4f}%\")\n",
        "        st.write(f\"  Tiempo de Entrenamiento: {model_results[model_name]['Time (s)']:.2f} segundos\")\n",
        "        if model_name in ['Lasso', 'ElasticNet', 'KernelRidge', 'SGDRegressor', 'RandomForestRegressor', 'SVR']:\n",
        "             st.write(f\"  Mejores Hiperparámetros (Optimizados): {optimizer.best_params_}\")\n",
        "\n",
        "\n",
        "    status_text.text(\"Entrenamiento completado!\")\n",
        "    progress_bar.empty() # Ocultar la barra de progreso\n",
        "\n",
        "\n",
        "    st.subheader(\"Comparación de Modelos\")\n",
        "    results_df = pd.DataFrame.from_dict(model_results, orient='index')\n",
        "\n",
        "    # Mostrar tabla de resultados\n",
        "    st.dataframe(results_df)\n",
        "\n",
        "    # Generar gráficas de comparación\n",
        "    metrics_to_plot = ['MAE', 'MSE', 'R2', 'MAPE']\n",
        "    for metric in metrics_to_plot:\n",
        "        fig_comp, ax_comp = plt.subplots(figsize=(12, 6))\n",
        "        ascending = True if metric in ['MAE', 'MSE', 'MAPE'] else False\n",
        "        results_df_sorted = results_df.sort_values(by=metric, ascending=ascending)\n",
        "\n",
        "        sns.barplot(x=results_df_sorted.index, y=metric, data=results_df_sorted, ax=ax_comp)\n",
        "        ax_comp.set_title(f'Comparación de Modelos por {metric}', fontsize=16)\n",
        "        ax_comp.set_xlabel(\"Modelo\", fontsize=12)\n",
        "        ax_comp.set_ylabel(metric, fontsize=12)\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig_comp)\n",
        "\n",
        "    # Visualizar importancia de características para modelos aplicables\n",
        "    st.subheader(\"Importancia de Características (para modelos aplicables)\")\n",
        "    models_with_importance = [m for m in selected_models if m in ['Linear Regression', 'Lasso', 'ElasticNet', 'SGDRegressor', 'BayesianRidge', 'RandomForestRegressor']]\n",
        "\n",
        "    if models_with_importance:\n",
        "        selected_model_importance = st.selectbox(\"Selecciona un modelo para ver la importancia de características:\",\n",
        "                                                models_with_importance)\n",
        "\n",
        "        if selected_model_importance:\n",
        "            best_model_importance = model_results[selected_model_importance]['Best Estimator']\n",
        "\n",
        "            if hasattr(best_model_importance, 'coef_'): # Linear models\n",
        "                 if isinstance(X_train, pd.DataFrame):\n",
        "                     coefficients = pd.Series(best_model_importance.coef_, index=X_train.columns)\n",
        "                     sorted_coefficients = coefficients.abs().sort_values(ascending=False)\n",
        "\n",
        "                     fig_coef, ax_coef = plt.subplots(figsize=(10, min(20, len(sorted_coefficients) * 0.3)))\n",
        "                     sns.barplot(x=sorted_coefficients.values, y=sorted_coefficients.index, ax=ax_coef)\n",
        "                     ax_coef.set_title(f'Importancia de Características (Coeficientes Absolutos) - {selected_model_importance}', fontsize=14)\n",
        "                     ax_coef.set_xlabel('Magnitud del Coeficiente', fontsize=12)\n",
        "                     ax_coef.set_ylabel('Característica', fontsize=12)\n",
        "                     plt.tight_layout()\n",
        "                     st.pyplot(fig_coef)\n",
        "                 else:\n",
        "                      st.warning(\"X_train no es un DataFrame. No se puede mostrar la importancia de características basada en coeficientes.\")\n",
        "\n",
        "\n",
        "            elif hasattr(best_model_importance, 'feature_importances_'): # Tree-based models\n",
        "                 if isinstance(X_train, pd.DataFrame):\n",
        "                     importances = pd.Series(best_model_importance.feature_importances_, index=X_train.columns)\n",
        "                     sorted_importances = importances.sort_values(ascending=False)\n",
        "\n",
        "                     fig_feat_imp, ax_feat_imp = plt.subplots(figsize=(10, min(20, len(sorted_importances) * 0.3)))\n",
        "                     sns.barplot(x=sorted_importances.values, y=sorted_importances.index, ax=ax_feat_imp)\n",
        "                     ax_feat_imp.set_title(f'Importancia de Características - {selected_model_importance}', fontsize=14)\n",
        "                     ax_feat_imp.set_xlabel('Importancia', fontsize=12)\n",
        "                     ax_feat_imp.set_ylabel('Característica', fontsize=12)\n",
        "                     plt.tight_layout()\n",
        "                     st.pyplot(fig_feat_imp)\n",
        "                 else:\n",
        "                     st.warning(\"X_train no es un DataFrame. No se puede mostrar la importancia de características basada en feature_importances.\")\n",
        "            else:\n",
        "                st.info(f\"El modelo {selected_model_importance} no proporciona una medida estándar de importancia de características (coeficientes o feature_importances).\")\n",
        "    else:\n",
        "        st.info(\"Selecciona modelos lineales o basados en árboles para visualizar la importancia de características.\")\n",
        "\n",
        "elif run_model_training and not selected_models:\n",
        "    st.warning(\"Por favor, selecciona al menos un modelo para entrenar.\")\n"
      ],
      "metadata": {
        "id": "LhreGeYSmYYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Para los modelos de random, kernel y elasticnet crea Gráficas de barras para comparar MAE, MSE, R2 y MAPE con plt.bar(model_names, mae_values) y st.pyplot().\n",
        "# Una tabla con las métricas usando st.dataframe(pd.DataFrame(model_results).T).\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure the necessary metric variables from previous steps are available.\n",
        "# Assuming cv_results_bayes_krr, cv_results_bayes_en, cv_results_best_rf and their corresponding\n",
        "# mean metric values (cv_mae_bayes_krr.mean(), cv_mse_bayes_krr.mean(), etc.) are defined in\n",
        "# the preceding code blocks.\n",
        "\n",
        "# --- Comparison Graphs and Table for KernelRidge, ElasticNet, RandomForestRegressor ---\n",
        "\n",
        "# Define the model names and their corresponding metric values\n",
        "model_names = ['KernelRidge (Bayes)', 'ElasticNet (Bayes)', 'RandomForestRegressor (Bayes)']\n",
        "\n",
        "# Collect the metric values for the selected models\n",
        "mae_values = [\n",
        "    -cv_results_bayes_best_krr['test_mae'].mean(),\n",
        "    -cv_results_bayes_best_en['test_mae'].mean(),\n",
        "    -cv_results_best_rf['test_mae'].mean()\n",
        "]\n",
        "\n",
        "mse_values = [\n",
        "    -cv_results_bayes_best_krr['test_mse'].mean(),\n",
        "    -cv_results_bayes_best_en['test_mse'].mean(),\n",
        "    -cv_results_best_rf['test_mse'].mean()\n",
        "]\n",
        "\n",
        "r2_values = [\n",
        "    cv_results_bayes_best_krr['test_r2'].mean(),\n",
        "    cv_results_bayes_best_en['test_r2'].mean(),\n",
        "    cv_results_best_rf['test_r2'].mean()\n",
        "]\n",
        "\n",
        "mape_values = [\n",
        "    -cv_results_bayes_best_krr['test_mape'].mean(),\n",
        "    -cv_results_bayes_best_en['test_mape'].mean(),\n",
        "    -cv_results_best_rf['test_mape'].mean()\n",
        "]\n",
        "\n",
        "\n",
        "# Create a dictionary for the results table\n",
        "model_comparison_results = {\n",
        "    'KernelRidge (Bayes)': {\n",
        "        'MAE': mae_values[0],\n",
        "        'MSE': mse_values[0],\n",
        "        'R2': r2_values[0],\n",
        "        'MAPE': mape_values[0]\n",
        "    },\n",
        "    'ElasticNet (Bayes)': {\n",
        "        'MAE': mae_values[1],\n",
        "        'MSE': mse_values[1],\n",
        "        'R2': r2_values[1],\n",
        "        'MAPE': mape_values[1]\n",
        "    },\n",
        "    'RandomForestRegressor (Bayes)': {\n",
        "        'MAE': mae_values[2],\n",
        "        'MSE': mse_values[2],\n",
        "        'R2': r2_values[2],\n",
        "        'MAPE': mape_values[2]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Create a DataFrame for the table\n",
        "model_comparison_df = pd.DataFrame(model_comparison_results).T\n",
        "\n",
        "# Display the comparison table using Streamlit\n",
        "st.subheader(\"Comparación de Métricas (KernelRidge, ElasticNet, RandomForestRegressor)\")\n",
        "st.dataframe(model_comparison_df)\n",
        "\n",
        "# Create and display bar charts for each metric using Streamlit\n",
        "metrics_to_plot = ['MAE', 'MSE', 'R2', 'MAPE']\n",
        "y_labels = {'MAE': 'MAE', 'MSE': 'MSE', 'R2': 'R2 Score', 'MAPE': 'MAPE (%)'}\n",
        "titles = {\n",
        "    'MAE': 'Comparación de MAE',\n",
        "    'MSE': 'Comparación de MSE',\n",
        "    'R2': 'Comparación de R2 Score',\n",
        "    'MAPE': 'Comparación de MAPE'\n",
        "}\n",
        "ascending = {'MAE': True, 'MSE': True, 'R2': False, 'MAPE': True} # Lower is better for MAE, MSE, MAPE; Higher is better for R2\n",
        "\n",
        "for metric in metrics_to_plot:\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    # Sort values for better visualization (optional but recommended)\n",
        "    sorted_df = model_comparison_df.sort_values(by=metric, ascending=ascending[metric])\n",
        "    ax.bar(sorted_df.index, sorted_df[metric])\n",
        "    ax.set_title(titles[metric], fontsize=16)\n",
        "    ax.set_xlabel(\"Modelo\", fontsize=12)\n",
        "    ax.set_ylabel(y_labels[metric], fontsize=12)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "# prompt: Para los modelos de random, kernel y elasticnet crea Gráficas de barras para comparar MAE, MSE, R2 y MAPE con plt.bar(model_names, mae_values) y st.pyplot().\n",
        "# Una tabla con las métricas usando st.dataframe(pd.DataFrame(model_results).T).\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure the necessary metric variables from previous steps are available.\n",
        "# Assuming cv_results_bayes_krr, cv_results_bayes_en, cv_results_best_rf and their corresponding\n",
        "# mean metric values (cv_mae_bayes_krr.mean(), cv_mse_bayes_krr.mean(), etc.) are defined in\n",
        "# the preceding code blocks.\n",
        "\n",
        "# --- Comparison Graphs and Table for KernelRidge, ElasticNet, RandomForestRegressor ---\n",
        "\n",
        "# Define the model names and their corresponding metric values\n",
        "model_names = ['KernelRidge (Bayes)', 'ElasticNet (Bayes)', 'RandomForestRegressor (Bayes)']\n",
        "\n",
        "# Collect the metric values for the selected models\n",
        "mae_values = [\n",
        "    -cv_results_bayes_best_krr['test_mae'].mean(),\n",
        "    -cv_results_bayes_best_en['test_mae'].mean(),\n",
        "    -cv_results_best_rf['test_mae'].mean()\n",
        "]\n",
        "\n",
        "mse_values = [\n",
        "    -cv_results_bayes_best_krr['test_mse'].mean(),\n",
        "    -cv_results_bayes_best_en['test_mse'].mean(),\n",
        "    -cv_results_best_rf['test_mse'].mean()\n",
        "]\n",
        "\n",
        "r2_values = [\n",
        "    cv_results_bayes_best_krr['test_r2'].mean(),\n",
        "    cv_results_bayes_best_en['test_r2'].mean(),\n",
        "    cv_results_best_rf['test_r2'].mean()\n",
        "]\n",
        "\n",
        "mape_values = [\n",
        "    -cv_results_bayes_best_krr['test_mape'].mean(),\n",
        "    -cv_results_bayes_best_en['test_mape'].mean(),\n",
        "    -cv_results_best_rf['test_mape'].mean()\n",
        "]\n",
        "\n",
        "\n",
        "# Create a dictionary for the results table\n",
        "model_comparison_results = {\n",
        "    'KernelRidge (Bayes)': {\n",
        "        'MAE': mae_values[0],\n",
        "        'MSE': mse_values[0],\n",
        "        'R2': r2_values[0],\n",
        "        'MAPE': mape_values[0]\n",
        "    },\n",
        "    'ElasticNet (Bayes)': {\n",
        "        'MAE': mae_values[1],\n",
        "        'MSE': mse_values[1],\n",
        "        'R2': r2_values[1],\n",
        "        'MAPE': mape_values[1]\n",
        "    },\n",
        "    'RandomForestRegressor (Bayes)': {\n",
        "        'MAE': mae_values[2],\n",
        "        'MSE': mse_values[2],\n",
        "        'R2': r2_values[2],\n",
        "        'MAPE': mape_values[2]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Create a DataFrame for the table\n",
        "model_comparison_df = pd.DataFrame(model_comparison_results).T\n",
        "\n",
        "# Display the comparison table using Streamlit\n",
        "st.subheader(\"Comparación de Métricas (KernelRidge, ElasticNet, RandomForestRegressor)\")\n",
        "st.dataframe(model_comparison_df)\n",
        "\n",
        "# Create and display bar charts for each metric using Streamlit\n",
        "metrics_to_plot = ['MAE', 'MSE', 'R2', 'MAPE']\n",
        "y_labels = {'MAE': 'MAE', 'MSE': 'MSE', 'R2': 'R2 Score', 'MAPE': 'MAPE (%)'}\n",
        "titles = {\n",
        "    'MAE': 'Comparación de MAE',\n",
        "    'MSE': 'Comparación de MSE',\n",
        "    'R2': 'Comparación de R2 Score',\n",
        "    'MAPE': 'Comparación de MAPE'\n",
        "}\n",
        "ascending = {'MAE': True, 'MSE': True, 'R2': False, 'MAPE': True} # Lower is better for MAE, MSE, MAPE; Higher is better for R2\n",
        "\n",
        "for metric in metrics_to_plot:\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    # Sort values for better visualization (optional but recommended)\n",
        "    sorted_df = model_comparison_df.sort_values(by=metric, ascending=ascending[metric])\n",
        "    ax.bar(sorted_df.index, sorted_df[metric])\n",
        "    ax.set_title(titles[metric], fontsize=16)\n",
        "    ax.set_xlabel(\"Modelo\", fontsize=12)\n",
        "    ax.set_ylabel(y_labels[metric], fontsize=12)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    st.pyplot(fig)\n"
      ],
      "metadata": {
        "id": "Disv2aJTmulG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64\n",
        "!mv cloudflared-linux-amd64 /usr/local/bin/cloudflared\n",
        "\n",
        "#Ejecutar Streamlit\n",
        "!streamlit run TAM.py &>/content/logs.txt & #Cambiar 0_👋_Hello.py por el nombre de tu archivo principal\n",
        "\n",
        "#Exponer el puerto 8501 con Cloudflare Tunnel\n",
        "!cloudflared tunnel --url http://localhost:8501 > /content/cloudflared.log 2>&1 &\n",
        "\n",
        "#Leer la URL pública generada por Cloudflare\n",
        "import time\n",
        "time.sleep(5)  # Esperar que se genere la URL\n",
        "\n",
        "import re\n",
        "found_context = False  # Indicador para saber si estamos en la sección correcta\n",
        "\n",
        "with open('/content/cloudflared.log') as f:\n",
        "    for line in f:\n",
        "        #Detecta el inicio del contexto que nos interesa\n",
        "        if \"Your quick Tunnel has been created\" in line:\n",
        "            found_context = True\n",
        "\n",
        "        #Busca una URL si ya se encontró el contexto relevante\n",
        "        if found_context:\n",
        "            match = re.search(r'https?://\\S+', line)\n",
        "            if match:\n",
        "                url = match.group(0)  #Extrae la URL encontrada\n",
        "                print(f'Tu aplicación está disponible en: {url}')\n",
        "                break  #Termina el bucle después de encontrar la URL"
      ],
      "metadata": {
        "id": "y77Xhwx9l4Rp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}