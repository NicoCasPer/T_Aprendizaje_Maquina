{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1wtnL0Ir9xs8-BmzcLENbDFs_ZF6Nvdfc",
      "authorship_tag": "ABX9TyMlqtpleTWxtoIZ/tBV/+AS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NicoCasPer/T_Aprendizaje_Maquina/blob/main/Parcial1_TAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pregunta 1\n",
        "\n",
        "\n",
        "\n",
        "**Formulaciones Matemáticas:**  \n",
        "- **OLS:** Minimiza la suma de cuadrados de residuos, con solución $\\hat{\\beta} = (X^T X)^{-1} X^T y$.  \n",
        "- **Ridge:** Añade penalización L2, solución $\\hat{\\beta} = (X^T X + \\lambda I)^{-1} X^T y$.  \n",
        "- **Lasso:** Penalización L1, resuelto iterativamente, selecciona características.  \n",
        "- **MLE:** Maximiza la verosimilitud, equivalente a OLS con errores normales.  \n",
        "- **MAP:** Incorpora prior, como Ridge con prior normal.  \n",
        "- **Bayesian Linear:** Usa distribución posterior, permite incertidumbre.  \n",
        "- **Kernel Ridge:** Usa kernels para no linealidades, solución $\\hat{\\alpha} = (K + \\lambda I)^{-1} y$.  \n",
        "- **Gaussian Processes:** Modelo bayesiano no paramétrico, predice con distribución normal.  \n",
        "\n",
        "**Diferencias y Similitudes:**  \n",
        "- Los modelos frecuentistas (OLS, MLE) dan estimaciones puntuales, mientras que los bayesianos (MAP, Bayesian, GPs) ofrecen incertidumbre.  \n",
        "- Kernel Ridge y GPs capturan no linealidades, a diferencia de OLS o Ridge.  \n",
        "\n",
        "---\n",
        "\n",
        "#### **Demostraciones Matemáticas Paso a Paso**\n",
        "\n",
        "A continuación, se presentan las formulaciones matemáticas, los problemas de optimización y las soluciones para cada modelo, explicando cada paso de manera detallada.\n",
        "\n",
        "##### **1. Least Squares (OLS)**\n",
        "\n",
        "**Formulación Matemática:**  \n",
        "El modelo lineal se expresa como:\n",
        "\\begin{equation}\n",
        " y = X\\beta + \\epsilon\n",
        "\\end{equation}\n",
        "donde:  \n",
        "- $y$ es el vector de respuestas observadas $n \\times 1$,  \n",
        "- $X$ es la matriz de diseño $n \\times p$,  \n",
        "- $\\beta$ es el vector de parámetros $p \\times 1$,  \n",
        "- $\\epsilon$ es el vector de errores $n \\times 1$, asumido i.i.d. con media 0 y varianza $\\sigma^2$.\n",
        "\n",
        "El objetivo es minimizar la suma de los cuadrados de los residuos:\n",
        "\\begin{equation}\n",
        " \\text{RSS}(\\beta) = \\| y - X\\beta \\|^2 = (y - X\\beta)^T (y - X\\beta)\n",
        "\\end{equation}\n",
        "\n",
        "**Problema de Optimización:**  \n",
        "Minimizar:  \n",
        "\\begin{equation}\n",
        " \\min_{\\beta} (y - X\\beta)^T (y - X\\beta)\n",
        "\\end{equation}\n",
        "\n",
        "**Solución Paso a Paso:**  \n",
        "1. Expandimos el RSS:\n",
        "\\begin{equation}\n",
        "    \\text{RSS}(\\beta) = y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta   \n",
        "\\end{equation}\n",
        "\n",
        "   Esto es una función cuadrática en $\\beta$, y para minimizarla, tomamos la derivada con respecto a $\\beta$ y la igualamos a cero.  \n",
        "2. La derivada es:  \n",
        "\\begin{equation}\n",
        "    \\frac{\\partial \\text{RSS}}{\\partial \\beta} = -2X^T y + 2X^T X \\beta = 0   \n",
        "\\end{equation}\n",
        "\n",
        "3. Simplificamos:  \n",
        "\\begin{equation}\n",
        "    X^T X \\beta = X^T y   \n",
        "\\end{equation}\n",
        "\n",
        "4. Si $X^T X$ es invertible (asumiendo $n > p$ y $X$ de rango completo), resolvemos:\n",
        "\\begin{equation}\n",
        "    \\beta = (X^T X)^{-1} X^T y\n",
        "\\end{equation}  \n",
        "   Esta es la solución cerrada, conocida como la ecuación normal de OLS.\n",
        "\n",
        "**Notas:**  \n",
        "- OLS asume errores normales, homocedasticidad y ausencia de multicolinealidad perfecta.  \n",
        "- Si $p > n$, $X^T X$ no es invertible, y se requiere regularización.\n",
        "\n",
        "##### **2. Regularized Least Squares**\n",
        "\n",
        "Incluye Ridge, Lasso y Elastic Net, que añaden términos de penalización para controlar la complejidad.\n",
        "\n",
        "###### **a. Ridge Regression (L2 Regularization)**\n",
        "\n",
        "**Formulación Matemática:**  \n",
        "El objetivo es minimizar:\n",
        "\\begin{equation}  \n",
        " \\text{Ridge}(\\beta) = \\| y - X\\beta \\|^2 + \\lambda \\| \\beta \\|_2^2\n",
        "\\end{equation}\n",
        "\n",
        "donde $\\| \\beta \\|_2^2 = \\sum_{j=1}^p \\beta_j^2$ y $\\lambda > 0$ es el parámetro de regularización.\n",
        "\n",
        "**Problema de Optimización:**  \n",
        "Minimizar:  \n",
        "\\begin{equation}\n",
        " \\min_{\\beta} \\| y - X\\beta \\|^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\n",
        "\\end{equation}\n",
        "\n",
        "**Solución Paso a Paso:**  \n",
        "1. Expandimos:  \n",
        "\\begin{equation}\n",
        "    \\text{Ridge}(\\beta) = (y - X\\beta)^T (y - X\\beta) + \\lambda \\beta^T \\beta   \n",
        "\\end{equation}\n",
        "\n",
        "2. Tomamos la derivada con respecto a \\(\\beta\\):\n",
        "\\begin{equation}\n",
        "    \\frac{\\partial \\text{Ridge}}{\\partial \\beta} = -2X^T (y - X\\beta) + 2\\lambda \\beta = 0   \n",
        "\\end{equation}\n",
        "\n",
        "3. Reorganizamos:  \n",
        "\\begin{equation}\n",
        "    -2X^T y + 2X^T X \\beta + 2\\lambda \\beta = 0\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "    X^T X \\beta + \\lambda \\beta = X^T y   \n",
        "\\end{equation}\n",
        "\n",
        "4. Factorizamos:  \n",
        "\\begin{equation}\n",
        "    (X^T X + \\lambda I) \\beta = X^T y   \n",
        "\\end{equation}\n",
        "\n",
        "5. Resolviendo, asumiendo invertibilidad (siempre lo es con $\\lambda > 0$):  \n",
        "\\begin{equation}\n",
        "    \\beta = (X^T X + \\lambda I)^{-1} X^T y   \n",
        "\\end{equation}\n",
        "\n",
        "   Esto es la solución cerrada de Ridge, que siempre existe y es única.\n",
        "\n",
        "**Notas:**  \n",
        "- Ridge reduce la magnitud de los coeficientes, manejando multicolinealidad, pero no los reduce a cero.\n",
        "\n",
        "###### **b. Lasso Regression (L1 Regularization)**\n",
        "\n",
        "**Formulación Matemática:**  \n",
        "Minimizar:  \n",
        "\\begin{equation}\n",
        " \\text{Lasso}(\\beta) = \\| y - X\\beta \\|^2 + \\lambda \\| \\beta \\|_1   \n",
        "\\end{equation}\n",
        "\n",
        "donde $\\| \\beta \\|_1 = \\sum_{j=1}^p | \\beta_j |$.\n",
        "\n",
        "**Problema de Optimización:**  \n",
        "Minimizar:  \n",
        "\\begin{equation}\n",
        " \\min_{\\beta} \\| y - X\\beta \\|^2 + \\lambda \\sum_{j=1}^p | \\beta_j |\n",
        "\\end{equation}\n",
        "\n",
        "**Solución Paso a Paso:**  \n",
        "- Lasso no tiene solución cerrada debido a la norma L1, que es no diferenciable en 0.  \n",
        "- Se resuelve mediante métodos iterativos como descenso coordenado:  \n",
        "  1. Para cada $\\beta_j$, fijamos los demás y optimizamos, usando la regla de actualización:  \n",
        "  \\begin{equation}\n",
        "      \\beta_j \\leftarrow S(\\beta_j - \\eta \\frac{\\partial L}{\\partial \\beta_j}, \\lambda \\eta)   \n",
        "  \\end{equation}\n",
        "     donde\n",
        "  \\begin{equation}\n",
        "     S(z, \\lambda) = \\text{sign}(z) \\max(|z| - \\lambda, 0)\n",
        "  \\end{equation}\n",
        " es el operador soft-thresholding.  \n",
        "- Lasso puede reducir coeficientes a cero, permitiendo selección de características.\n",
        "\n",
        "###### **c. Elastic Net**\n",
        "\n",
        "**Formulación Matemática:**  \n",
        "Minimizar:  \n",
        "\\begin{equation}\n",
        " \\text{ElasticNet}(\\beta) = \\| y - X\\beta \\|^2 + \\lambda_1 \\| \\beta \\|_1 + \\lambda_2 \\| \\beta \\|_2^2\n",
        "\\end{equation}\n",
        "\n",
        "**Solución:**  \n",
        "Similar a Lasso, requiere métodos iterativos, combinando las propiedades de Ridge y Lasso.\n",
        "\n",
        "**Notas:**  \n",
        "- Regularized Least Squares son extensiones de OLS, con Ridge manejando multicolinealidad y Lasso seleccionando características.\n",
        "\n",
        "##### **3. Maximum Likelihood Estimation (MLE)**\n",
        "\n",
        "**Formulación Matemática:**  \n",
        "Asumimos $\\epsilon \\sim N(0, \\sigma^2 I)$, entonces:\n",
        "\\begin{equation}\n",
        " y | \\beta, \\sigma^2 \\sim N(X\\beta, \\sigma^2 I)   \n",
        "\\end{equation}\n",
        "\n",
        "La función de verosimilitud es:\n",
        "\\begin{equation}\n",
        " L(\\beta, \\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{(y_i - x_i^T \\beta)^2}{2\\sigma^2} \\right)   \n",
        "\\end{equation}\n",
        "\n",
        "La log-verosimilitud es:\n",
        "\\begin{equation}\n",
        " \\ell(\\beta, \\sigma^2) = -\\frac{n}{2} \\log(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} \\| y - X\\beta \\|^2\n",
        "\\end{equation}\n",
        "\n",
        "**Problema de Optimización:**  \n",
        "Maximizar:  \n",
        "\\begin{equation}\n",
        " \\max_{\\beta, \\sigma^2} -\\frac{n}{2} \\log(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} \\| y - X\\beta \\|^2\n",
        "\\end{equation}\n",
        "\n",
        "**Solución Paso a Paso:**  \n",
        "1. Fijamos $\\sigma^2$ y maximizamos con respecto a $\\beta$, lo que equivale a minimizar $\\| y - X\\beta \\|^2$, dando:\n",
        "\\begin{equation}\n",
        "    \\hat{\\beta} = (X^T X)^{-1} X^T y  (solución OLS).\n",
        "\\end{equation}  \n",
        "\n",
        "2. Sustituyendo $\\hat{\\beta}$, maximizamos con respecto a $\\sigma^2$:  \n",
        "   Derivamos $\\ell$ con respecto a $\\sigma^2$:  \n",
        "\\begin{equation}\n",
        "    \\frac{\\partial \\ell}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\| y - X\\hat{\\beta} \\|^2 = 0  \n",
        "\\end{equation}\n",
        "\n",
        "   Reorganizando:  \n",
        "\\begin{equation}\n",
        "    \\frac{n}{2\\sigma^2} = \\frac{1}{2(\\sigma^2)^2} \\| y - X\\hat{\\beta} \\|^2   \n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "    n (\\sigma^2)^2 = \\| y - X\\hat{\\beta} \\|^2 \\sigma^2   \n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "    n \\sigma^2 = \\| y - X\\hat{\\beta} \\|^2  \n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "    \\hat{\\sigma}^2 = \\frac{1}{n} \\| y - X\\hat{\\beta} \\|^2  \n",
        "\\end{equation}\n",
        "\n",
        "   En la práctica, usamos el estimador no sesgado:  \n",
        "\\begin{equation}\n",
        "    \\hat{\\sigma}^2 = \\frac{1}{n-p} \\| y - X\\hat{\\beta} \\|^2\n",
        "\\end{equation}\n",
        "\n",
        "**Notas:**  \n",
        "- MLE asume normalidad de errores, equivalente a OLS en este caso.\n",
        "\n",
        "##### **4. Maximum A-Posteriori (MAP)**\n",
        "\n",
        "**Formulación Matemática:**  \n",
        "Incorporamos un prior, por ejemplo, $\\beta \\sim N(0, \\tau^2 I)$. La posterior es:\n",
        "\\begin{equation}\n",
        " p(\\beta | y) \\propto p(y | \\beta) p(\\beta)   \n",
        "\\end{equation}\n",
        "\n",
        "Log-posterior:  \n",
        "\\begin{equation}\n",
        " \\log p(\\beta | y) \\propto -\\frac{1}{2\\sigma^2} \\| y - X\\beta \\|^2 - \\frac{1}{2\\tau^2} \\| \\beta \\|^2\n",
        "\\end{equation}\n",
        "\n",
        "**Problema de Optimización:**  \n",
        "Maximizar:  \n",
        "\\begin{equation}\n",
        " \\max_{\\beta} -\\frac{1}{2\\sigma^2} \\| y - X\\beta \\|^2 - \\frac{1}{2\\tau^2} \\| \\beta \\|^2  \n",
        "\\end{equation}\n",
        "\n",
        "Equivalente a minimizar:\n",
        "\\begin{equation}\n",
        " \\| y - X\\beta \\|^2 + \\frac{\\sigma^2}{\\tau^2} \\| \\beta \\|^2\n",
        "\\end{equation}\n",
        "\n",
        "**Solución:**  \n",
        "Esto es Ridge con $\\lambda = \\frac{\\sigma^2}{\\tau^2}$, solución:\n",
        "\\begin{equation}\n",
        " \\hat{\\beta} = (X^T X + \\lambda I)^{-1} X^T y\n",
        "\\end{equation}\n",
        "\n",
        "**Notas:**  \n",
        "- MAP incorpora información previa, como Ridge con prior normal.\n",
        "\n",
        "##### **5. Bayesian Linear Regression**\n",
        "\n",
        "**Formulación Matemática:**  \n",
        "Priors: $\\beta \\sim N(0, \\tau^2 I)$, $\\sigma^2 \\sim \\text{Inv-Gamma}(a, b)$.  \n",
        "La posterior de $\\beta | \\sigma^2, y$ es normal:  \n",
        "\\begin{equation}\n",
        " \\beta | \\sigma^2, y \\sim N(\\mu, \\Sigma)   \n",
        "\\end{equation}\n",
        "\n",
        "donde:  \n",
        "\\begin{equation}\n",
        " \\Sigma = (\\tau^{-2} I + \\sigma^{-2} X^T X)^{-1}, \\quad \\mu = \\Sigma (\\sigma^{-2} X^T y)  \n",
        "\\end{equation}\n",
        "\n",
        "Predicciones integran sobre la posterior.\n",
        "\n",
        "**Notas:**  \n",
        "- Proporciona incertidumbre, más intensivo computacionalmente.\n",
        "\n",
        "##### **6. Kernel Ridge Regression**\n",
        "\n",
        "**Formulación Matemática:**  \n",
        "Usa kernel $k(x, x')$, minimiza:  \n",
        "\\begin{equation}\n",
        " \\min_{\\alpha} \\| y - K\\alpha \\|^2 + \\lambda \\| \\alpha \\|^2  \n",
        "\\end{equation}\n",
        "\n",
        "Solución:  \n",
        "\\begin{equation}\n",
        " \\hat{\\alpha} = (K + \\lambda I)^{-1} y  \n",
        "\\end{equation}\n",
        "\n",
        "Predicción: $\\hat{y}_* = k(X, x_*) \\hat{\\alpha}$.\n",
        "\n",
        "**Notas:**  \n",
        "- Captura no linealidades, requiere elección de kernel.\n",
        "\n",
        "##### **7. Gaussian Processes**\n",
        "\n",
        "**Formulación Matemática:**  \n",
        "GP con media $m(x)$, kernel $k(x, x')$. Posterior:  \n",
        "- Media: $ \\mu_* = K(X_*, X) (K(X, X) + \\sigma^2 I)^{-1} y $  \n",
        "- Varianza: $ \\Sigma_* = K(X_*, X_*) - K(X_*, X) (K(X, X) + \\sigma^2 I)^{-1} K(X, X_*) $\n",
        "\n",
        "**Notas:**  \n",
        "- Bayesiano no paramétrico, permite incertidumbre, pero lento para grandes datos.\n",
        "\n",
        "#### **Diferencias y Similitudes**\n",
        "\n",
        "| **Modelo**              | **Enfoque**                     | **Optimización**                  | **Ventajas**                          | **Desventajas**                       |\n",
        "|--------------------------|---------------------------------|-----------------------------------|---------------------------------------|---------------------------------------|\n",
        "| **OLS**                 | Frecuentista, sin regularización | Minimizar RSS                    | Simple, interpretable                | Sensible a multicolinealidad, sobreajuste |\n",
        "| **Ridge**               | Regularizado (L2)              | Minimizar RSS + L2 penalización   | Maneja multicolinealidad, estable     | No selecciona características          |\n",
        "| **Lasso**               | Regularizado (L1)              | Minimizar RSS + L1 penalización   | Selecciona características            | No maneja bien multicolinealidad       |\n",
        "| **Elastic Net**         | Regularizado (L1 + L2)         | Minimizar RSS + L1 + L2          | Combina ventajas de Lasso y Ridge     | Más parámetros para ajustar            |\n",
        "| **MLE**                 | Frecuentista, verosimilitud    | Maximizar log-verosimilitud      | Asume distribución de errores         | Requiere suposiciones fuertes          |\n",
        "| **MAP**                 | Bayesiano, con prior           | Maximizar log-posterior          | Incorpora información previa          | Requiere elección de prior             |\n",
        "| **Bayesian Linear**     | Bayesiano, full posterior      | Integración sobre posterior       | Incertidumbre en parámetros           | Computacionalmente intensivo           |\n",
        "| **Kernel Ridge**        | No lineal, kernel              | Minimizar RSS en espacio kernel  | Captura no linealidades               | Requiere selección de kernel           |\n",
        "| **Gaussian Processes**  | Bayesiano, no paramétrico      | Integración sobre funciones       | Flexibilidad, incertidumbre           | Escalabilidad limitada                  |\n",
        "\n",
        "- **Similitudes:** Todos buscan minimizar error o maximizar verosimilitud/posterior. Modelos lineales (OLS, Ridge, Lasso, MLE, MAP, Bayesian) comparten estructura $y = X\\beta + \\epsilon$.  \n",
        "- **Diferencias:** Bayesianos (MAP, Bayesian, GPs) ofrecen incertidumbre, frecuentistas (OLS, MLE) estimaciones puntuales. Kernel Ridge y GPs capturan no linealidades, OLS y Ridge son lineales.\n",
        "\n",
        "Esta respuesta cumple con los requisitos del examen, proporcionando demostraciones completas y explicaciones paso a paso."
      ],
      "metadata": {
        "id": "-Nf_Rt94_zjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesamiento"
      ],
      "metadata": {
        "id": "oY4XutTndH89"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOw54MKXdo1P"
      },
      "outputs": [],
      "source": [
        "# prompt: Usa la biblioteca pandas para cargar el archivo CSV con pd.read_csv('AmesHousing.csv').\n",
        "# Revisa la estructura con df.head(), df.info() y df.describe() para identificar:\n",
        "# Número de filas y columnas.\n",
        "# Tipos de datos (numéricos, categóricos).\n",
        "# Valores faltantes iniciales.\n",
        "# Esto es crucial para el EDA y para planificar los pasos de preprocesamiento.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Cargar el archivo CSV\n",
        "df = pd.read_csv('/content/drive/Shareddrives/UNAL_Colab/Teoría de Aprendizaje de Máquina/AmesHousing.csv')\n",
        "\n",
        "print(\"\\nInformación del DataFrame:\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Usa la biblioteca pandas para cargar el archivo CSV con pd.read_csv('AmesHousing.csv').\n",
        "# Revisa la estructura con df.head(), df.info() y df.describe() para identificar:\n",
        "# Número de filas y columnas.\n",
        "# Tipos de datos (numéricos, categóricos).\n",
        "# Valores faltantes iniciales.\n",
        "# Esto es crucial para el EDA y para planificar los pasos de preprocesamiento.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "\n",
        "# Cargar el archivo CSV\n",
        "df = pd.read_csv('/content/drive/Shareddrives/UNAL_Colab/Teoría de Aprendizaje de Máquina/AmesHousing.csv')\n",
        "\n",
        "# prompt: Usa df.isnull().sum() para identificar columnas con valores faltantes. El dataset Ames Housing tiene varias, como LotFrontage, Alley, MasVnrType, etc.\n",
        "# Estrategias de imputación:\n",
        "# Para variables numéricas (e.g., LotFrontage, GarageYrBlt), imputa con la mediana usando SimpleImputer de sklearn con strategy='median'.\n",
        "# Para variables categóricas (e.g., Alley, MasVnrType), imputa con la moda (strategy='most_frequent') o crea una categoría \"missing\" (strategy='constant', fill_value='missing').\n",
        "# Justificación: La elección debe basarse en el contexto. Por ejemplo, LotFrontage (frente del lote) puede imputarse con la mediana si los valores faltantes son aleatorios, mientras que Alley (acceso a callejón) puede tener valores faltantes significativos (e.g., sin acceso), justificando una categoría \"missing\".\n",
        "\n",
        "\n",
        "# Identificar columnas con valores faltantes\n",
        "missing_values_count = df.isnull().sum()\n",
        "missing_columns = missing_values_count[missing_values_count > 0].index.tolist()\n",
        "\n",
        "print(\"\\nColumnas con valores faltantes y su conteo:\")\n",
        "print(missing_values_count[missing_columns])\n",
        "\n",
        "# Estrategia de imputación\n",
        "\n",
        "# Columnas numéricas a imputar con la mediana (ejemplo: LotFrontage, GarageYrBlt)\n",
        "numeric_cols_median = ['LotFrontage', 'GarageYrBlt'] # Agrega más columnas numéricas si es necesario\n",
        "\n",
        "# Columnas categóricas a imputar con la moda (ejemplo: MasVnrType, Electrical)\n",
        "categorical_cols_mode = ['MasVnrType', 'Electrical'] # Agrega más columnas categóricas si es necesario\n",
        "\n",
        "# Columnas categóricas a imputar con una categoría \"missing\" (ejemplo: Alley, Fence, MiscFeature, PoolQC, FireplaceQu)\n",
        "categorical_cols_missing = ['Alley', 'Fence', 'MiscFeature', 'PoolQC', 'FireplaceQu'] # Agrega más columnas categóricas si es necesario\n",
        "\n",
        "# Imputación para columnas numéricas (mediana)\n",
        "for col in numeric_cols_median:\n",
        "    if col in missing_columns:\n",
        "        imputer_median = SimpleImputer(strategy='median')\n",
        "        # Flatten the output of fit_transform\n",
        "        df[col] = imputer_median.fit_transform(df[[col]]).ravel()\n",
        "        print(f\"Imputada columna '{col}' con la mediana.\")\n",
        "\n",
        "# Imputación para columnas categóricas (moda)\n",
        "for col in categorical_cols_mode:\n",
        "     if col in missing_columns:\n",
        "        imputer_mode = SimpleImputer(strategy='most_frequent')\n",
        "        # Flatten the output of fit_transform\n",
        "        df[col] = imputer_mode.fit_transform(df[[col]]).ravel()\n",
        "        print(f\"Imputada columna '{col}' con la moda.\")\n",
        "\n",
        "# Imputación para columnas categóricas (\"missing\")\n",
        "for col in categorical_cols_missing:\n",
        "    if col in missing_columns:\n",
        "        imputer_missing = SimpleImputer(strategy='constant', fill_value='missing')\n",
        "        # Flatten the output of fit_transform\n",
        "        df[col] = imputer_missing.fit_transform(df[[col]]).ravel()\n",
        "        print(f\"Imputada columna '{col}' con 'missing'.\")\n",
        "\n",
        "# Verificar si aún hay valores faltantes (para otras columnas no consideradas en los ejemplos)\n",
        "missing_values_after_imputation = df.isnull().sum()\n",
        "remaining_missing_columns = missing_values_after_imputation[missing_values_after_imputation > 0].index.tolist()\n",
        "\n",
        "if len(remaining_missing_columns) > 0:\n",
        "    print(\"\\nColumnas que aún tienen valores faltantes después de la imputación:\")\n",
        "    print(missing_values_after_imputation[remaining_missing_columns])\n",
        "else:\n",
        "    print(\"\\n¡Imputación completada! No hay columnas con valores faltantes.\")\n",
        "\n",
        "# Mostrar información actualizada del DataFrame\n",
        "print(\"\\nInformación del DataFrame después de la imputación:\")\n",
        "df.info()"
      ],
      "metadata": {
        "id": "yf9RnsQHk9IN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Codificación de Variables Categóricas\n",
        "# Identifica columnas categóricas con df.select_dtypes(include=['object']).columns.\n",
        "# Tipos de codificación:\n",
        "# Para variables nominales (sin orden, e.g., Neighborhood, MSZoning), usa one-hot encoding con pd.get_dummies() o OneHotEncoder de sklearn.\n",
        "# Para variables ordinales (con orden, e.g., ExterQual, ExterCond, con valores como \"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\"), asigna valores numéricos basados en el orden (e.g., \"Ex\" = 5, \"Gd\" = 4, etc.) usando OrdinalEncoder o mapeo manual.\n",
        "# Consideración: One-hot encoding puede generar muchas columnas, lo que puede afectar la eficiencia computacional, especialmente para modelos como Gaussian Process. Considera reducir dimensionalidad si es necesario.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
        "\n",
        "# Identificar columnas categóricas\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "print(\"\\nColumnas categóricas identificadas:\")\n",
        "print(categorical_cols)\n",
        "\n",
        "# Separar columnas nominales y ordinales (ejemplo, necesitarás ajustar esto basado en tu conocimiento del dataset)\n",
        "# Para este ejemplo, asumiremos algunas columnas como nominales y otras como ordinales.\n",
        "# **Importante:** Debes verificar la naturaleza de cada columna categórica en el dataset real.\n",
        "\n",
        "# Columnas nominales (ejemplo)\n",
        "nominal_cols = ['MSZoning', 'Neighborhood', 'MasVnrType', 'Exterior1st', 'Exterior2nd', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'SaleType', 'SaleCondition']\n",
        "nominal_cols = [col for col in nominal_cols if col in categorical_cols] # Asegurarse de que existan en el df\n",
        "\n",
        "# Columnas ordinales (ejemplo)\n",
        "# Definir el orden para las variables ordinales. Esto es crucial y depende del conocimiento del dominio.\n",
        "# Ejemplo de orden para 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC'\n",
        "# Asumimos un orden general: 'Po' < 'Fa' < 'TA' < 'Gd' < 'Ex' < 'missing' (si fue imputada así)\n",
        "# Para 'missing', podemos decidir si es el valor más bajo o si tiene su propio tratamiento.\n",
        "# Aquí, si 'missing' fue usado en la imputación, lo incluimos en el orden si tiene un significado ordinal.\n",
        "# Si no tiene un orden natural, podrías tratarlo por separado o como parte de la imputación.\n",
        "# En este ejemplo, consideraremos 'missing' como el valor más bajo si se usó.\n",
        "\n",
        "# Definir el orden de las categorías para cada columna ordinal\n",
        "ordinal_mapping = {\n",
        "    'ExterQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex', 'missing'], # Ajusta 'missing' si aplica\n",
        "    'ExterCond': ['Po', 'Fa', 'TA', 'Gd', 'Ex', 'missing'], # Ajusta 'missing' si aplica\n",
        "    'BsmtQual': ['missing', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], # Ajusta 'missing' si aplica (por ejemplo, N/A = no basement)\n",
        "    'BsmtCond': ['missing', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], # Ajusta 'missing' si aplica\n",
        "    'HeatingQC': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
        "    'KitchenQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
        "    'FireplaceQu': ['missing', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], # Ajusta 'missing' si aplica\n",
        "    'GarageQual': ['missing', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], # Ajusta 'missing' si aplica\n",
        "    'GarageCond': ['missing', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], # Ajusta 'missing' si aplica\n",
        "    'PoolQC': ['missing', 'Fa', 'Gd', 'Ex'] # Ajusta 'missing' si aplica y el orden si es diferente\n",
        "    # Añade más columnas ordinales y sus órdenes aquí\n",
        "}\n",
        "\n",
        "ordinal_cols = list(ordinal_mapping.keys())\n",
        "ordinal_cols = [col for col in ordinal_cols if col in categorical_cols] # Asegurarse de que existan en el df\n",
        "\n",
        "# Columnas categóricas que no son ni nominales ni ordinales en esta definición (podrían ser otras categóricas o identificadores)\n",
        "other_categorical_cols = [col for col in categorical_cols if col not in nominal_cols and col not in ordinal_cols]\n",
        "print(\"\\nColumnas categóricas no clasificadas como nominales u ordinales (para revisión):\")\n",
        "print(other_categorical_cols)\n",
        "\n",
        "# Codificación One-Hot para variables nominales\n",
        "if nominal_cols:\n",
        "    print(f\"\\nAplicando One-Hot Encoding a las columnas: {nominal_cols}\")\n",
        "    # Usa drop='first' para evitar la multicolinealidad (elimina la primera categoría de cada columna)\n",
        "    df = pd.get_dummies(df, columns=nominal_cols, drop_first=True)\n",
        "    print(\"One-Hot Encoding aplicado.\")\n",
        "else:\n",
        "    print(\"\\nNo hay columnas nominales para One-Hot Encoding.\")\n",
        "\n",
        "# Codificación Ordinal para variables ordinales\n",
        "if ordinal_cols:\n",
        "    print(f\"\\nAplicando Ordinal Encoding a las columnas: {ordinal_cols}\")\n",
        "    # Crear el encoder con el orden de las categorías\n",
        "    ordinal_encoder = OrdinalEncoder(categories=[ordinal_mapping[col] for col in ordinal_cols])\n",
        "\n",
        "    # Aplicar la transformación\n",
        "    df[ordinal_cols] = ordinal_encoder.fit_transform(df[ordinal_cols])\n",
        "    print(\"Ordinal Encoding aplicado.\")\n",
        "else:\n",
        "    print(\"\\nNo hay columnas ordinales para Ordinal Encoding.\")\n",
        "\n",
        "# Mostrar información actualizada del DataFrame para ver los nuevos tipos de datos y columnas\n",
        "print(\"\\nInformación del DataFrame después de la codificación de variables categóricas:\")\n",
        "df.info()\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame para ver los resultados\n",
        "print(\"\\nPrimeras filas del DataFrame después de la codificación:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "26pLZYUgZRWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Escalado de Variables Numéricas\n",
        "# Usa StandardScaler o MinMaxScaler de sklearn para estandarizar las variables numéricas (e.g., GrLivArea, TotalBsmtSF).\n",
        "# Justificación: Modelos como Linear Regression, Lasso, ElasticNet, KernelRidge, SVR y SGDRegressor son sensibles a la escala de las características, por lo que el escalado es esencial para un rendimiento óptimo.\n",
        "# Nota: RandomForest y BayesianRidge no requieren escalado, pero aplicarlo uniformemente facilita la comparación.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# Identificar columnas numéricas que no son dummies de la codificación One-Hot\n",
        "# Excluir columnas que ya fueron transformadas (ordinales) y las variables target/ID si las hay\n",
        "numeric_cols_to_scale = df.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "# Excluir las columnas ordinales que ya fueron codificadas numéricamente\n",
        "numeric_cols_to_scale = [col for col in numeric_cols_to_scale if col not in ordinal_cols]\n",
        "\n",
        "# Excluir columnas que son resultados de One-Hot Encoding (son binarias y no necesitan escalado típico)\n",
        "# Una forma simple es excluir columnas que antes eran nominales, pero ahora son múltiples columnas.\n",
        "# Sin embargo, esto puede ser complicado si hay muchas columnas nuevas.\n",
        "# Una mejor aproximación es excluir la columna 'Id' si existe y la columna target 'SalePrice'.\n",
        "if 'Id' in numeric_cols_to_scale:\n",
        "    numeric_cols_to_scale.remove('Id')\n",
        "if 'SalePrice' in numeric_cols_to_scale: # Asumiendo 'SalePrice' es la variable target\n",
        "    numeric_cols_to_scale.remove('SalePrice')\n",
        "\n",
        "# Opcional: Puedes refinar esta lista manualmente si conoces qué columnas numéricas deben escalarse.\n",
        "# Por ejemplo: numeric_cols_to_scale = ['GrLivArea', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LotFrontage', 'LotArea', ...]\n",
        "\n",
        "print(f\"\\nColumnas numéricas a escalar: {numeric_cols_to_scale}\")\n",
        "\n",
        "# Selección del escalador: StandardScaler o MinMaxScaler\n",
        "# StandardScaler: Escala los datos para tener media 0 y desviación estándar 1. Útil para algoritmos sensibles a la distancia.\n",
        "# MinMaxScaler: Escala los datos a un rango específico (por defecto [0, 1]). Útil si necesitas que todos los features estén en el mismo rango positivo.\n",
        "\n",
        "# Elije uno de los dos:\n",
        "scaler = StandardScaler()\n",
        "# scaler = MinMaxScaler()\n",
        "\n",
        "# Aplicar el escalado a las columnas numéricas seleccionadas\n",
        "if numeric_cols_to_scale:\n",
        "    print(f\"\\nAplicando {type(scaler).__name__} a las columnas numéricas...\")\n",
        "    df[numeric_cols_to_scale] = scaler.fit_transform(df[numeric_cols_to_scale])\n",
        "    print(f\"Escalado aplicado usando {type(scaler).__name__}.\")\n",
        "else:\n",
        "    print(\"\\nNo hay columnas numéricas identificadas para escalar.\")\n",
        "\n",
        "# Mostrar información actualizada del DataFrame para ver los tipos de datos (seguirán siendo numéricos)\n",
        "print(\"\\nInformación del DataFrame después del escalado de variables numéricas:\")\n",
        "df.info()\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame para ver los resultados del escalado\n",
        "print(\"\\nPrimeras filas del DataFrame después del escalado:\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "Gt00-GvXZuxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Ingeniería de Características\n",
        "# Crea nuevas características relevantes, como:\n",
        "# TotalSF: Suma de TotalBsmtSF, 1stFlrSF y 2ndFlrSF para total de área habitable.\n",
        "# Age: Diferencia entre YrSold y YearBuilt para la edad de la vivienda.\n",
        "# Usa correlaciones para identificar y eliminar características redundantes (e.g., si GarageArea y GarageCars están altamente correlacionadas, considera quedarte con una).\n",
        "# Consideración: Usa técnicas como PCA si hay muchas características correlacionadas, pero esto es opcional y debe justificarse en el EDA.\n",
        "\n",
        "import numpy as np\n",
        "# Feature Engineering\n",
        "print(\"\\nRealizando Ingeniería de Características...\")\n",
        "\n",
        "# TotalSF: Suma de TotalBsmtSF, 1stFlrSF y 2ndFlrSF\n",
        "# Asegurarse de que las columnas existan y sean numéricas antes de sumar\n",
        "sf_cols = ['TotalBsmtSF', '1stFlrSF', '2ndFlrSF']\n",
        "# Verificar si todas las columnas necesarias existen y son numéricas\n",
        "if all(col in df.columns and df[col].dtype in [np.number] for col in sf_cols):\n",
        "    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n",
        "    print(\"Característica 'TotalSF' creada.\")\n",
        "else:\n",
        "    print(f\"No se pudo crear 'TotalSF'. Asegúrese de que las columnas {sf_cols} existan y sean numéricas.\")\n",
        "\n",
        "\n",
        "# Age: Diferencia entre YrSold y YearBuilt\n",
        "# Asegurarse de que las columnas existan y sean numéricas antes de restar\n",
        "age_cols = ['YrSold', 'YearBuilt']\n",
        "if all(col in df.columns and df[col].dtype in [np.number] for col in age_cols):\n",
        "    df['Age'] = df['YrSold'] - df['YearBuilt']\n",
        "    # Considerar casas vendidas antes de ser construidas (posibles errores de datos)\n",
        "    # Podríamos corregirlos o considerarlos como outliers. Aquí, simplemente los marcamos si existen.\n",
        "    if (df['Age'] < 0).any():\n",
        "        print(\"Advertencia: Se encontraron edades negativas en la característica 'Age'.\")\n",
        "    print(\"Característica 'Age' creada.\")\n",
        "else:\n",
        "     print(f\"No se pudo crear 'Age'. Asegúrese de que las columnas {age_cols} existan y sean numéricas.\")\n",
        "\n",
        "\n",
        "# Opcional: Crear otras características si son relevantes para el dominio\n",
        "# Ejemplo: Ratio entre áreas (ej. GrLivArea/TotalSF), número total de baños, etc.\n",
        "# df['Bathrooms'] = df['FullBath'] + 0.5 * df['HalfBath'] + df['BsmtFullBath'] + 0.5 * df['BsmtHalfBath']\n",
        "\n",
        "\n",
        "print(\"\\nIngeniería de Características completada.\")\n",
        "\n",
        "# Mostrar información actualizada del DataFrame para ver las nuevas columnas\n",
        "print(\"\\nInformación del DataFrame después de la Ingeniería de Características:\")\n",
        "df.info()\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame para ver los resultados\n",
        "print(\"\\nPrimeras filas del DataFrame después de la Ingeniería de Características:\")\n",
        "print(df.head())\n",
        "\n",
        "# Análisis de Correlaciones para identificar características redundantes\n",
        "print(\"\\nAnalizando correlaciones entre características...\")\n",
        "\n",
        "# Calcular la matriz de correlación. Excluir columnas no numéricas si aún existen.\n",
        "# Si ya todas las columnas relevantes son numéricas después de la codificación y escalado, usa todo el df.\n",
        "numeric_df = df.select_dtypes(include=np.number)\n",
        "correlation_matrix = numeric_df.corr()\n",
        "\n",
        "# Visualizar la matriz de correlación (opcional, requiere matplotlib/seaborn)\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# plt.figure(figsize=(12, 10))\n",
        "# sns.heatmap(correlation_matrix, cmap='coolwarm', annot=False) # annot=True si quieres ver los valores\n",
        "# plt.title('Matriz de Correlación')\n",
        "# plt.show()\n",
        "\n",
        "# Identificar pares de características altamente correlacionadas\n",
        "# Define un umbral de correlación (por ejemplo, 0.8 o 0.9)\n",
        "correlation_threshold = 0.8\n",
        "\n",
        "# Crear una máscara para la parte superior del triángulo de la matriz de correlación\n",
        "upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "# Encontrar las columnas con correlación absoluta alta\n",
        "to_drop_high_corr = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\n",
        "\n",
        "print(f\"\\nColumnas con alta correlación (> {correlation_threshold}) detectadas:\")\n",
        "print(to_drop_high_corr)\n",
        "\n",
        "# Decidir qué columna eliminar de los pares altamente correlacionados\n",
        "# Esto requiere conocimiento del dominio o una justificación (ej. quedarse con la que tiene mayor correlación con la variable target)\n",
        "# Para este ejemplo, simplemente mostraremos los pares y el usuario debe decidir cuál eliminar.\n",
        "\n",
        "high_corr_pairs = []\n",
        "for col1 in upper.columns:\n",
        "    for col2 in upper.index:\n",
        "        if col1 != col2 and abs(upper.loc[col2, col1]) > correlation_threshold:\n",
        "             high_corr_pairs.append((col2, col1, upper.loc[col2, col1]))\n",
        "\n",
        "print(\"\\nPares de características altamente correlacionadas y su valor:\")\n",
        "for pair in high_corr_pairs:\n",
        "    print(f\"{pair[0]} y {pair[1]}: {pair[2]:.2f}\")\n",
        "\n",
        "# Ejemplo de decisión (MANUAL): Si GarageArea y GarageCars están altamente correlacionadas,\n",
        "# podríamos decidir quedarnos con 'GarageCars' ya que representa el número de coches,\n",
        "# lo cual podría ser más directamente relacionado con el valor percibido.\n",
        "\n",
        "# Lista de columnas a considerar eliminar basadas en alta correlación (EJEMPLO - AJUSTAR SEGÚN ANÁLISIS)\n",
        "# Por ejemplo, si GarageArea y GarageCars son muy correlacionadas, podríamos añadir 'GarageArea' aquí.\n",
        "# Si TotalBsmtSF, 1stFlrSF, GrLivArea son muy correlacionadas con TotalSF, podríamos considerar eliminar algunas de las originales.\n",
        "# Este paso es CRUCIAL y debe basarse en el análisis de los pares identificados.\n",
        "\n",
        "# Ejemplo: Si 'GarageArea' y 'GarageCars' tienen alta correlación y decides eliminar 'GarageArea'\n",
        "# columns_to_potentially_drop = ['GarageArea'] # Inicializa la lista de columnas a considerar eliminar\n",
        "\n",
        "# Puedes iterar sobre los pares de alta correlación y aplicar una regla o decidir manualmente\n",
        "# for col1, col2, corr_value in high_corr_pairs:\n",
        "    # Decide cuál eliminar basándote en alguna métrica o dominio\n",
        "    # if col1 == 'GarageArea' and col2 == 'GarageCars':\n",
        "    #     columns_to_potentially_drop.append('GarageArea')\n",
        "    # elif col1 == 'GrLivArea' and col2 == 'TotalSF':\n",
        "    #      columns_to_potentially_drop.append('GrLivArea') # Ejemplo: quedarse con TotalSF si es más informativo\n",
        "\n",
        "# Eliminar duplicados de la lista (una columna puede estar en múltiples pares)\n",
        "# columns_to_potentially_drop = list(set(columns_to_potentially_drop))\n",
        "\n",
        "# print(f\"\\nColumnas que se considerarán eliminar por alta correlación: {columns_to_potentially_drop}\")\n",
        "\n",
        "# if columns_to_potentially_drop:\n",
        "#     # Asegurarse de que las columnas existen antes de intentar eliminarlas\n",
        "#     existing_cols_to_drop = [col for col in columns_to_potentially_drop if col in df.columns]\n",
        "#     if existing_cols_to_drop:\n",
        "#         print(f\"\\nEliminando columnas altamente correlacionadas: {existing_cols_to_drop}\")\n",
        "#         df = df.drop(columns=existing_cols_to_drop)\n",
        "#         print(\"Columnas eliminadas.\")\n",
        "#     else:\n",
        "#          print(\"\\nLas columnas a eliminar consideradas no existen en el DataFrame actual.\")\n",
        "# else:\n",
        "#     print(\"\\nNo hay columnas identificadas para eliminar por alta correlación (basado en el umbral y reglas de ejemplo).\")\n",
        "\n",
        "# Nota sobre PCA: PCA puede ser útil si hay muchas características numéricas correlacionadas y quieres reducir la dimensionalidad\n",
        "# manteniendo la mayor parte de la varianza. Sin embargo, los componentes de PCA son combinaciones lineales\n",
        "# de las características originales y son menos interpretables. Si el objetivo es la interpretabilidad,\n",
        "# la eliminación de columnas altamente correlacionadas es a menudo preferible.\n",
        "\n",
        "# PCA es una transformación, no una eliminación directa de columnas. Si se usa PCA,\n",
        "# se reemplazarían las columnas numéricas correlacionadas por menos componentes principales.\n",
        "# No se implementa PCA aquí, ya que es opcional y requiere justificación en el EDA.\n",
        "\n",
        "# Mostrar información final del DataFrame\n",
        "print(\"\\nInformación final del DataFrame después de la ingeniería de características y consideración de correlaciones:\")\n",
        "df.info()\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame final\n",
        "print(\"\\nPrimeras filas del DataFrame final:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "fhzAQ5gYZ9dB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Análisis Exploratorio de Datos (EDA)\n",
        "# Realiza visualizaciones clave:\n",
        "# Histogramas y boxplots para SalePrice y variables numéricas para identificar distribución y outliers.\n",
        "# Gráficos de barras para variables categóricas importantes (e.g., Neighborhood, OverallQual).\n",
        "# Matriz de correlación (e.g., con seaborn.heatmap) para identificar relaciones entre características y SalePrice.\n",
        "# Discute hallazgos, como:\n",
        "# Si SalePrice tiene sesgo, considera transformaciones logarítmicas.\n",
        "# Identifica outliers (e.g., casas con precios extremadamente altos o bajos) y decide si eliminarlos o manejarlos.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Visualizaciones clave\n",
        "\n",
        "# Histograma y Boxplot para SalePrice\n",
        "print(\"\\nVisualizando la distribución de SalePrice...\")\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Histograma de SalePrice\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df['SalePrice'], kde=True)\n",
        "plt.title('Distribución de SalePrice')\n",
        "plt.xlabel('SalePrice')\n",
        "plt.ylabel('Frecuencia')\n",
        "\n",
        "# Boxplot de SalePrice\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(y=df['SalePrice'])\n",
        "plt.title('Boxplot de SalePrice')\n",
        "plt.ylabel('SalePrice')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Discusión sobre SalePrice: Sesgo y Outliers\n",
        "print(\"\\nAnálisis de SalePrice:\")\n",
        "skewness = df['SalePrice'].skew()\n",
        "print(f\"Sesgo de SalePrice: {skewness:.2f}\")\n",
        "\n",
        "if abs(skewness) > 0.5: # Un umbral común para considerar un sesgo significativo\n",
        "    print(\"SalePrice parece tener un sesgo significativo.\")\n",
        "    print(\"Considerar una transformación logarítmica (ej. np.log1p) podría ser beneficioso para la modelización.\")\n",
        "\n",
        "# Identificar outliers en SalePrice usando IQR\n",
        "Q1 = df['SalePrice'].quantile(0.25)\n",
        "Q3 = df['SalePrice'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "outliers = df[(df['SalePrice'] < lower_bound) | (df['SalePrice'] > upper_bound)]\n",
        "print(f\"\\nSe encontraron {len(outliers)} outliers en SalePrice (usando el método IQR).\")\n",
        "print(\"Decisión sobre manejar o eliminar outliers debe basarse en el contexto y el modelo a usar.\")\n",
        "\n",
        "# Visualizaciones para variables numéricas (ejemplo: algunas de las importantes)\n",
        "numeric_cols_viz = ['GrLivArea', 'TotalBsmtSF', 'Age', 'TotalSF'] # Agrega otras columnas numéricas relevantes\n",
        "print(f\"\\nVisualizando distribuciones y outliers para algunas variables numéricas: {numeric_cols_viz}\")\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(numeric_cols_viz):\n",
        "    if col in df.columns and df[col].dtype in [np.number]:\n",
        "        plt.subplot(2, len(numeric_cols_viz), i + 1)\n",
        "        sns.histplot(df[col], kde=True)\n",
        "        plt.title(f'Distribución de {col}')\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel('Frecuencia')\n",
        "\n",
        "        plt.subplot(2, len(numeric_cols_viz), i + len(numeric_cols_viz) + 1)\n",
        "        sns.boxplot(y=df[col])\n",
        "        plt.title(f'Boxplot de {col}')\n",
        "        plt.ylabel(col)\n",
        "    else:\n",
        "        print(f\"Advertencia: La columna '{col}' no existe o no es numérica para visualización.\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualizaciones para variables categóricas importantes\n",
        "# Selecciona algunas variables categóricas interesantes para visualizar\n",
        "# Usa las columnas nominales y ordinales después de la imputación pero antes de la codificación\n",
        "# (ya que pd.get_dummies convierte las nominales en múltiples columnas)\n",
        "# Si ya ejecutaste la codificación, usa las columnas originales para esta visualización.\n",
        "# Asumiendo que las columnas originales están disponibles o usando las columnas ordinales codificadas y algunas nominales clave originales si las tienes\n",
        "# Si solo tienes el df codificado, puedes visualizar las ordinales (ahora numéricas) con boxplots o analizar las dummies.\n",
        "\n",
        "# Si quieres visualizar las columnas nominales originales, necesitas el df antes de get_dummies.\n",
        "# Para este ejemplo, asumiremos que podemos usar las columnas ordinales codificadas (ahora numéricas)\n",
        "# y tal vez algunas de las columnas originales si no se han perdido.\n",
        "# Si el código anterior ya corrió y transformó las columnas nominales, no podemos visualizarlas como barras fácilmente aquí.\n",
        "\n",
        "# Vamos a visualizar las columnas ordinales codificadas (que ahora son numéricas)\n",
        "ordinal_cols_viz = ordinal_cols # Usamos la lista de columnas ordinales definida anteriormente\n",
        "print(f\"\\nVisualizando la relación entre SalePrice y columnas ordinales: {ordinal_cols_viz}\")\n",
        "plt.figure(figsize=(15, 8))\n",
        "for i, col in enumerate(ordinal_cols_viz):\n",
        "    if col in df.columns and df[col].dtype in [np.number] and 'SalePrice' in df.columns:\n",
        "        plt.subplot(2, int(np.ceil(len(ordinal_cols_viz)/2)), i + 1)\n",
        "        # Boxplot para ver la relación entre la categoría ordinal y SalePrice\n",
        "        sns.boxplot(x=df[col], y=df['SalePrice'])\n",
        "        plt.title(f'SalePrice vs {col}')\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel('SalePrice')\n",
        "    else:\n",
        "         print(f\"Advertencia: La columna '{col}' o 'SalePrice' no existe o no es numérica para visualización ordinal.\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Visualizar la relación entre SalePrice y algunas columnas nominales CLAVE (si las tienes disponibles en un formato adecuado)\n",
        "# Si ya aplicaste One-Hot Encoding, la mejor manera es analizar la relación de SalePrice con los grupos de cada columna nominal original.\n",
        "# Ejemplo usando 'Neighborhood' (si la columna original está disponible o si trabajas con el df antes de one-hot)\n",
        "# Si 'Neighborhood' fue one-hoteada, necesitarías agrupar por el valor original.\n",
        "# Asumiendo que puedes acceder al dataframe antes de la codificación o tienes la columna original\n",
        "# Si no, este paso de visualización de categóricas con barras después de OHE es complicado.\n",
        "\n",
        "# **Alternativa (si ya aplicaste OHE):** Puedes analizar la relación entre SalePrice y los grupos de una variable categórica original\n",
        "# calculando la mediana de SalePrice por grupo.\n",
        "# Si tienes el dataframe original o una copia antes de la codificación:\n",
        "# df_original = pd.read_csv('/content/drive/Shareddrives/UNAL_Colab/Teoría de Aprendizaje de Máquina/AmesHousing.csv') # Cargar de nuevo o usar una copia\n",
        "# if 'Neighborhood' in df_original.columns and 'SalePrice' in df_original.columns:\n",
        "#     plt.figure(figsize=(15, 6))\n",
        "#     sns.boxplot(x='Neighborhood', y='SalePrice', data=df_original)\n",
        "#     plt.title('SalePrice por Neighborhood (original)')\n",
        "#     plt.xticks(rotation=90)\n",
        "#     plt.show()\n",
        "\n",
        "# Si no tienes el df original disponible fácilmente aquí, puedes saltarte la visualización de barras para nominales después de OHE.\n",
        "\n",
        "# Matriz de correlación con SalePrice\n",
        "print(\"\\nGenerando matriz de correlación con SalePrice...\")\n",
        "\n",
        "# Calcular la matriz de correlación solo para columnas numéricas\n",
        "numeric_df_for_corr = df.select_dtypes(include=np.number)\n",
        "\n",
        "# Calcular las correlaciones con SalePrice\n",
        "# Asegurarse de que SalePrice está en el DataFrame numérico\n",
        "if 'SalePrice' in numeric_df_for_corr.columns:\n",
        "    correlation_with_saleprice = numeric_df_for_corr.corr()['SalePrice'].sort_values(ascending=False)\n",
        "    print(\"\\nCorrelación de variables numéricas con SalePrice:\")\n",
        "    print(correlation_with_saleprice)\n",
        "\n",
        "    # Visualizar la matriz de correlación completa (puede ser grande)\n",
        "    # plt.figure(figsize=(15, 12))\n",
        "    # sns.heatmap(numeric_df_for_corr.corr(), cmap='coolwarm', annot=False) # annot=True puede ser muy denso\n",
        "    # plt.title('Matriz de Correlación de Variables Numéricas')\n",
        "    # plt.show()\n",
        "\n",
        "    # Visualizar solo las correlaciones con SalePrice (más legible)\n",
        "    plt.figure(figsize=(8, 10))\n",
        "    sns.heatmap(correlation_with_saleprice.to_frame(), cmap='coolwarm', annot=True, fmt=\".2f\", cbar=False)\n",
        "    plt.title('Correlación de Variables Numéricas con SalePrice')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"\\n'SalePrice' no se encontró en las columnas numéricas para calcular la correlación.\")\n",
        "\n",
        "\n",
        "# Discusión de hallazgos de correlación:\n",
        "print(\"\\nDiscusión de hallazgos de correlación:\")\n",
        "print(\"Las variables con los valores de correlación más altos (positivos o negativos) son las que tienen una relación lineal más fuerte con SalePrice.\")\n",
        "print(\"Ejemplos de variables con alta correlación positiva suelen ser 'OverallQual', 'GrLivArea', 'TotalBsmtSF', 'GarageCars'.\")\n",
        "print(\"Ejemplos de variables con alta correlación negativa pueden ser menos comunes, pero podrían existir.\")\n",
        "print(\"Considerar estas variables más correlacionadas como potencialmente importantes para el modelo.\")\n",
        "\n",
        "# Resumen general del EDA y próximos pasos\n",
        "print(\"\\nResumen del Análisis Exploratorio de Datos (EDA):\")\n",
        "print(\"- Se analizó la distribución de SalePrice, identificando un posible sesgo y outliers.\")\n",
        "print(\"- Se visualizaron distribuciones y outliers para variables numéricas clave.\")\n",
        "print(\"- Se analizó la relación entre SalePrice y variables ordinales.\")\n",
        "# Si pudiste visualizar nominales: print(\"- Se visualizaron las distribuciones de variables categóricas importantes y su relación con SalePrice.\")\n",
        "print(\"- Se calculó y visualizó la matriz de correlación, identificando variables fuertemente relacionadas con SalePrice.\")\n",
        "# Si analizaste alta correlación entre features: print(\"- Se identificaron pares de características altamente correlacionadas.\")\n",
        "\n",
        "print(\"\\nPróximos pasos:\")\n",
        "print(\"- Decidir el tratamiento para el sesgo de SalePrice (ej. transformación logarítmica).\")\n",
        "print(\"- Decidir cómo manejar los outliers (eliminar, transformar, usar modelos robustos).\")\n",
        "print(\"- Refinar la selección de características basadas en el análisis de correlación y la importancia percibida.\")\n",
        "print(\"- Preparar los datos finales para el entrenamiento del modelo (separación en conjuntos de entrenamiento/prueba).\")\n",
        "print(\"- Seleccionar y entrenar modelos de aprendizaje automático para predecir SalePrice.\")\n",
        "print(\"- Evaluar el rendimiento de los modelos.\")"
      ],
      "metadata": {
        "id": "UomZEEQhaD5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Genera estadísticas descriptivas (media, mediana, desviación estándar, etc.) para las variables numéricas.\n",
        "# Crea histogramas para variables numéricas importantes (por ejemplo, 'SalePrice', 'GrLivArea', 'TotalBsmtSF').\n",
        "# Crea gráficas de barras para variables categóricas relevantes (por ejemplo, 'Neighborhood', 'OverallQual').\n",
        "# Genera una matriz de correlación y su correspondiente heatmap para las variables numéricas.\n",
        "# Identifica y visualiza cualquier patrón o tendencia en los datos.\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Generar estadísticas descriptivas para las variables numéricas\n",
        "print(\"\\nEstadísticas descriptivas para variables numéricas:\")\n",
        "# Selecciona solo las columnas numéricas para las estadísticas descriptivas\n",
        "df_numeric = df.select_dtypes(include=np.number)\n",
        "print(df_numeric.describe())\n",
        "\n",
        "\n",
        "# Crear histogramas para variables numéricas importantes\n",
        "# Ya hemos generado histogramas en la sección anterior, pero podemos volver a ejecutarlos o seleccionar otras variables si es necesario.\n",
        "# Las variables 'SalePrice', 'GrLivArea', 'TotalBsmtSF' ya fueron visualizadas.\n",
        "# Podemos añadir '1stFlrSF', 'GarageArea', 'YearBuilt' como ejemplos adicionales.\n",
        "numeric_cols_hist = ['1stFlrSF', 'GarageArea', 'YearBuilt']\n",
        "print(f\"\\nCreando histogramas para: {numeric_cols_hist}\")\n",
        "plt.figure(figsize=(15, 5))\n",
        "for i, col in enumerate(numeric_cols_hist):\n",
        "    if col in df.columns and df[col].dtype in [np.number]:\n",
        "        plt.subplot(1, len(numeric_cols_hist), i + 1)\n",
        "        sns.histplot(df[col], kde=True)\n",
        "        plt.title(f'Distribución de {col}')\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel('Frecuencia')\n",
        "    else:\n",
        "        print(f\"Advertencia: La columna '{col}' no existe o no es numérica para histograma.\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Crear gráficas de barras para variables categóricas relevantes\n",
        "# Para graficar variables categóricas relevantes, necesitamos las columnas antes de One-Hot Encoding.\n",
        "# Si el código anterior ya ejecutó One-Hot Encoding, las columnas nominales ya no existen en su formato original.\n",
        "# Podemos volver a cargar el dataframe original o usar una copia antes de la codificación.\n",
        "# Asumiendo que trabajamos con el dataframe original o una copia para esta visualización.\n",
        "# NOTA: Si solo tienes el dataframe después de OHE, puedes omitir este paso o encontrar una alternativa de visualización.\n",
        "\n",
        "# Vamos a asumir que podemos usar el dataframe original para visualizar las columnas categóricas.\n",
        "# Cargar el dataframe original si no está disponible en su estado pre-codificado.\n",
        "try:\n",
        "    df_original = pd.read_csv('/content/drive/Shareddrives/UNAL_Colab/Teoría de Aprendizaje de Máquina/AmesHousing.csv')\n",
        "    print(\"\\nCargado el DataFrame original para visualizaciones categóricas.\")\n",
        "\n",
        "    # Seleccionar columnas categóricas relevantes para gráfica de barras\n",
        "    # Usar columnas que NO fueron imputadas con 'missing' para barras simples, a menos que 'missing' sea una categoría significativa.\n",
        "    # Columnas como 'Neighborhood', 'OverallQual' (si se trata como categórica), 'MSZoning'\n",
        "    # 'OverallQual' es numérica, usaremos otras categóricas como 'Neighborhood' y 'MSZoning'.\n",
        "    categorical_cols_bar = ['Neighborhood', 'MSZoning'] # Agrega otras columnas categóricas relevantes\n",
        "\n",
        "    print(f\"\\nCreando gráficas de barras para: {categorical_cols_bar}\")\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    for i, col in enumerate(categorical_cols_bar):\n",
        "        if col in df_original.columns and df_original[col].dtype == 'object':\n",
        "            plt.subplot(1, len(categorical_cols_bar), i + 1)\n",
        "            # Contar la frecuencia de cada categoría\n",
        "            sns.countplot(y=col, data=df_original, order=df_original[col].value_counts().index)\n",
        "            plt.title(f'Distribución de {col}')\n",
        "            plt.xlabel('Frecuencia')\n",
        "            plt.ylabel(col)\n",
        "        else:\n",
        "            print(f\"Advertencia: La columna '{col}' no existe o no es categórica en el DataFrame original para gráfica de barras.\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"\\nNo se pudo cargar el DataFrame original. Saltando la creación de gráficas de barras para columnas categóricas.\")\n",
        "    print(\"Asegúrese de que el archivo 'AmesHousing.csv' esté disponible en la ruta especificada si desea estas visualizaciones.\")\n",
        "\n",
        "\n",
        "# Generar una matriz de correlación y su correspondiente heatmap para las variables numéricas\n",
        "# Ya hemos calculado y visualizado la matriz de correlación numérico_df_for_corr.corr()\n",
        "# Podemos visualizar el heatmap completo aquí si es necesario, aunque puede ser denso.\n",
        "# La visualización de correlación con SalePrice (univariada) ya se hizo.\n",
        "\n",
        "print(\"\\nGenerando heatmap de la matriz de correlación para variables numéricas...\")\n",
        "# Calcular la matriz de correlación solo para columnas numéricas del dataframe actual (post-procesamiento)\n",
        "df_numeric_current = df.select_dtypes(include=np.number)\n",
        "correlation_matrix_current = df_numeric_current.corr()\n",
        "\n",
        "plt.figure(figsize=(15, 12))\n",
        "# Usar annot=False para evitar que sea ilegible en datasets grandes, o ajustar el tamaño de la figura.\n",
        "sns.heatmap(correlation_matrix_current, cmap='coolwarm', annot=False)\n",
        "plt.title('Heatmap de la Matriz de Correlación (Variables Numéricas Post-Procesamiento)')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Identificar y visualizar cualquier patrón o tendencia en los datos\n",
        "# Esto es una tarea amplia, pero podemos visualizar relaciones clave usando scatter plots.\n",
        "# Visualizar la relación entre 'SalePrice' y las variables numéricas más correlacionadas identificadas anteriormente.\n",
        "\n",
        "print(\"\\nVisualizando patrones y tendencias con scatter plots...\")\n",
        "\n",
        "# Seleccionar las N variables más correlacionadas con SalePrice (excluyendo SalePrice misma)\n",
        "if 'SalePrice' in df_numeric_current.columns:\n",
        "    correlation_with_saleprice = df_numeric_current.corr()['SalePrice'].sort_values(ascending=False)\n",
        "    # Seleccionar las top N variables (excluyendo SalePrice)\n",
        "    top_n = 5 # Número de variables para visualizar\n",
        "    top_correlated_cols = correlation_with_saleprice.head(top_n + 1).index.tolist()\n",
        "    if 'SalePrice' in top_correlated_cols:\n",
        "        top_correlated_cols.remove('SalePrice')\n",
        "\n",
        "    print(f\"\\nVisualizando la relación de SalePrice con las {top_n} variables más correlacionadas: {top_correlated_cols}\")\n",
        "    plt.figure(figsize=(15, 5 * int(np.ceil(len(top_correlated_cols)/3)))) # Ajustar el tamaño de la figura\n",
        "    for i, col in enumerate(top_correlated_cols):\n",
        "         if col in df_numeric_current.columns:\n",
        "            plt.subplot(int(np.ceil(len(top_correlated_cols)/3)), 3, i + 1)\n",
        "            sns.scatterplot(x=df_numeric_current[col], y=df_numeric_current['SalePrice'])\n",
        "            plt.title(f'SalePrice vs {col}')\n",
        "            plt.xlabel(col)\n",
        "            plt.ylabel('SalePrice')\n",
        "         else:\n",
        "             print(f\"Advertencia: La columna '{col}' no está disponible para scatter plot.\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"\\n'SalePrice' no está disponible para crear scatter plots de correlación.\")\n",
        "\n",
        "\n",
        "# Otros patrones a considerar (requieren análisis manual):\n",
        "# - Relaciones no lineales (observar scatter plots)\n",
        "# - Interacciones entre características (requiere crear nuevas características o probar modelos con interacciones)\n",
        "# - Patrones temporales si existen (ej. 'YearBuilt', 'YrSold') - 'Age' ya ayuda aquí.\n",
        "\n",
        "print(\"\\nAnálisis de patrones y tendencias adicionales (requiere inspección visual):\")\n",
        "print(\"- Observa los scatter plots para identificar relaciones lineales o no lineales.\")\n",
        "print(\"- Considera cómo 'Age' se relaciona con 'SalePrice'.\")\n",
        "print(\"- Revisa las distribuciones de variables categóricas importantes para entender la composición del dataset.\")\n",
        "print(\"- La matriz de correlación (heatmap) ayuda a ver grupos de variables relacionadas.\")\n"
      ],
      "metadata": {
        "id": "V1eyryBmZQoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Usa train_test_split de sklearn para dividir en entrenamiento (80%) y prueba (20%).\n",
        "# Dado que el examen requiere validación cruzada de 5 folds, usa KFold de sklearn para asegurar consistencia en la evaluación.\n",
        "# Nota: Como es un problema de regresión, KFold es adecuado; no necesitas StratifiedKFold, ya que no hay clases balanceadas.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "\n",
        "# Separar las características (X) y la variable objetivo (y)\n",
        "# Asumimos que 'SalePrice' es la variable objetivo\n",
        "if 'SalePrice' in df.columns:\n",
        "    X = df.drop('SalePrice', axis=1)\n",
        "    y = df['SalePrice']\n",
        "    print(\"\\nCaracterísticas (X) y variable objetivo (y) separadas.\")\n",
        "else:\n",
        "    print(\"\\nError: 'SalePrice' no se encontró en el DataFrame. No se puede separar X e y.\")\n",
        "\n",
        "# Asegurarse de que todas las columnas en X son numéricas antes de la división\n",
        "X = X.select_dtypes(include=np.number)\n",
        "# Eliminar cualquier fila con NaN que pudiera haber quedado (aunque con la imputación no debería haber)\n",
        "X = X.dropna()\n",
        "y = y.loc[X.index] # Asegurar que y coincide con las filas de X\n",
        "\n",
        "print(f\"\\nForma de X antes de la división: {X.shape}\")\n",
        "print(f\"Forma de y antes de la división: {y.shape}\")\n",
        "\n",
        "\n",
        "# División en conjuntos de entrenamiento y prueba\n",
        "# Stratify no se usa porque es un problema de regresión, no de clasificación\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # random_state para reproducibilidad\n",
        "\n",
        "print(f\"\\nForma del conjunto de entrenamiento (X_train): {X_train.shape}\")\n",
        "print(f\"Forma del conjunto de prueba (X_test): {X_test.shape}\")\n",
        "print(f\"Forma de la variable objetivo de entrenamiento (y_train): {y_train.shape}\")\n",
        "print(f\"Forma de la variable objetivo de prueba (y_test): {y_test.shape}\")\n",
        "\n",
        "# Configurar KFold para la validación cruzada\n",
        "n_splits = 5 # Número de folds para la validación cruzada\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42) # shuffle=True y random_state para mezclar los datos consistentemente\n",
        "\n",
        "print(f\"\\nConfiguración de KFold con {n_splits} folds:\")\n",
        "print(kf)\n",
        "\n",
        "# Ejemplo de cómo usar KFold (esto se usa DENTRO del proceso de entrenamiento del modelo)\n",
        "# El siguiente bucle es solo para demostrar cómo iterar sobre los folds.\n",
        "# En la práctica, pasarías el objeto kf a una función como cross_val_score o GridSearchCV.\n",
        "\n",
        "print(\"\\nGenerando índices para los folds de validación cruzada (ejemplo):\")\n",
        "fold_indices = []\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
        "    print(f\"Fold {fold + 1}:\")\n",
        "    print(f\"  Índices de entrenamiento: {train_index[:10]}... ({len(train_index)} total)\") # Mostrar solo los primeros 10\n",
        "    print(f\"  Índices de validación:   {val_index[:10]}... ({len(val_index)} total)\")    # Mostrar solo los primeros 10\n",
        "    fold_indices.append((train_index, val_index))\n",
        "\n",
        "print(\"\\nValidación cruzada configurada.\")\n"
      ],
      "metadata": {
        "id": "KdPH3rqVaSd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LinearRegression"
      ],
      "metadata": {
        "id": "UyIrgX0SeCbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Implementa LinearRegression en Python con sklearn, usando validación cruzada de 5 folds, y reporta MAE, MSE, R2 y MAPE.\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
        "import numpy as np\n",
        "\n",
        "# Definir las métricas de evaluación\n",
        "# MAE: Error Absoluto Medio\n",
        "# MSE: Error Cuadrático Medio\n",
        "# R2: Coeficiente de Determinación\n",
        "# MAPE: Error Porcentual Absoluto Medio\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    \"\"\"Calcula el Error Porcentual Absoluto Medio (MAPE).\"\"\"\n",
        "    # Evitar división por cero\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    # Reemplazar ceros en y_true para evitar inf o nan.\n",
        "    # Se puede usar un pequeño épsilon o filtrar. Aquí usamos un épsilon.\n",
        "    epsilon = 1e-8\n",
        "    return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
        "\n",
        "# Crear objetos scorer para las métricas (MAPE necesita un scorer personalizado)\n",
        "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False) # greater_is_better=False para métricas de error\n",
        "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
        "r2_scorer = make_scorer(r2_score, greater_is_better=True)\n",
        "mape_scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
        "\n",
        "# Inicializar el modelo de Regresión Lineal\n",
        "model = LinearRegression()\n",
        "\n",
        "# Realizar validación cruzada con 5 folds\n",
        "# cross_val_score por defecto usa la métrica 'score' del estimador (que para LinearRegression es R2)\n",
        "# Podemos pasar nuestros propios scorers para evaluar múltiples métricas\n",
        "print(\"\\nRealizando validación cruzada con Linear Regression...\")\n",
        "\n",
        "# Usamos cross_validate para obtener múltiples métricas simultáneamente\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "scoring = {\n",
        "    'mae': mae_scorer,\n",
        "    'mse': mse_scorer,\n",
        "    'r2': r2_scorer,\n",
        "    'mape': mape_scorer\n",
        "}\n",
        "\n",
        "cv_results = cross_validate(model, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "# Los resultados de cross_validate son diccionarios. Accedemos a los arrays de resultados.\n",
        "# Las métricas de error (MAE, MSE, MAPE) vienen negativas porque make_scorer las optimiza para maximizar (por defecto).\n",
        "# Necesitamos tomar el valor absoluto para el reporte.\n",
        "cv_mae = -cv_results['test_mae']\n",
        "cv_mse = -cv_results['test_mse']\n",
        "cv_r2 = cv_results['test_r2']\n",
        "cv_mape = -cv_results['test_mape']\n",
        "\n",
        "# Reportar los resultados promedio y desviación estándar de cada métrica\n",
        "print(f\"\\nResultados de Validación Cruzada (promedio +/- std dev):\")\n",
        "print(f\"  MAE:  {cv_mae.mean():.4f} +/- {cv_mae.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse.mean():.4f} +/- {cv_mse.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2.mean():.4f} +/- {cv_r2.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape.mean():.4f}% +/- {cv_mape.std():.4f}%\")\n",
        "\n",
        "# Opcional: Entrenar el modelo final en todo el conjunto de entrenamiento y evaluar en el conjunto de prueba\n",
        "# Este paso es típicamente posterior a la selección del modelo y hyperparámetros\n",
        "# print(\"\\nEntrenando modelo final en el conjunto de entrenamiento completo...\")\n",
        "# model.fit(X_train, y_train)\n",
        "# print(\"Evaluando en el conjunto de prueba...\")\n",
        "# y_pred_test = model.predict(X_test)\n",
        "\n",
        "# # Calcular métricas en el conjunto de prueba\n",
        "# test_mae = mean_absolute_error(y_test, y_pred_test)\n",
        "# test_mse = mean_squared_error(y_test, y_pred_test)\n",
        "# test_r2 = r2_score(y_test, y_pred_test)\n",
        "# test_mape = mean_absolute_percentage_error(y_test, y_pred_test)\n",
        "\n",
        "\n",
        "# print(\"\\nMétricas en el conjunto de prueba:\")\n",
        "# print(f\"  MAE:  {test_mae:.4f}\")\n",
        "# print(f\"  MSE:  {test_mse:.4f}\")\n",
        "# print(f\"  R2:   {test_r2:.4f}\")\n",
        "# print(f\"  MAPE: {test_mape:.4f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "sUNyrZ38eFXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lasso"
      ],
      "metadata": {
        "id": "NQhV3faZeWos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Implementa Lasso en Python con sklearn, optimiza alpha usando GridSearchCV, RandomizedSearchCV y BayesSearchCV de skopt, con rangos logarítmicos, y reporta MAE, MSE, R2 y MAPE con validación cruzada de 5 folds.\"\n",
        "# Rango: alpha = [0.001, 0.01, 0.1, 1, 10] (Grid), loguniform(0.001, 10) (Random/Bayesian).\n",
        "# Justificación: Alpha controla la regularización L1, y rangos logarítmicos cubren fuerzas variadas, útil para selección de características.\n",
        "\n",
        "!pip install scikit-optimize\n",
        "!pip install scipy\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
        "from sklearn.model_selection import cross_validate\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real\n",
        "from scipy.stats import loguniform # Import loguniform from scipy\n",
        "\n",
        "# Ensure scorers are defined (as per the original code context)\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    \"\"\"Calcula el Error Porcentual Absoluto Medio (MAPE).\"\"\"\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    epsilon = 1e-8\n",
        "    return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
        "\n",
        "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
        "r2_scorer = make_scorer(r2_score, greater_is_better=True)\n",
        "mape_scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
        "\n",
        "scoring = {\n",
        "    'mae': mae_scorer,\n",
        "    'mse': mse_scorer,\n",
        "    'r2': r2_scorer,\n",
        "    'mape': mape_scorer\n",
        "}\n",
        "\n",
        "scoring_optimizer = {'mae': mae_scorer}\n",
        "\n",
        "\n",
        "# Definir el modelo Lasso\n",
        "lasso = Lasso(random_state=42, max_iter=10000) # Aumentar max_iter si no converge\n",
        "\n",
        "\n",
        "# --- Optimización con GridSearchCV ---\n",
        "print(\"\\nIniciando optimización con GridSearchCV para Lasso...\")\n",
        "\n",
        "# Rango de alpha para GridSearchCV (escala logarítmica)\n",
        "param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10]} # Rango específico para Grid\n",
        "\n",
        "grid_search = GridSearchCV(estimator=lasso, param_grid=param_grid,\n",
        "                           scoring=scoring_optimizer, refit='mae', # Optimizar usando MAE\n",
        "                           cv=kf, verbose=1, n_jobs=-1)\n",
        "\n",
        "start_time_grid = time.time()\n",
        "grid_search.fit(X_train, y_train)\n",
        "end_time_grid = time.time()\n",
        "\n",
        "print(\"\\nResultados de GridSearchCV:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {grid_search.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-grid_search.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de GridSearchCV: {end_time_grid - start_time_grid:.2f} segundos\")\n",
        "\n",
        "best_lasso_grid = grid_search.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de GridSearchCV con validación cruzada completa:\")\n",
        "cv_results_grid_best = cross_validate(best_lasso_grid, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_grid = -cv_results_grid_best['test_mae']\n",
        "cv_mse_grid = -cv_results_grid_best['test_mse']\n",
        "cv_r2_grid = cv_results_grid_best['test_r2']\n",
        "cv_mape_grid = -cv_results_grid_best['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_grid.mean():.4f} +/- {cv_mae_grid.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_grid.mean():.4f} +/- {cv_mse_grid.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_grid.mean():.4f} +/- {cv_r2_grid.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_grid.mean():.4f}% +/- {cv_mape_grid.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Optimización con RandomizedSearchCV ---\n",
        "print(\"\\nIniciando optimización con RandomizedSearchCV para Lasso...\")\n",
        "\n",
        "# Rango de alpha para RandomizedSearchCV (distribución loguniform usando scipy)\n",
        "param_dist = {'alpha': loguniform(0.001, 10)} # Use loguniform from scipy\n",
        "\n",
        "# Número de iteraciones (puntos a probar). Ajusta este valor según el tiempo disponible.\n",
        "n_iter_rand = 50 # Ejemplo: probar 50 combinaciones aleatorias\n",
        "\n",
        "random_search = RandomizedSearchCV(estimator=lasso, param_distributions=param_dist,\n",
        "                                   n_iter=n_iter_rand,\n",
        "                                   scoring=scoring_optimizer, refit='mae', # Optimizar usando MAE\n",
        "                                   cv=kf, verbose=1, random_state=42, n_jobs=-1) # random_state para reproducibilidad\n",
        "\n",
        "start_time_rand = time.time()\n",
        "random_search.fit(X_train, y_train)\n",
        "end_time_rand = time.time()\n",
        "\n",
        "print(\"\\nResultados de RandomizedSearchCV:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {random_search.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-random_search.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de RandomizedSearchCV: {end_time_rand - start_time_rand:.2f} segundos\")\n",
        "\n",
        "best_lasso_rand = random_search.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de RandomizedSearchCV con validación cruzada completa:\")\n",
        "cv_results_rand_best = cross_validate(best_lasso_rand, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_rand = -cv_results_rand_best['test_mae']\n",
        "cv_mse_rand = -cv_results_rand_best['test_mse']\n",
        "cv_r2_rand = cv_results_rand_best['test_r2']\n",
        "cv_mape_rand = -cv_results_rand_best['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_rand.mean():.4f} +/- {cv_mae_rand.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_rand.mean():.4f} +/- {cv_mse_rand.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_rand.mean():.4f} +/- {cv_r2_rand.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_rand.mean():.4f}% +/- {cv_mape_rand.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Optimización con BayesSearchCV ---\n",
        "print(\"\\nIniciando optimización con BayesSearchCV para Lasso...\")\n",
        "\n",
        "# Rango de alpha para BayesSearchCV (espacio de búsqueda de skopt)\n",
        "search_spaces = {'alpha': Real(0.001, 10, prior='log-uniform')}\n",
        "\n",
        "# Número de iteraciones (puntos a explorar). Generalmente requiere menos que RandomizedSearch.\n",
        "n_iter_bayes = 50 # Ejemplo: probar 50 iteraciones\n",
        "\n",
        "\n",
        "# Definir BayesSearchCV\n",
        "bayes_search = BayesSearchCV(estimator=lasso, search_spaces=search_spaces,\n",
        "                             n_iter=n_iter_bayes,\n",
        "                             scoring=scoring_optimizer, refit='mae', # Optimizar usando MAE\n",
        "                             cv=kf, verbose=1, random_state=42, n_jobs=-1)\n",
        "\n",
        "start_time_bayes = time.time()\n",
        "bayes_search.fit(X_train, y_train)\n",
        "end_time_bayes = time.time()\n",
        "\n",
        "print(\"\\nResultados de BayesSearchCV:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {bayes_search.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-bayes_search.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de BayesSearchCV: {end_time_bayes - start_time_bayes:.2f} segundos\")\n",
        "\n",
        "best_lasso_bayes = bayes_search.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de BayesSearchCV con validación cruzada completa:\")\n",
        "cv_results_bayes_best = cross_validate(best_lasso_bayes, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_bayes = -cv_results_bayes_best['test_mae']\n",
        "cv_mse_bayes = -cv_results_bayes_best['test_mse']\n",
        "cv_r2_bayes = cv_results_bayes_best['test_r2']\n",
        "cv_mape_bayes = -cv_results_bayes_best['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_bayes.mean():.4f} +/- {cv_mae_bayes.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_bayes.mean():.4f} +/- {cv_mse_bayes.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_bayes.mean():.4f} +/- {cv_r2_bayes.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_bayes.mean():.4f}% +/- {cv_mape_bayes.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Comparación de resultados ---\n",
        "print(\"\\n--- Resumen de Resultados de Optimización (MAE promedio en validación) ---\")\n",
        "print(f\"GridSearchCV:      {-grid_search.best_score_:.4f}\")\n",
        "print(f\"RandomizedSearchCV: {-random_search.best_score_:.4f}\")\n",
        "print(f\"BayesSearchCV:      {-bayes_search.best_score_:.4f}\")\n",
        "\n",
        "print(\"\\n--- Reporte Completo de Métricas (Promedio de 5-fold CV) ---\")\n",
        "\n",
        "print(\"\\nGridSearchCV Mejor Modelo:\")\n",
        "print(f\"  MAE:  {cv_mae_grid.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_grid.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_grid.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_grid.mean():.4f}%\")\n",
        "\n",
        "print(\"\\nRandomizedSearchCV Mejor Modelo:\")\n",
        "print(f\"  MAE:  {cv_mae_rand.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_rand.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_rand.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_rand.mean():.4f}%\")\n",
        "\n",
        "print(\"\\nBayesSearchCV Mejor Modelo:\")\n",
        "print(f\"  MAE:  {cv_mae_bayes.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_bayes.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_bayes.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_bayes.mean():.4f}%\")\n",
        "\n",
        "# Puedes seleccionar el mejor modelo general basado en el MAE promedio de validación\n",
        "# Por ejemplo, si BayesSearchCV dio el mejor resultado:\n",
        "# final_best_model = best_lasso_bayes\n",
        "# print(\"\\nEl mejor modelo general basado en MAE promedio de validación es el de BayesSearchCV.\")\n",
        "\n",
        "\n",
        "# Opcional: Entrenar el modelo final elegido en todo X_train y evaluar en X_test\n",
        "# final_best_model = best_lasso_bayes # O grid_search.best_estimator_ o random_search.best_estimator_\n",
        "\n",
        "# print(\"\\nEntrenando el modelo final seleccionado en el conjunto de entrenamiento completo...\")\n",
        "# final_best_model.fit(X_train, y_train)\n",
        "# print(\"Evaluando en el conjunto de prueba (X_test)...\")\n",
        "# y_pred_test_final = final_best_model.predict(X_test)\n",
        "\n",
        "# # Calcular métricas en el conjunto de prueba con el modelo final\n",
        "# test_mae_final = mean_absolute_error(y_test, y_pred_test_final)\n",
        "# test_mse_final = mean_squared_error(y_test, y_pred_test_final)\n",
        "# test_r2_final = r2_score(y_test, y_pred_test_final)\n",
        "# test_mape_final = mean_absolute_percentage_error(y_test, y_pred_test_final)\n",
        "\n",
        "# print(\"\\nMétricas en el conjunto de prueba con el modelo Lasso final seleccionado:\")\n",
        "# print(f\"  MAE:  {test_mae_final:.4f}\")\n",
        "# print(f\"  MSE:  {test_mse_final:.4f}\")\n",
        "# print(f\"  R2:   {test_r2_final:.4f}\")\n",
        "# print(f\"  MAPE: {test_mape_final:.4f}%\")"
      ],
      "metadata": {
        "id": "o22LoPfAfNlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ElasticNet"
      ],
      "metadata": {
        "id": "u8eH4CoFgK5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Implementa ElasticNet en Python con sklearn, optimiza alpha y l1_ratio usando GridSearchCV, RandomizedSearchCV y BayesSearchCV de skopt, y reporta MAE, MSE, R2 y MAPE con validación cruzada de 5 folds.\"\n",
        "# Rango: alpha = [0.001, 0.01, 0.1, 1], l1_ratio = [0.1, 0.3, 0.5] (Grid); alpha loguniform(0.001, 1), l1_ratio uniform(0, 1) (Random/Bayesian).\n",
        "# Justificación: Alpha regula fuerza, l1_ratio balancea L1/L2; rangos logarítmicos para alpha capturan variabilidad, l1_ratio cubre todo espectro.\n",
        "\n",
        "from scipy.stats import loguniform, uniform\n",
        "from sklearn.linear_model import ElasticNet\n",
        "!pip install scikit-optimize\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
        "from sklearn.model_selection import cross_validate, GridSearchCV, RandomizedSearchCV\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real\n",
        "\n",
        "# Ensure scorers are defined (as per the original code context)\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    \"\"\"Calcula el Error Porcentual Absoluto Medio (MAPE).\"\"\"\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    epsilon = 1e-8\n",
        "    return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
        "\n",
        "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
        "r2_scorer = make_scorer(r2_score, greater_is_better=True)\n",
        "mape_scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
        "\n",
        "scoring = {\n",
        "    'mae': mae_scorer,\n",
        "    'mse': mse_scorer,\n",
        "    'r2': r2_scorer,\n",
        "    'mape': mape_scorer\n",
        "}\n",
        "\n",
        "scoring_optimizer = {'mae': mae_scorer}\n",
        "\n",
        "\n",
        "# Definir el modelo ElasticNet\n",
        "elastic_net = ElasticNet(random_state=42, max_iter=10000) # Aumentar max_iter si no converge\n",
        "\n",
        "# Assuming X_train, y_train, and kf are defined in previous cells\n",
        "\n",
        "# --- Optimization with GridSearchCV ---\n",
        "print(\"\\nIniciando optimización con GridSearchCV para ElasticNet...\")\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "# alpha = [0.001, 0.01, 0.1, 1], l1_ratio = [0.1, 0.3, 0.5]\n",
        "param_grid_en = {\n",
        "    'alpha': [0.001, 0.01, 0.1, 1],\n",
        "    'l1_ratio': [0.1, 0.3, 0.5]\n",
        "}\n",
        "\n",
        "grid_search_en = GridSearchCV(estimator=elastic_net, param_grid=param_grid_en,\n",
        "                              scoring=scoring_optimizer, refit='mae', # Optimize using MAE\n",
        "                              cv=kf, verbose=1, n_jobs=-1)\n",
        "\n",
        "start_time_grid_en = time.time()\n",
        "grid_search_en.fit(X_train, y_train)\n",
        "end_time_grid_en = time.time()\n",
        "\n",
        "print(\"\\nResultados de GridSearchCV para ElasticNet:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {grid_search_en.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-grid_search_en.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de GridSearchCV: {end_time_grid_en - start_time_grid_en:.2f} segundos\")\n",
        "\n",
        "best_elastic_net_grid = grid_search_en.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de GridSearchCV para ElasticNet con validación cruzada completa:\")\n",
        "cv_results_grid_best_en = cross_validate(best_elastic_net_grid, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_grid_en = -cv_results_grid_best_en['test_mae']\n",
        "cv_mse_grid_en = -cv_results_grid_best_en['test_mse']\n",
        "cv_r2_grid_en = cv_results_grid_best_en['test_r2']\n",
        "cv_mape_grid_en = -cv_results_grid_best_en['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_grid_en.mean():.4f} +/- {cv_mae_grid_en.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_grid_en.mean():.4f} +/- {cv_mse_grid_en.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_grid_en.mean():.4f} +/- {cv_r2_grid_en.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_grid_en.mean():.4f}% +/- {cv_mape_grid_en.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Optimization with RandomizedSearchCV ---\n",
        "print(\"\\nIniciando optimización con RandomizedSearchCV para ElasticNet...\")\n",
        "\n",
        "# Define parameter distribution for RandomizedSearchCV\n",
        "# alpha loguniform(0.001, 1), l1_ratio uniform(0, 1)\n",
        "param_dist_en = {\n",
        "    'alpha': loguniform(0.001, 1),\n",
        "    'l1_ratio': uniform(0, 1)  # Use scipy.stats.uniform for RandomizedSearchCV\n",
        "}\n",
        "\n",
        "# Number of iterations (adjust as needed)\n",
        "n_iter_rand_en = 50\n",
        "\n",
        "random_search_en = RandomizedSearchCV(estimator=elastic_net, param_distributions=param_dist_en,\n",
        "                                      n_iter=n_iter_rand_en,\n",
        "                                      scoring=scoring_optimizer, refit='mae', # Optimize using MAE\n",
        "                                      cv=kf, verbose=1, random_state=42, n_jobs=-1) # random_state para reproducibilidad\n",
        "\n",
        "start_time_rand_en = time.time()\n",
        "random_search_en.fit(X_train, y_train)\n",
        "end_time_rand_en = time.time()\n",
        "\n",
        "print(\"\\nResultados de RandomizedSearchCV para ElasticNet:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {random_search_en.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-random_search_en.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de RandomizedSearchCV: {end_time_rand_en - start_time_rand_en:.2f} segundos\")\n",
        "\n",
        "best_elastic_net_rand = random_search_en.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de RandomizedSearchCV para ElasticNet con validación cruzada completa:\")\n",
        "cv_results_rand_best_en = cross_validate(best_elastic_net_rand, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_rand_en = -cv_results_rand_best_en['test_mae']\n",
        "cv_mse_rand_en = -cv_results_rand_best_en['test_mse']\n",
        "cv_r2_rand_en = cv_results_rand_best_en['test_r2']\n",
        "cv_mape_rand_en = -cv_results_rand_best_en['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_rand_en.mean():.4f} +/- {cv_mae_rand_en.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_rand_en.mean():.4f} +/- {cv_mse_rand_en.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_rand_en.mean():.4f} +/- {cv_r2_rand_en.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_rand_en.mean():.4f}% +/- {cv_mape_rand_en.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Optimization with BayesSearchCV ---\n",
        "print(\"\\nIniciando optimización con BayesSearchCV para ElasticNet...\")\n",
        "\n",
        "# Define search spaces for BayesSearchCV\n",
        "# alpha loguniform(0.001, 1), l1_ratio uniform(0, 1)\n",
        "search_spaces_en = {\n",
        "    'alpha': Real(0.001, 1, prior='log-uniform'),\n",
        "    'l1_ratio': Real(0, 1, prior='uniform') # Use skopt's Real for BayesSearchCV\n",
        "}\n",
        "\n",
        "# Number of iterations (adjust as needed)\n",
        "n_iter_bayes_en = 50\n",
        "\n",
        "bayes_search_en = BayesSearchCV(estimator=elastic_net, search_spaces=search_spaces_en,\n",
        "                                n_iter=n_iter_bayes_en,\n",
        "                                scoring=scoring_optimizer, refit='mae', # Optimize using MAE\n",
        "                                cv=kf, verbose=1, random_state=42, n_jobs=-1)\n",
        "\n",
        "start_time_bayes_en = time.time()\n",
        "bayes_search_en.fit(X_train, y_train)\n",
        "end_time_bayes_en = time.time()\n",
        "\n",
        "print(\"\\nResultados de BayesSearchCV para ElasticNet:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {bayes_search_en.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-bayes_search_en.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de BayesSearchCV: {end_time_bayes_en - start_time_bayes_en:.2f} segundos\")\n",
        "\n",
        "best_elastic_net_bayes = bayes_search_en.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de BayesSearchCV para ElasticNet con validación cruzada completa:\")\n",
        "cv_results_bayes_best_en = cross_validate(best_elastic_net_bayes, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_bayes_en = -cv_results_bayes_best_en['test_mae']\n",
        "cv_mse_bayes_en = -cv_results_bayes_best_en['test_mse']\n",
        "cv_r2_bayes_en = cv_results_bayes_best_en['test_r2']\n",
        "cv_mape_bayes_en = -cv_results_bayes_best_en['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_bayes_en.mean():.4f} +/- {cv_mae_bayes_en.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_bayes_en.mean():.4f} +/- {cv_mse_bayes_en.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_bayes_en.mean():.4f} +/- {cv_r2_bayes_en.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_bayes_en.mean():.4f}% +/- {cv_mape_bayes_en.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Comparación de resultados de ElasticNet ---\n",
        "print(\"\\n--- Resumen de Resultados de Optimización para ElasticNet (MAE promedio en validación) ---\")\n",
        "print(f\"GridSearchCV:      {-grid_search_en.best_score_:.4f}\")\n",
        "print(f\"RandomizedSearchCV: {-random_search_en.best_score_:.4f}\")\n",
        "print(f\"BayesSearchCV:      {-bayes_search_en.best_score_:.4f}\")\n",
        "\n",
        "print(\"\\n--- Reporte Completo de Métricas para ElasticNet (Promedio de 5-fold CV) ---\")\n",
        "\n",
        "print(\"\\nGridSearchCV Mejor Modelo (ElasticNet):\")\n",
        "print(f\"  MAE:  {cv_mae_grid_en.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_grid_en.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_grid_en.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_grid_en.mean():.4f}%\")\n",
        "\n",
        "print(\"\\nRandomizedSearchCV Mejor Modelo (ElasticNet):\")\n",
        "print(f\"  MAE:  {cv_mae_rand_en.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_rand_en.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_rand_en.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_rand_en.mean():.4f}%\")\n",
        "\n",
        "print(\"\\nBayesSearchCV Mejor Modelo (ElasticNet):\")\n",
        "print(f\"  MAE:  {cv_mae_bayes_en.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_bayes_en.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_bayes_en.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_bayes_en.mean():.4f}%\")\n",
        "\n",
        "# You can select the overall best ElasticNet model based on validation MAE\n",
        "# For instance, if BayesSearchCV gave the best result:\n",
        "# final_best_elastic_net_model = best_elastic_net_bayes\n",
        "# print(\"\\nEl mejor modelo ElasticNet general basado en MAE promedio de validación es el de BayesSearchCV.\")\n",
        "\n",
        "# Optional: Train the chosen best ElasticNet model on the entire X_train and evaluate on X_test\n",
        "# final_best_elastic_net_model = best_elastic_net_bayes # Or grid_search_en.best_estimator_ or random_search_en.best_estimator_\n",
        "\n",
        "# print(\"\\nEntrenando el modelo ElasticNet final seleccionado en el conjunto de entrenamiento completo...\")\n",
        "# final_best_elastic_net_model.fit(X_train, y_train)\n",
        "# print(\"Evaluando en el conjunto de prueba (X_test) con el modelo ElasticNet final...\")\n",
        "# y_pred_test_final_en = final_best_elastic_net_model.predict(X_test)\n",
        "\n",
        "# # Calculate metrics on the test set with the final ElasticNet model\n",
        "# test_mae_final_en = mean_absolute_error(y_test, y_pred_test_final_en)\n",
        "# test_mse_final_en = mean_squared_error(y_test, y_pred_test_final_en)\n",
        "# test_r2_final_en = r2_score(y_test, y_pred_test_final_en)\n",
        "# test_mape_final_en = mean_absolute_percentage_error(y_test, y_pred_test_final_en)\n",
        "\n",
        "# print(\"\\nMétricas en el conjunto de prueba con el modelo ElasticNet final seleccionado:\")\n",
        "# print(f\"  MAE:  {test_mae_final_en:.4f}\")\n",
        "# print(f\"  MSE:  {test_mse_final_en:.4f}\")\n",
        "# print(f\"  R2:   {test_r2_final_en:.4f}\")\n",
        "# print(f\"  MAPE: {test_mape_final_en:.4f}%\")"
      ],
      "metadata": {
        "id": "b-9lMkjMhoEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KernelRidge"
      ],
      "metadata": {
        "id": "HJWqwv_XiQ0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Implementa KernelRidge en Python con sklearn, optimiza alpha y gamma para kernel RBF usando GridSearchCV, RandomizedSearchCV y BayesSearchCV de skopt, y reporta MAE, MSE, R2 y MAPE con validación cruzada de 5 folds.\"\n",
        "# Rango: alpha = [0.001, 0.01, 0.1], gamma = [0.001, 0.01] (Grid); alpha loguniform(0.001, 1), gamma loguniform(0.001, 1) (Random/Bayesian).\n",
        "# Justificación: Alpha controla regularización, gamma ajusta escala del kernel RBF; rangos logarítmicos capturan no linealidades.\n",
        "\n",
        "!pip install scikit-optimize\n",
        "\n",
        "# Ensure scorers and kf are defined (as per the original code context)\n",
        "# (assuming the previous cells defining these have been run)\n",
        "\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "\n",
        "\n",
        "# Definir el modelo KernelRidge con kernel RBF\n",
        "# Aumentar max_iter si es necesario para la convergencia (aunque KRR no tiene un solver iterativo como las lineales)\n",
        "# kernel='rbf' especifica el tipo de kernel\n",
        "kernel_ridge = KernelRidge(kernel='rbf')\n",
        "\n",
        "\n",
        "# --- Optimization with GridSearchCV ---\n",
        "print(\"\\nIniciando optimización con GridSearchCV para KernelRidge (kernel RBF)...\")\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "# alpha = [0.001, 0.01, 0.1], gamma = [0.001, 0.01]\n",
        "param_grid_krr = {\n",
        "    'alpha': [0.001, 0.01, 0.1],\n",
        "    'gamma': [0.001, 0.01]\n",
        "}\n",
        "\n",
        "grid_search_krr = GridSearchCV(estimator=kernel_ridge, param_grid=param_grid_krr,\n",
        "                               scoring=scoring_optimizer, refit='mae', # Optimize using MAE\n",
        "                               cv=kf, verbose=1, n_jobs=-1)\n",
        "\n",
        "start_time_grid_krr = time.time()\n",
        "grid_search_krr.fit(X_train, y_train)\n",
        "end_time_grid_krr = time.time()\n",
        "\n",
        "print(\"\\nResultados de GridSearchCV para KernelRidge:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {grid_search_krr.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-grid_search_krr.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de GridSearchCV: {end_time_grid_krr - start_time_grid_krr:.2f} segundos\")\n",
        "\n",
        "best_kernel_ridge_grid = grid_search_krr.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de GridSearchCV para KernelRidge con validación cruzada completa:\")\n",
        "cv_results_grid_best_krr = cross_validate(best_kernel_ridge_grid, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_grid_krr = -cv_results_grid_best_krr['test_mae']\n",
        "cv_mse_grid_krr = -cv_results_grid_best_krr['test_mse']\n",
        "cv_r2_grid_krr = cv_results_grid_best_krr['test_r2']\n",
        "cv_mape_grid_krr = -cv_results_grid_best_krr['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_grid_krr.mean():.4f} +/- {cv_mae_grid_krr.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_grid_krr.mean():.4f} +/- {cv_mse_grid_krr.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_grid_krr.mean():.4f} +/- {cv_r2_grid_krr.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_grid_krr.mean():.4f}% +/- {cv_mape_grid_krr.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Optimization with RandomizedSearchCV ---\n",
        "print(\"\\nIniciando optimización con RandomizedSearchCV para KernelRidge (kernel RBF)...\")\n",
        "\n",
        "# Define parameter distribution for RandomizedSearchCV\n",
        "# alpha loguniform(0.001, 1), gamma loguniform(0.001, 1)\n",
        "param_dist_krr = {\n",
        "    'alpha': loguniform(0.001, 1),\n",
        "    'gamma': loguniform(0.001, 1)\n",
        "}\n",
        "\n",
        "# Number of iterations (adjust as needed)\n",
        "n_iter_rand_krr = 50 # Example: try 50 random combinations\n",
        "\n",
        "random_search_krr = RandomizedSearchCV(estimator=kernel_ridge, param_distributions=param_dist_krr,\n",
        "                                       n_iter=n_iter_rand_krr,\n",
        "                                       scoring=scoring_optimizer, refit='mae', # Optimize using MAE\n",
        "                                       cv=kf, verbose=1, random_state=42, n_jobs=-1) # random_state para reproducibilidad\n",
        "\n",
        "start_time_rand_krr = time.time()\n",
        "random_search_krr.fit(X_train, y_train)\n",
        "end_time_rand_krr = time.time()\n",
        "\n",
        "print(\"\\nResultados de RandomizedSearchCV para KernelRidge:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {random_search_krr.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-random_search_krr.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de RandomizedSearchCV: {end_time_rand_krr - start_time_rand_krr:.2f} segundos\")\n",
        "\n",
        "best_kernel_ridge_rand = random_search_krr.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de RandomizedSearchCV para KernelRidge con validación cruzada completa:\")\n",
        "cv_results_rand_best_krr = cross_validate(best_kernel_ridge_rand, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_rand_krr = -cv_results_rand_best_krr['test_mae']\n",
        "cv_mse_rand_krr = -cv_results_rand_best_krr['test_mse']\n",
        "cv_r2_rand_krr = cv_results_rand_best_krr['test_r2']\n",
        "cv_mape_rand_krr = -cv_results_rand_best_krr['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_rand_krr.mean():.4f} +/- {cv_mae_rand_krr.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_rand_krr.mean():.4f} +/- {cv_mse_rand_krr.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_rand_krr.mean():.4f} +/- {cv_r2_rand_krr.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_rand_krr.mean():.4f}% +/- {cv_mape_rand_krr.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Optimization with BayesSearchCV ---\n",
        "print(\"\\nIniciando optimización con BayesSearchCV para KernelRidge (kernel RBF)...\")\n",
        "\n",
        "# Define search spaces for BayesSearchCV\n",
        "# alpha loguniform(0.001, 1), gamma loguniform(0.001, 1)\n",
        "search_spaces_krr = {\n",
        "    'alpha': Real(0.001, 1, prior='log-uniform'),\n",
        "    'gamma': Real(0.001, 1, prior='log-uniform')\n",
        "}\n",
        "\n",
        "# Number of iterations (adjust as needed)\n",
        "n_iter_bayes_krr = 50 # Example: try 50 iterations\n",
        "\n",
        "bayes_search_krr = BayesSearchCV(estimator=kernel_ridge, search_spaces=search_spaces_krr,\n",
        "                                 n_iter=n_iter_bayes_krr,\n",
        "                                 scoring=scoring_optimizer, refit='mae', # Optimize using MAE\n",
        "                                 cv=kf, verbose=1, random_state=42, n_jobs=-1)\n",
        "\n",
        "start_time_bayes_krr = time.time()\n",
        "bayes_search_krr.fit(X_train, y_train)\n",
        "end_time_bayes_krr = time.time()\n",
        "\n",
        "print(\"\\nResultados de BayesSearchCV para KernelRidge:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {bayes_search_krr.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-bayes_search_krr.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de BayesSearchCV: {end_time_bayes_krr - start_time_bayes_krr:.2f} segundos\")\n",
        "\n",
        "best_kernel_ridge_bayes = bayes_search_krr.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de BayesSearchCV para KernelRidge con validación cruzada completa:\")\n",
        "cv_results_bayes_best_krr = cross_validate(best_kernel_ridge_bayes, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_bayes_krr = -cv_results_bayes_best_krr['test_mae']\n",
        "cv_mse_bayes_krr = -cv_results_bayes_best_krr['test_mse']\n",
        "cv_r2_bayes_krr = cv_results_bayes_best_krr['test_r2']\n",
        "cv_mape_bayes_krr = -cv_results_bayes_best_krr['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_bayes_krr.mean():.4f} +/- {cv_mae_bayes_krr.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_bayes_krr.mean():.4f} +/- {cv_mse_bayes_krr.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_bayes_krr.mean():.4f} +/- {cv_r2_bayes_krr.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_bayes_krr.mean():.4f}% +/- {cv_mape_bayes_krr.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Comparación de resultados de KernelRidge ---\n",
        "print(\"\\n--- Resumen de Resultados de Optimización para KernelRidge (MAE promedio en validación) ---\")\n",
        "print(f\"GridSearchCV:      {-grid_search_krr.best_score_:.4f}\")\n",
        "print(f\"RandomizedSearchCV: {-random_search_krr.best_score_:.4f}\")\n",
        "print(f\"BayesSearchCV:      {-bayes_search_krr.best_score_:.4f}\")\n",
        "\n",
        "print(\"\\n--- Reporte Completo de Métricas para KernelRidge (Promedio de 5-fold CV) ---\")\n",
        "\n",
        "print(\"\\nGridSearchCV Mejor Modelo (KernelRidge):\")\n",
        "print(f\"  MAE:  {cv_mae_grid_krr.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_grid_krr.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_grid_krr.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_grid_krr.mean():.4f}%\")\n",
        "\n",
        "print(\"\\nRandomizedSearchCV Mejor Modelo (KernelRidge):\")\n",
        "print(f\"  MAE:  {cv_mae_rand_krr.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_rand_krr.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_rand_krr.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_rand_krr.mean():.4f}%\")\n",
        "\n",
        "print(\"\\nBayesSearchCV Mejor Modelo (KernelRidge):\")\n",
        "print(f\"  MAE:  {cv_mae_bayes_krr.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_bayes_krr.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_bayes_krr.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_bayes_krr.mean():.4f}%\")\n",
        "\n",
        "# You can select the overall best KernelRidge model based on validation MAE\n",
        "# For instance, if BayesSearchCV gave the best result:\n",
        "# final_best_kernel_ridge_model = best_kernel_ridge_bayes\n",
        "# print(\"\\nEl mejor modelo KernelRidge general basado en MAE promedio de validación es el de BayesSearchCV.\")\n",
        "\n",
        "# Optional: Train the chosen best KernelRidge model on the entire X_train and evaluate on X_test\n",
        "# final_best_kernel_ridge_model = best_kernel_ridge_bayes # Or grid_search_krr.best_estimator_ or random_search_krr.best_estimator_\n",
        "\n",
        "# print(\"\\nEntrenando el modelo KernelRidge final seleccionado en el conjunto de entrenamiento completo...\")\n",
        "# final_best_kernel_ridge_model.fit(X_train, y_train)\n",
        "# print(\"Evaluando en el conjunto de prueba (X_test) con el modelo KernelRidge final...\")\n",
        "# y_pred_test_final_krr = final_best_kernel_ridge_model.predict(X_test)\n",
        "\n",
        "# # Calculate metrics on the test set with the final KernelRidge model\n",
        "# test_mae_final_krr = mean_absolute_error(y_test, y_pred_test_final_krr)\n",
        "# test_mse_final_krr = mean_squared_error(y_test, y_pred_test_final_krr)\n",
        "# test_r2_final_krr = r2_score(y_test, y_pred_test_final_krr)\n",
        "# test_mape_final_krr = mean_absolute_percentage_error(y_test, y_pred_test_final_krr)\n",
        "\n",
        "# print(\"\\nMétricas en el conjunto de prueba con el modelo KernelRidge final seleccionado:\")\n",
        "# print(f\"  MAE:  {test_mae_final_krr:.4f}\")\n",
        "# print(f\"  MSE:  {test_mse_final_krr:.4f}\")\n",
        "# print(f\"  R2:   {test_r2_final_krr:.4f}\")\n",
        "# print(f\"  MAPE: {test_mape_final_krr:.4f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "dZyvVT5xiX5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SGDRegressor"
      ],
      "metadata": {
        "id": "uTj3I8f6jOGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Implementa SGDRegressor en Python con sklearn, optimiza alpha con learning_rate='invscaling' usando GridSearchCV, RandomizedSearchCV y BayesSearchCV de skopt, y reporta MAE, MSE, R2 y MAPE con validación cruzada de 5 folds.\"\n",
        "# Rango: alpha = [0.0001, 0.001] (Grid); alpha loguniform(0.0001, 0.01) (Random/Bayesian).\n",
        "# Justificación: Alpha regula regularización, learning_rate fijo para estabilidad; rangos pequeños para evitar overfitting.\n",
        "\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "# Ensure scorers and kf are defined (as per the original code context)\n",
        "# (assuming the previous cells defining these have been run)\n",
        "\n",
        "# Define the SGDRegressor model\n",
        "# Use learning_rate='invscaling' and set early_stopping=True for better performance and stability\n",
        "sgd_regressor = SGDRegressor(learning_rate='invscaling', early_stopping=True,\n",
        "                             random_state=42, max_iter=10000) # Increased max_iter\n",
        "\n",
        "# --- Optimization with GridSearchCV ---\n",
        "print(\"\\nIniciando optimización con GridSearchCV para SGDRegressor...\")\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "# Range: alpha = [0.0001, 0.001]\n",
        "param_grid_sgd = {\n",
        "    'alpha': [0.0001, 0.001]\n",
        "}\n",
        "\n",
        "grid_search_sgd = GridSearchCV(estimator=sgd_regressor, param_grid=param_grid_sgd,\n",
        "                               scoring=scoring_optimizer, refit='mae', # Optimize using MAE\n",
        "                               cv=kf, verbose=1, n_jobs=-1)\n",
        "\n",
        "start_time_grid_sgd = time.time()\n",
        "grid_search_sgd.fit(X_train, y_train)\n",
        "end_time_grid_sgd = time.time()\n",
        "\n",
        "print(\"\\nResultados de GridSearchCV para SGDRegressor:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {grid_search_sgd.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-grid_search_sgd.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de GridSearchCV: {end_time_grid_sgd - start_time_grid_sgd:.2f} segundos\")\n",
        "\n",
        "best_sgd_grid = grid_search_sgd.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de GridSearchCV para SGDRegressor con validación cruzada completa:\")\n",
        "cv_results_grid_best_sgd = cross_validate(best_sgd_grid, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_grid_sgd = -cv_results_grid_best_sgd['test_mae']\n",
        "cv_mse_grid_sgd = -cv_results_grid_best_sgd['test_mse']\n",
        "cv_r2_grid_sgd = cv_results_grid_best_sgd['test_r2']\n",
        "cv_mape_grid_sgd = -cv_results_grid_best_sgd['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_grid_sgd.mean():.4f} +/- {cv_mae_grid_sgd.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_grid_sgd.mean():.4f} +/- {cv_mse_grid_sgd.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_grid_sgd.mean():.4f} +/- {cv_r2_grid_sgd.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_grid_sgd.mean():.4f}% +/- {cv_mape_grid_sgd.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Optimization with RandomizedSearchCV ---\n",
        "print(\"\\nIniciando optimización con RandomizedSearchCV para SGDRegressor...\")\n",
        "\n",
        "# Define parameter distribution for RandomizedSearchCV\n",
        "# Range: alpha loguniform(0.0001, 0.01)\n",
        "param_dist_sgd = {\n",
        "    'alpha': loguniform(0.0001, 0.01)\n",
        "}\n",
        "\n",
        "# Number of iterations (adjust as needed)\n",
        "n_iter_rand_sgd = 50 # Example: try 50 random combinations\n",
        "\n",
        "random_search_sgd = RandomizedSearchCV(estimator=sgd_regressor, param_distributions=param_dist_sgd,\n",
        "                                       n_iter=n_iter_rand_sgd,\n",
        "                                       scoring=scoring_optimizer, refit='mae', # Optimize using MAE\n",
        "                                       cv=kf, verbose=1, random_state=42, n_jobs=-1) # random_state para reproducibilidad\n",
        "\n",
        "start_time_rand_sgd = time.time()\n",
        "random_search_sgd.fit(X_train, y_train)\n",
        "end_time_rand_sgd = time.time()\n",
        "\n",
        "print(\"\\nResultados de RandomizedSearchCV para SGDRegressor:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {random_search_sgd.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-random_search_sgd.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de RandomizedSearchCV: {end_time_rand_sgd - start_time_rand_sgd:.2f} segundos\")\n",
        "\n",
        "best_sgd_rand = random_search_sgd.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de RandomizedSearchCV para SGDRegressor con validación cruzada completa:\")\n",
        "cv_results_rand_best_sgd = cross_validate(best_sgd_rand, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_rand_sgd = -cv_results_rand_best_sgd['test_mae']\n",
        "cv_mse_rand_sgd = -cv_results_rand_best_sgd['test_mse']\n",
        "cv_r2_rand_sgd = cv_results_rand_best_sgd['test_r2']\n",
        "cv_mape_rand_sgd = -cv_results_rand_best_sgd['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_rand_sgd.mean():.4f} +/- {cv_mae_rand_sgd.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_rand_sgd.mean():.4f} +/- {cv_mse_rand_sgd.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_rand_sgd.mean():.4f} +/- {cv_r2_rand_sgd.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_rand_sgd.mean():.4f}% +/- {cv_mape_rand_sgd.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Optimization with BayesSearchCV ---\n",
        "print(\"\\nIniciando optimización con BayesSearchCV para SGDRegressor...\")\n",
        "\n",
        "# Define search spaces for BayesSearchCV\n",
        "# Range: alpha loguniform(0.0001, 0.01)\n",
        "search_spaces_sgd = {\n",
        "    'alpha': Real(0.0001, 0.01, prior='log-uniform')\n",
        "}\n",
        "\n",
        "# Number of iterations (adjust as needed)\n",
        "n_iter_bayes_sgd = 50 # Example: try 50 iterations\n",
        "\n",
        "bayes_search_sgd = BayesSearchCV(estimator=sgd_regressor, search_spaces=search_spaces_sgd,\n",
        "                                 n_iter=n_iter_bayes_sgd,\n",
        "                                 scoring=scoring_optimizer, refit='mae', # Optimize using MAE\n",
        "                                 cv=kf, verbose=1, random_state=42, n_jobs=-1)\n",
        "\n",
        "start_time_bayes_sgd = time.time()\n",
        "bayes_search_sgd.fit(X_train, y_train)\n",
        "end_time_bayes_sgd = time.time()\n",
        "\n",
        "print(\"\\nResultados de BayesSearchCV para SGDRegressor:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {bayes_search_sgd.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-bayes_search_sgd.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de BayesSearchCV: {end_time_bayes_sgd - start_time_bayes_sgd:.2f} segundos\")\n",
        "\n",
        "best_sgd_bayes = bayes_search_sgd.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de BayesSearchCV para SGDRegressor con validación cruzada completa:\")\n",
        "cv_results_bayes_best_sgd = cross_validate(best_sgd_bayes, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_bayes_sgd = -cv_results_bayes_best_sgd['test_mae']\n",
        "cv_mse_bayes_sgd = -cv_results_bayes_best_sgd['test_mse']\n",
        "cv_r2_bayes_sgd = cv_results_bayes_best_sgd['test_r2']\n",
        "cv_mape_bayes_sgd = -cv_results_bayes_best_sgd['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_bayes_sgd.mean():.4f} +/- {cv_mae_bayes_sgd.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_bayes_sgd.mean():.4f} +/- {cv_mse_bayes_sgd.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_bayes_sgd.mean():.4f} +/- {cv_r2_bayes_sgd.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_bayes_sgd.mean():.4f}% +/- {cv_mape_bayes_sgd.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Comparación de resultados de SGDRegressor ---\n",
        "print(\"\\n--- Resumen de Resultados de Optimización para SGDRegressor (MAE promedio en validación) ---\")\n",
        "print(f\"GridSearchCV:      {-grid_search_sgd.best_score_:.4f}\")\n",
        "print(f\"RandomizedSearchCV: {-random_search_sgd.best_score_:.4f}\")\n",
        "print(f\"BayesSearchCV:      {-bayes_search_sgd.best_score_:.4f}\")\n",
        "\n",
        "print(\"\\n--- Reporte Completo de Métricas para SGDRegressor (Promedio de 5-fold CV) ---\")\n",
        "\n",
        "print(\"\\nGridSearchCV Mejor Modelo (SGDRegressor):\")\n",
        "print(f\"  MAE:  {cv_mae_grid_sgd.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_grid_sgd.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_grid_sgd.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_grid_sgd.mean():.4f}%\")\n",
        "\n",
        "print(\"\\nRandomizedSearchCV Mejor Modelo (SGDRegressor):\")\n",
        "print(f\"  MAE:  {cv_mae_rand_sgd.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_rand_sgd.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_rand_sgd.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_rand_sgd.mean():.4f}%\")\n",
        "\n",
        "print(\"\\nBayesSearchCV Mejor Modelo (SGDRegressor):\")\n",
        "print(f\"  MAE:  {cv_mae_bayes_sgd.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_bayes_sgd.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_bayes_sgd.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_bayes_sgd.mean():.4f}%\")\n",
        "\n",
        "# You can select the overall best SGDRegressor model based on validation MAE\n",
        "# For instance, if BayesSearchCV gave the best result:\n",
        "# final_best_sgd_model = best_sgd_bayes\n",
        "# print(\"\\nEl mejor modelo SGDRegressor general basado en MAE promedio de validación es el de BayesSearchCV.\")\n",
        "\n",
        "# Optional: Train the chosen best SGDRegressor model on the entire X_train and evaluate on X_test\n",
        "# final_best_sgd_model = best_sgd_bayes # Or grid_search_sgd.best_estimator_ or random_search_sgd.best_estimator_\n",
        "\n",
        "# print(\"\\nEntrenando el modelo SGDRegressor final seleccionado en el conjunto de entrenamiento completo...\")\n",
        "# final_best_sgd_model.fit(X_train, y_train)\n",
        "# print(\"Evaluando en el conjunto de prueba (X_test) con el modelo SGDRegressor final...\")\n",
        "# y_pred_test_final_sgd = final_best_sgd_model.predict(X_test)\n",
        "\n",
        "# # Calculate metrics on the test set with the final SGDRegressor model\n",
        "# test_mae_final_sgd = mean_absolute_error(y_test, y_pred_test_final_sgd)\n",
        "# test_mse_final_sgd = mean_squared_error(y_test, y_pred_test_final_sgd)\n",
        "# test_r2_final_sgd = r2_score(y_test, y_pred_test_final_sgd)\n",
        "# test_mape_final_sgd = mean_absolute_percentage_error(y_test, y_pred_test_final_sgd)\n",
        "\n",
        "# print(\"\\nMétricas en el conjunto de prueba con el modelo SGDRegressor final seleccionado:\")\n",
        "# print(f\"  MAE:  {test_mae_final_sgd:.4f}\")\n",
        "# print(f\"  MSE:  {test_mse_final_sgd:.4f}\")\n",
        "# print(f\"  R2:   {test_r2_final_sgd:.4f}\")\n",
        "# print(f\"  MAPE: {test_mape_final_sgd:.4f}%\")\n"
      ],
      "metadata": {
        "id": "-OFHH83SjW3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BayesianRidge"
      ],
      "metadata": {
        "id": "nMwIZ9S_jxa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Implementa BayesianRidge en Python con sklearn, usa validación cruzada de 5 folds, y reporta MAE, MSE, R2 y MAPE, sin optimización de hiperparámetros.\"\n",
        "# Rango: n_iter = [300] (Grid); no aplica para Random/Bayesian.\n",
        "# Justificación: Robustez a multicolinealidad, parámetros predeterminados son suficientes.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "\n",
        "# Ensure scorers and kf are defined (as per the original code context)\n",
        "# (assuming the previous cells defining these have been run)\n",
        "\n",
        "# Redefine the custom MAPE scorer if it wasn't in the preceding code or is needed again\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    \"\"\"Calcula el Error Porcentual Absoluto Medio (MAPE).\"\"\"\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    epsilon = 1e-8\n",
        "    return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
        "\n",
        "# Create scorer objects for the metrics\n",
        "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
        "r2_scorer = make_scorer(r2_score, greater_is_better=True)\n",
        "mape_scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
        "\n",
        "# Define the scoring dictionary for cross_validate\n",
        "scoring = {\n",
        "    'mae': mae_scorer,\n",
        "    'mse': mse_scorer,\n",
        "    'r2': r2_scorer,\n",
        "    'mape': mape_scorer\n",
        "}\n",
        "\n",
        "# Define the BayesianRidge model with default parameters\n",
        "# n_iter=300 as specified, but the prompt says \"sin optimización de hiperparámetros\",\n",
        "# and n_iter doesn't apply to default sklearn BayesianRidge without tuning.\n",
        "# Let's use the default parameters as requested by \"sin optimización de hiperparámetros\".\n",
        "# If n_iter was a requirement, we'd clarify or set it, but the prompt suggests default.\n",
        "# The default max_iter for BayesianRidge is 300, so it aligns if we were to set it,\n",
        "# but sticking to \"sin optimización\" implies using the model as is.\n",
        "model = BayesianRidge()\n",
        "\n",
        "# Perform cross-validation with 5 folds (kf)\n",
        "print(\"\\nRealizando validación cruzada con BayesianRidge (parámetros predeterminados)...\")\n",
        "\n",
        "# Use cross_validate to get multiple metrics simultaneously\n",
        "cv_results = cross_validate(model, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "# Access the results arrays\n",
        "# Error metrics (MAE, MSE, MAPE) are negative because make_scorer optimizes by maximizing.\n",
        "# Take the absolute value for reporting.\n",
        "cv_mae = -cv_results['test_mae']\n",
        "cv_mse = -cv_results['test_mse']\n",
        "cv_r2 = cv_results['test_r2']\n",
        "cv_mape = -cv_results['test_mape']\n",
        "\n",
        "# Report the average and standard deviation of each metric\n",
        "print(f\"\\nResultados de Validación Cruzada para BayesianRidge (promedio +/- std dev):\")\n",
        "print(f\"  MAE:  {cv_mae.mean():.4f} +/- {cv_mae.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse.mean():.4f} +/- {cv_mse.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2.mean():.4f} +/- {cv_r2.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape.mean():.4f}% +/- {cv_mape.std():.4f}%\")\n",
        "\n",
        "# Optional: Train the model on the entire training set and evaluate on the test set\n",
        "# This step is typically performed after model selection and hyperparameter tuning,\n",
        "# but can be done here to show performance on unseen data with default parameters.\n",
        "# print(\"\\nEntrenando modelo final BayesianRidge en el conjunto de entrenamiento completo...\")\n",
        "# model.fit(X_train, y_train)\n",
        "# print(\"Evaluando en el conjunto de prueba...\")\n",
        "# y_pred_test = model.predict(X_test)\n",
        "\n",
        "# # Calculate metrics on the test set\n",
        "# test_mae = mean_absolute_error(y_test, y_pred_test)\n",
        "# test_mse = mean_squared_error(y_test, y_pred_test)\n",
        "# test_r2 = r2_score(y_test, y_pred_test)\n",
        "# test_mape = mean_absolute_percentage_error(y_test, y_pred_test)\n",
        "\n",
        "# print(\"\\nMétricas en el conjunto de prueba (BayesianRidge con parámetros predeterminados):\")\n",
        "# print(f\"  MAE:  {test_mae:.4f}\")\n",
        "# print(f\"  MSE:  {test_mse:.4f}\")\n",
        "# print(f\"  R2:   {test_r2:.4f}\")\n",
        "# print(f\"  MAPE: {test_mape:.4f}%\")"
      ],
      "metadata": {
        "id": "KmjvCGJ3j4Ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GaussianProcessRegressor"
      ],
      "metadata": {
        "id": "TNsIVmcKkF8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Implementa GaussianProcessRegressor en Python con sklearn, optimiza length_scale y alpha para kernel RBF usando GridSearchCV, RandomizedSearchCV y BayesSearchCV de skopt, y reporta MAE, MSE, R2 y MAPE con validación cruzada de 5 folds.\"\n",
        "# # # Rango: length_scale = [0.1, 1], alpha = [0.001] (Grid); length_scale loguniform(0.1, 2), alpha loguniform(0.001, 0.1) (Random/Bayesian).\n",
        "# # # Justificación: Length_scale ajusta correlación, alpha controla ruido; rangos logarítmicos para capturar variabilidad, alpha fijo pequeño para estabilidad.\n",
        "# # Organízame el código para que al momento realizar la optimización de los hiperparámetros, no use tantos fits y no se demore\n",
        "\n",
        "# Instalar scikit-optimize si no está instalado\n",
        "!pip install scikit-optimize\n",
        "\n",
        "import time\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "from sklearn.model_selection import cross_validate, KFold\n",
        "from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real\n",
        "from scipy.stats import loguniform\n",
        "\n",
        "# Suponiendo que X_train, y_train, X_test, y_test están definidos\n",
        "# Definir KFold para validación cruzada\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Definir métricas para evaluación\n",
        "scoring_optimizer = {'mae': make_scorer(mean_absolute_error, greater_is_better=False)}\n",
        "scoring = {\n",
        "    'mae': make_scorer(mean_absolute_error, greater_is_better=False),\n",
        "    'mse': make_scorer(mean_squared_error, greater_is_better=False),\n",
        "    'r2': make_scorer(r2_score, greater_is_better=True),\n",
        "    'mape': make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
        "}\n",
        "\n",
        "# Definir el modelo con menos reinicios\n",
        "kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (0.1, 2.0))\n",
        "gpr = GaussianProcessRegressor(kernel=kernel, random_state=42, n_restarts_optimizer=5)\n",
        "\n",
        "# --- Optimización con GridSearchCV ---\n",
        "print(\"\\nIniciando optimización con GridSearchCV...\")\n",
        "param_grid_gpr = {\n",
        "    'kernel__k2__length_scale': [0.1, 1.0],\n",
        "    'alpha': [0.001]\n",
        "}\n",
        "grid_search_gpr = GridSearchCV(estimator=gpr, param_grid=param_grid_gpr,\n",
        "                               scoring=scoring_optimizer, refit='mae',\n",
        "                               cv=kf, verbose=1, n_jobs=-1)\n",
        "start_time_grid_gpr = time.time()\n",
        "grid_search_gpr.fit(X_train, y_train)\n",
        "end_time_grid_gpr = time.time()\n",
        "\n",
        "print(\"\\nResultados de GridSearchCV:\")\n",
        "print(f\"Mejores hiperparámetros: {grid_search_gpr.best_params_}\")\n",
        "print(f\"Mejor MAE promedio: {-grid_search_gpr.best_score_:.4f}\")\n",
        "print(f\"Tiempo: {end_time_grid_gpr - start_time_grid_gpr:.2f} segundos\")\n",
        "\n",
        "# --- Optimización con RandomizedSearchCV ---\n",
        "print(\"\\nIniciando optimización con RandomizedSearchCV...\")\n",
        "param_dist_gpr = {\n",
        "    'kernel__k2__length_scale': loguniform(0.1, 2.0),\n",
        "    'alpha': loguniform(0.001, 0.1)\n",
        "}\n",
        "random_search_gpr = RandomizedSearchCV(estimator=gpr, param_distributions=param_dist_gpr,\n",
        "                                      n_iter=10, scoring=scoring_optimizer, refit='mae',\n",
        "                                      cv=kf, verbose=1, random_state=42, n_jobs=-1)\n",
        "start_time_rand_gpr = time.time()\n",
        "random_search_gpr.fit(X_train, y_train)\n",
        "end_time_rand_gpr = time.time()\n",
        "\n",
        "print(\"\\nResultados de RandomizedSearchCV:\")\n",
        "print(f\"Mejores hiperparámetros: {random_search_gpr.best_params_}\")\n",
        "print(f\"Mejor MAE promedio: {-random_search_gpr.best_score_:.4f}\")\n",
        "print(f\"Tiempo: {end_time_rand_gpr - start_time_rand_gpr:.2f} segundos\")\n",
        "\n",
        "# --- Optimización con BayesSearchCV ---\n",
        "print(\"\\nIniciando optimización con BayesSearchCV...\")\n",
        "search_spaces_gpr = {\n",
        "    'kernel__k2__length_scale': Real(0.1, 2.0, prior='log-uniform'),\n",
        "    'alpha': Real(0.001, 0.1, prior='log-uniform')\n",
        "}\n",
        "bayes_search_gpr = BayesSearchCV(estimator=gpr, search_spaces=search_spaces_gpr,\n",
        "                                 n_iter=10, scoring=scoring_optimizer, refit='mae',\n",
        "                                 cv=kf, verbose=1, random_state=42, n_jobs=-1)\n",
        "start_time_bayes_gpr = time.time()\n",
        "bayes_search_gpr.fit(X_train, y_train)\n",
        "end_time_bayes_gpr = time.time()\n",
        "\n",
        "print(\"\\nResultados de BayesSearchCV:\")\n",
        "print(f\"Mejores hiperparámetros: {bayes_search_gpr.best_params_}\")\n",
        "print(f\"Mejor MAE promedio: {-bayes_search_gpr.best_score_:.4f}\")\n",
        "print(f\"Tiempo: {end_time_bayes_gpr - start_time_bayes_gpr:.2f} segundos\")\n",
        "\n",
        "# Evaluación final del mejor modelo (BayesSearchCV) en el conjunto completo\n",
        "best_gpr = bayes_search_gpr.best_estimator_\n",
        "cv_results_best_gpr = cross_validate(best_gpr, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "print(\"\\nMétricas del mejor modelo (BayesSearchCV) con validación cruzada:\")\n",
        "print(f\"  MAE:  {-cv_results_best_gpr['test_mae'].mean():.4f} +/- {cv_results_best_gpr['test_mae'].std():.4f}\")\n",
        "print(f\"  MSE:  {-cv_results_best_gpr['test_mse'].mean():.4f} +/- {cv_results_best_gpr['test_mse'].std():.4f}\")\n",
        "print(f\"  R2:   {cv_results_best_gpr['test_r2'].mean():.4f} +/- {cv_results_best_gpr['test_r2'].std():.4f}\")\n",
        "print(f\"  MAPE: {-cv_results_best_gpr['test_mape'].mean():.4f}% +/- {cv_results_best_gpr['test_mape'].std():.4f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OXKwdmI0qIaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RandomForestRegressor"
      ],
      "metadata": {
        "id": "kDTO6-FMmd1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Implementa RandomForestRegressor en Python con sklearn, optimiza n_estimators, max_depth y min_samples_split usando GridSearchCV, RandomizedSearchCV y BayesSearchCV de skopt, y reporta MAE, MSE, R2 y MAPE con validación cruzada de 5 folds.\"\n",
        "# Rango: n_estimators = [50, 256], max_depth = [None, 16], min_samples_split = [2, 10] (Grid); n_estimators randint(56, 256), max_depth [None] + list(range(5, 30)), min_samples_split randint(2, 25) (Random); n_estimators Integer(50, 256), max_depth Integer(5, 35), min_samples_split Integer(2, 25) (Bayesian).\n",
        "# Justificación: n_estimators controla ensamblado, max_depth y min_samples_split evitan overfitting; rangos amplios para explorar complejidad.\n",
        "\n",
        "# Importaciones necesarias\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Integer\n",
        "from scipy.stats import randint\n",
        "import time\n",
        "from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
        "\n",
        "# Suponiendo que X_train, y_train están definidos\n",
        "# Definir KFold para validación cruzada\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Definir métricas para evaluación\n",
        "scoring_optimizer = {'mae': make_scorer(mean_absolute_error, greater_is_better=False)}\n",
        "scoring = {\n",
        "    'mae': make_scorer(mean_absolute_error, greater_is_better=False),\n",
        "    'mse': make_scorer(mean_squared_error, greater_is_better=False),\n",
        "    'r2': make_scorer(r2_score, greater_is_better=True),\n",
        "    'mape': make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
        "}\n",
        "\n",
        "# Definir el modelo RandomForestRegressor\n",
        "rf_regressor = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "\n",
        "# --- Optimización con GridSearchCV ---\n",
        "print(\"\\nIniciando optimización con GridSearchCV para RandomForestRegressor...\")\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100],  # Reducido para acelerar\n",
        "    'max_depth': [None, 10],    # Reducido para acelerar\n",
        "    'min_samples_split': [2, 5] # Reducido para acelerar\n",
        "}\n",
        "grid_search_rf = GridSearchCV(estimator=rf_regressor, param_grid=param_grid_rf,\n",
        "                              scoring=scoring_optimizer, refit='mae',\n",
        "                              cv=kf, verbose=1, n_jobs=-1)\n",
        "start_time_grid_rf = time.time()\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "end_time_grid_rf = time.time()\n",
        "\n",
        "print(\"\\nResultados de GridSearchCV para RandomForestRegressor:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {grid_search_rf.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-grid_search_rf.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de GridSearchCV: {end_time_grid_rf - start_time_grid_rf:.2f} segundos\")\n",
        "\n",
        "# --- Optimización con RandomizedSearchCV ---\n",
        "print(\"\\nIniciando optimización con RandomizedSearchCV para RandomForestRegressor...\")\n",
        "param_dist_rf = {\n",
        "    'n_estimators': randint(50, 150),  # Rango reducido\n",
        "    'max_depth': [None] + list(range(5, 15)),  # Rango reducido\n",
        "    'min_samples_split': randint(2, 10)  # Rango reducido\n",
        "}\n",
        "random_search_rf = RandomizedSearchCV(estimator=rf_regressor, param_distributions=param_dist_rf,\n",
        "                                      n_iter=10,  # Reducido de 50 a 10\n",
        "                                      scoring=scoring_optimizer, refit='mae',\n",
        "                                      cv=kf, verbose=1, random_state=42, n_jobs=-1)\n",
        "start_time_rand_rf = time.time()\n",
        "random_search_rf.fit(X_train, y_train)\n",
        "end_time_rand_rf = time.time()\n",
        "\n",
        "print(\"\\nResultados de RandomizedSearchCV para RandomForestRegressor:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {random_search_rf.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-random_search_rf.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de RandomizedSearchCV: {end_time_rand_rf - start_time_rand_rf:.2f} segundos\")\n",
        "\n",
        "# --- Optimización con BayesSearchCV ---\n",
        "print(\"\\nIniciando optimización con BayesSearchCV para RandomForestRegressor...\")\n",
        "search_spaces_rf = {\n",
        "    'n_estimators': Integer(50, 150),  # Rango reducido\n",
        "    'max_depth': Integer(5, 15),       # Rango reducido\n",
        "    'min_samples_split': Integer(2, 10)  # Rango reducido\n",
        "}\n",
        "bayes_search_rf = BayesSearchCV(estimator=rf_regressor, search_spaces=search_spaces_rf,\n",
        "                                n_iter=10,  # Reducido de 50 a 10\n",
        "                                scoring=scoring_optimizer, refit='mae',\n",
        "                                cv=kf, verbose=1, random_state=42, n_jobs=-1)\n",
        "start_time_bayes_rf = time.time()\n",
        "bayes_search_rf.fit(X_train, y_train)\n",
        "end_time_bayes_rf = time.time()\n",
        "\n",
        "print(\"\\nResultados de BayesSearchCV para RandomForestRegressor:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {bayes_search_rf.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-bayes_search_rf.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de BayesSearchCV: {end_time_bayes_rf - start_time_bayes_rf:.2f} segundos\")\n",
        "\n",
        "# Evaluación final del mejor modelo (BayesSearchCV) en el conjunto completo\n",
        "best_rf = bayes_search_rf.best_estimator_\n",
        "cv_results_best_rf = cross_validate(best_rf, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "print(\"\\nMétricas del mejor modelo (BayesSearchCV) con validación cruzada:\")\n",
        "print(f\"  MAE:  {-cv_results_best_rf['test_mae'].mean():.4f} +/- {cv_results_best_rf['test_mae'].std():.4f}\")\n",
        "print(f\"  MSE:  {-cv_results_best_rf['test_mse'].mean():.4f} +/- {cv_results_best_rf['test_mse'].std():.4f}\")\n",
        "print(f\"  R2:   {cv_results_best_rf['test_r2'].mean():.4f} +/- {cv_results_best_rf['test_r2'].std():.4f}\")\n",
        "print(f\"  MAPE: {-cv_results_best_rf['test_mape'].mean():.4f}% +/- {cv_results_best_rf['test_mape'].std():.4f}%\")\n"
      ],
      "metadata": {
        "id": "2DcaxUFomjBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVR"
      ],
      "metadata": {
        "id": "TWYOIGvhmj6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: \"Implementa SVR en Python con sklearn, optimiza C, epsilon y gamma para kernel RBF usando GridSearchCV, RandomizedSearchCV y BayesSearchCV de skopt, y reporta MAE, MSE, R2 y MAPE con validación cruzada de 5 folds.\"\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "from scipy.stats import uniform, randint # Ensure randint is imported for Random Forest later if needed\n",
        "from skopt.space import Real, Integer, Categorical # Ensure Integer and Categorical are imported if used later\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "\n",
        "# Ensure scorers and kf are defined (as per the original code context)\n",
        "# (assuming the previous cells defining these have been run)\n",
        "\n",
        "# Redefine the custom MAPE scorer if it wasn't in the preceding code or is needed again\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    \"\"\"Calcula el Error Porcentual Absoluto Medio (MAPE).\"\"\"\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    epsilon = 1e-8\n",
        "    return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
        "\n",
        "# Create scorer objects for the metrics\n",
        "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
        "r2_scorer = make_scorer(r2_score, greater_is_better=True)\n",
        "mape_scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
        "\n",
        "# Define the scoring dictionary for cross_validate and optimizers\n",
        "# We will optimize based on MAE, so define a single scorer for the optimizers\n",
        "scoring_optimizer = {'mae': mae_scorer}\n",
        "\n",
        "# Define the full scoring dictionary for reporting results\n",
        "scoring = {\n",
        "    'mae': mae_scorer,\n",
        "    'mse': mse_scorer,\n",
        "    'r2': r2_scorer,\n",
        "    'mape': mape_scorer\n",
        "}\n",
        "\n",
        "\n",
        "# Definir el modelo SVR con kernel RBF\n",
        "svr = SVR(kernel='rbf')\n",
        "\n",
        "# Assuming X_train, y_train, and kf are defined in previous cells\n",
        "\n",
        "# --- Optimization with GridSearchCV ---\n",
        "print(\"\\nIniciando optimización con GridSearchCV para SVR (kernel RBF)...\")\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "# C = [1, 10, 100], epsilon = [0.01, 0.1, 0.5], gamma = ['scale', 'auto', 0.001]\n",
        "param_grid_svr = {\n",
        "    'C': [1, 10, 100],\n",
        "    'epsilon': [0.01, 0.1, 0.5],\n",
        "    'gamma': ['scale', 'auto', 0.001] # 'scale' and 'auto' are also valid values\n",
        "}\n",
        "\n",
        "grid_search_svr = GridSearchCV(estimator=svr, param_grid=param_grid_svr,\n",
        "                               scoring=scoring_optimizer, refit='mae', # Optimize using MAE\n",
        "                               cv=kf, verbose=1, n_jobs=-1)\n",
        "\n",
        "start_time_grid_svr = time.time()\n",
        "grid_search_svr.fit(X_train, y_train)\n",
        "end_time_grid_svr = time.time()\n",
        "\n",
        "print(\"\\nResultados de GridSearchCV para SVR:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {grid_search_svr.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-grid_search_svr.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de GridSearchCV: {end_time_grid_svr - start_time_grid_svr:.2f} segundos\")\n",
        "\n",
        "best_svr_grid = grid_search_svr.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de GridSearchCV para SVR con validación cruzada completa:\")\n",
        "cv_results_grid_best_svr = cross_validate(best_svr_grid, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_grid_svr = -cv_results_grid_best_svr['test_mae']\n",
        "cv_mse_grid_svr = -cv_results_grid_best_svr['test_mse']\n",
        "cv_r2_grid_svr = cv_results_grid_best_svr['test_r2']\n",
        "cv_mape_grid_svr = -cv_results_grid_best_svr['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_grid_svr.mean():.4f} +/- {cv_mae_grid_svr.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_grid_svr.mean():.4f} +/- {cv_mse_grid_svr.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_grid_svr.mean():.4f} +/- {cv_r2_grid_svr.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_grid_svr.mean():.4f}% +/- {cv_mape_grid_svr.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Optimization with RandomizedSearchCV ---\n",
        "print(\"\\nIniciando optimización con RandomizedSearchCV para SVR (kernel RBF)...\")\n",
        "\n",
        "# Define parameter distribution for RandomizedSearchCV\n",
        "# C loguniform(1, 1000), epsilon uniform(0, 1), gamma loguniform(0.0001, 1)\n",
        "param_dist_svr = {\n",
        "    'C': loguniform(1, 1000),\n",
        "    'epsilon': uniform(0, 1),\n",
        "    'gamma': loguniform(0.0001, 1)\n",
        "}\n",
        "\n",
        "# Number of iterations (puntos a probar). Ajusta este valor según el tiempo disponible.\n",
        "n_iter_rand_svr = 100 # Example: try 100 random combinations\n",
        "\n",
        "random_search_svr = RandomizedSearchCV(estimator=svr, param_distributions=param_dist_svr,\n",
        "                                       n_iter=n_iter_rand_svr,\n",
        "                                       scoring=scoring_optimizer, refit='mae', # Optimize using MAE\n",
        "                                       cv=kf, verbose=1, random_state=42, n_jobs=-1) # random_state para reproducibilidad\n",
        "\n",
        "start_time_rand_svr = time.time()\n",
        "random_search_svr.fit(X_train, y_train)\n",
        "end_time_rand_svr = time.time()\n",
        "\n",
        "print(\"\\nResultados de RandomizedSearchCV para SVR:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {random_search_svr.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-random_search_svr.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de RandomizedSearchCV: {end_time_rand_svr - start_time_rand_svr:.2f} segundos\")\n",
        "\n",
        "best_svr_rand = random_search_svr.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de RandomizedSearchCV para SVR con validación cruzada completa:\")\n",
        "cv_results_rand_best_svr = cross_validate(best_svr_rand, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_rand_svr = -cv_results_rand_best_svr['test_mae']\n",
        "cv_mse_rand_svr = -cv_results_rand_best_svr['test_mse']\n",
        "cv_r2_rand_svr = cv_results_rand_best_svr['test_r2']\n",
        "cv_mape_rand_svr = -cv_results_rand_best_svr['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_rand_svr.mean():.4f} +/- {cv_mae_rand_svr.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_rand_svr.mean():.4f} +/- {cv_mse_rand_svr.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_rand_svr.mean():.4f} +/- {cv_r2_rand_svr.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_rand_svr.mean():.4f}% +/- {cv_mape_rand_svr.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Optimization with BayesSearchCV ---\n",
        "print(\"\\nIniciando optimización con BayesSearchCV para SVR (kernel RBF)...\")\n",
        "\n",
        "# Define search spaces for BayesSearchCV\n",
        "# C Real(1e-1, 1e3, prior='log-uniform'), epsilon Real(1e-3, 1e0, prior='uniform'), gamma Real(1e-4, 1e0, prior='log-uniform')\n",
        "search_spaces_svr = {\n",
        "    'C': Real(1e-1, 1e3, prior='log-uniform'),\n",
        "    'epsilon': Real(1e-3, 1e0, prior='uniform'),\n",
        "    'gamma': Real(1e-4, 1e0, prior='log-uniform')\n",
        "}\n",
        "\n",
        "# Number of iterations (puntos a explorar). Generalmente requiere menos que RandomizedSearch.\n",
        "n_iter_bayes_svr = 100 # Example: try 100 iterations. Adjust based on computational resources.\n",
        "\n",
        "bayes_search_svr = BayesSearchCV(estimator=svr, search_spaces=search_spaces_svr,\n",
        "                                 n_iter=n_iter_bayes_svr,\n",
        "                                 scoring=scoring_optimizer, refit='mae', # Optimize using MAE\n",
        "                                 cv=kf, verbose=1, random_state=42, n_jobs=-1)\n",
        "\n",
        "start_time_bayes_svr = time.time()\n",
        "bayes_search_svr.fit(X_train, y_train)\n",
        "end_time_bayes_svr = time.time()\n",
        "\n",
        "print(\"\\nResultados de BayesSearchCV para SVR:\")\n",
        "print(f\"Mejores hiperparámetros encontrados: {bayes_search_svr.best_params_}\")\n",
        "print(f\"Mejor MAE promedio en validación: {-bayes_search_svr.best_score_:.4f}\")\n",
        "print(f\"Tiempo de ejecución de BayesSearchCV: {end_time_bayes_svr - start_time_bayes_svr:.2f} segundos\")\n",
        "\n",
        "best_svr_bayes = bayes_search_svr.best_estimator_\n",
        "\n",
        "print(\"\\nEvaluando el mejor modelo de BayesSearchCV para SVR con validación cruzada completa:\")\n",
        "cv_results_bayes_best_svr = cross_validate(best_svr_bayes, X_train, y_train, cv=kf, scoring=scoring)\n",
        "\n",
        "cv_mae_bayes_svr = -cv_results_bayes_best_svr['test_mae']\n",
        "cv_mse_bayes_svr = -cv_results_bayes_best_svr['test_mse']\n",
        "cv_r2_bayes_svr = cv_results_bayes_best_svr['test_r2']\n",
        "cv_mape_bayes_svr = -cv_results_bayes_best_svr['test_mape']\n",
        "\n",
        "print(f\"  MAE:  {cv_mae_bayes_svr.mean():.4f} +/- {cv_mae_bayes_svr.std():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_bayes_svr.mean():.4f} +/- {cv_mse_bayes_svr.std():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_bayes_svr.mean():.4f} +/- {cv_r2_bayes_svr.std():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_bayes_svr.mean():.4f}% +/- {cv_mape_bayes_svr.std():.4f}%\")\n",
        "\n",
        "\n",
        "# --- Comparación de resultados de SVR ---\n",
        "print(\"\\n--- Resumen de Resultados de Optimización para SVR (MAE promedio en validación) ---\")\n",
        "print(f\"GridSearchCV:      {-grid_search_svr.best_score_:.4f}\")\n",
        "print(f\"RandomizedSearchCV: {-random_search_svr.best_score_:.4f}\")\n",
        "print(f\"BayesSearchCV:      {-bayes_search_svr.best_score_:.4f}\")\n",
        "\n",
        "print(\"\\n--- Reporte Completo de Métricas para SVR (Promedio de 5-fold CV) ---\")\n",
        "\n",
        "print(\"\\nGridSearchCV Mejor Modelo (SVR):\")\n",
        "print(f\"  MAE:  {cv_mae_grid_svr.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_grid_svr.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_grid_svr.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_grid_svr.mean():.4f}%\")\n",
        "\n",
        "print(\"\\nRandomizedSearchCV Mejor Modelo (SVR):\")\n",
        "print(f\"  MAE:  {cv_mae_rand_svr.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_rand_svr.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_rand_svr.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_rand_svr.mean():.4f}%\")\n",
        "\n",
        "print(\"\\nBayesSearchCV Mejor Modelo (SVR):\")\n",
        "print(f\"  MAE:  {cv_mae_bayes_svr.mean():.4f}\")\n",
        "print(f\"  MSE:  {cv_mse_bayes_svr.mean():.4f}\")\n",
        "print(f\"  R2:   {cv_r2_bayes_svr.mean():.4f}\")\n",
        "print(f\"  MAPE: {cv_mape_bayes_svr.mean():.4f}%\")\n",
        "\n",
        "# You can select the overall best SVR model based on validation MAE\n",
        "# For instance, if BayesSearchCV gave the best result:\n",
        "# final_best_svr_model = best_svr_bayes\n",
        "# print(\"\\nEl mejor modelo SVR general basado en MAE promedio de validación es el de BayesSearchCV.\")\n",
        "\n",
        "# Optional: Train the chosen best SVR model on the entire X_train and evaluate on X_test\n",
        "# final_best_svr_model = best_svr_bayes # Or grid_search_svr.best_estimator_ or random_search_svr.best_estimator_\n",
        "\n",
        "# print(\"\\nEntrenando el modelo SVR final seleccionado en el conjunto de entrenamiento completo...\")\n",
        "# final_best_svr_model.fit(X_train, y_train)\n",
        "# print(\"Evaluando en el conjunto de prueba (X_test) con el modelo SVR final...\")\n",
        "# y_pred_test_final_svr = final_best_svr_model.predict(X_test)\n",
        "\n",
        "# # Calculate metrics on the test set with the final SVR model\n",
        "# test_mae_final_svr = mean_absolute_error(y_test, y_pred_test_final_svr)\n",
        "# test_mse_final_svr = mean_squared_error(y_test, y_pred_test_final_svr)\n",
        "# test_r2_final_svr = r2_score(y_test, y_pred_test_final_svr)\n",
        "# test_mape_final_svr = mean_absolute_percentage_error(y_test, y_pred_test_final_svr)\n",
        "\n",
        "# print(\"\\nMétricas en el conjunto de prueba con el modelo SVR final seleccionado:\")\n",
        "# print(f\"  MAE:  {test_mae_final_svr:.4f}\")\n",
        "# print(f\"  MSE:  {test_mse_final_svr:.4f}\")\n",
        "# print(f\"  R2:   {test_r2_final_svr:.4f}\")\n",
        "# print(f\"  MAPE: {test_mape_final_svr:.4f}%\")"
      ],
      "metadata": {
        "id": "fe6MWsg3mnrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resultados"
      ],
      "metadata": {
        "id": "YwY69B0RaQE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Crea gráficas de barras que comparen las métricas MAE, MSE, R2 y MAPE de cada uno de los modelos.\n",
        "# Indica los tres mejores modelos según los rendimientos de cada uno y para los tres mejores modelos, genera gráficas de importancia de características.\n",
        "# Asegúrate de que todas las gráficas sean claras, bien etiquetadas y adecuadas para un dashboard de Streamlit.\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import BayesianRidge, SGDRegressor\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from scipy.stats import randint\n",
        "from sklearn.linear_model import LinearRegression # Import LinearRegression for coefficient plotting\n",
        "\n",
        "# Dictionary to store the best results for each model (optimized)\n",
        "model_results = {\n",
        "    'Linear Regression (Default)': {\n",
        "        'MAE': cv_mae.mean(),\n",
        "        'MSE': cv_mse.mean(),\n",
        "        'R2': cv_r2.mean(),\n",
        "        'MAPE': cv_mape.mean()\n",
        "    },\n",
        "    'Lasso (Bayes)': { # Using BayesSearchCV as it often finds better solutions\n",
        "        'MAE': cv_mae_bayes.mean(),\n",
        "        'MSE': cv_mse_bayes.mean(),\n",
        "        'R2': cv_r2_bayes.mean(),\n",
        "        'MAPE': cv_mape_bayes.mean()\n",
        "    },\n",
        "    'ElasticNet (Bayes)': { # Using BayesSearchCV\n",
        "        'MAE': cv_mae_bayes_en.mean(),\n",
        "        'MSE': cv_mse_bayes_en.mean(),\n",
        "        'R2': cv_r2_bayes_en.mean(),\n",
        "        'MAPE': cv_mape_bayes_en.mean()\n",
        "    },\n",
        "    'KernelRidge (Bayes)': { # Using BayesSearchCV\n",
        "        'MAE': cv_mae_bayes_krr.mean(),\n",
        "        'MSE': cv_mse_bayes_krr.mean(),\n",
        "        'R2': cv_r2_bayes_krr.mean(),\n",
        "        'MAPE': cv_mape_bayes_krr.mean()\n",
        "    },\n",
        "    'SGDRegressor (Bayes)': { # Using BayesSearchCV\n",
        "        'MAE': cv_mae_bayes_sgd.mean(),\n",
        "        'MSE': cv_mse_bayes_sgd.mean(),\n",
        "        'R2': cv_r2_bayes_sgd.mean(),\n",
        "        'MAPE': cv_mape_bayes_sgd.mean()\n",
        "    },\n",
        "     'BayesianRidge (Default)': { # As this model was run without optimization\n",
        "        'MAE': cv_mae.mean(), # These variables should hold the results from the last BayesianRidge run\n",
        "        'MSE': cv_mse.mean(),\n",
        "        'R2': cv_r2.mean(),\n",
        "        'MAPE': cv_mape.mean()\n",
        "    },\n",
        "    'GaussianProcessRegressor (Bayes)': { # Using BayesSearchCV\n",
        "        'MAE': -cv_results_best_gpr['test_mae'].mean(),\n",
        "        'MSE': -cv_results_best_gpr['test_mse'].mean(),\n",
        "        'R2': cv_results_best_gpr['test_r2'].mean(),\n",
        "        'MAPE': -cv_results_best_gpr['test_mape'].mean()\n",
        "    },\n",
        "    'RandomForestRegressor (Bayes)': { # Using BayesSearchCV\n",
        "        'MAE': -cv_results_best_rf['test_mae'].mean(),\n",
        "        'MSE': -cv_results_best_rf['test_mse'].mean(),\n",
        "        'R2': cv_results_best_rf['test_r2'].mean(),\n",
        "        'MAPE': -cv_results_best_rf['test_mape'].mean()\n",
        "    },\n",
        "    'SVR (Bayes)': { # Using BayesSearchCV\n",
        "        'MAE': -cv_results_bayes_best_svr['test_mae'].mean(),\n",
        "        'MSE': -cv_results_bayes_best_svr['test_mse'].mean(),\n",
        "        # Corrected: Access 'test_r2' from the dictionary\n",
        "        'R2': cv_results_bayes_best_svr['test_r2'].mean(),\n",
        "        'MAPE': cv_mape_bayes_svr.mean() # The variable name was slightly different\n",
        "    }\n",
        "}\n",
        "\n",
        "# Convert results to DataFrame for easy plotting\n",
        "results_df = pd.DataFrame.from_dict(model_results, orient='index')\n",
        "\n",
        "# Sort models by MAE (lower is better)\n",
        "results_df_sorted_mae = results_df.sort_values(by='MAE')\n",
        "print(\"\\nRanking de Modelos por MAE:\")\n",
        "print(results_df_sorted_mae[['MAE']])\n",
        "\n",
        "# Sort models by MSE (lower is better)\n",
        "results_df_sorted_mse = results_df.sort_values(by='MSE')\n",
        "print(\"\\nRanking de Modelos por MSE:\")\n",
        "print(results_df_sorted_mse[['MSE']])\n",
        "\n",
        "# Sort models by R2 (higher is better)\n",
        "results_df_sorted_r2 = results_df.sort_values(by='R2', ascending=False)\n",
        "print(\"\\nRanking de Modelos por R2:\")\n",
        "print(results_df_sorted_r2[['R2']])\n",
        "\n",
        "# Sort models by MAPE (lower is better)\n",
        "results_df_sorted_mape = results_df.sort_values(by='MAPE')\n",
        "print(\"\\nRanking de Modelos por MAPE:\")\n",
        "print(results_df_sorted_mape[['MAPE']])\n",
        "\n",
        "\n",
        "# Define the best models based on MAE (example, adjust if criteria changes)\n",
        "best_mae_models = results_df_sorted_mae.head(3).index.tolist()\n",
        "print(f\"\\nLos 3 mejores modelos según MAE son: {best_mae_models}\")\n",
        "\n",
        "# Define the best models based on MSE (example, adjust if criteria changes)\n",
        "best_mse_models = results_df_sorted_mse.head(3).index.tolist()\n",
        "print(f\"\\nLos 3 mejores modelos según MSE son: {best_mse_models}\")\n",
        "\n",
        "# Define the best models based on R2 (example, adjust if criteria changes)\n",
        "best_r2_models = results_df_sorted_r2.head(3).index.tolist()\n",
        "print(f\"\\nLos 3 mejores modelos según R2 son: {best_r2_models}\")\n",
        "\n",
        "# Define the best models based on MAPE (example, adjust if criteria changes)\n",
        "best_mape_models = results_df_sorted_mape.head(3).index.tolist()\n",
        "print(f\"\\nLos 3 mejores modelos según MAPE son: {best_mape_models}\")\n",
        "\n",
        "\n",
        "# Combine the top models from each metric (optional, to get a broader view)\n",
        "all_top_models = list(set(best_mae_models + best_mse_models + best_r2_models + best_mape_models))\n",
        "print(f\"\\nModelos que aparecieron en el top 3 de al menos una métrica: {all_top_models}\")\n",
        "\n",
        "# For the purpose of feature importance visualization, let's select the top 3 models based on MAE\n",
        "top_3_models_for_importance = best_mae_models\n",
        "print(f\"\\nSe generarán gráficas de importancia de características para los 3 mejores modelos según MAE: {top_3_models_for_importance}\")\n",
        "\n",
        "\n",
        "# Plotting function for metrics\n",
        "def plot_metrics_bar(results_df, metric, title, ylabel, ascending=True):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(x=results_df.index, y=metric, data=results_df.sort_values(by=metric, ascending=ascending))\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.xlabel(\"Modelo\", fontsize=12)\n",
        "    plt.ylabel(ylabel, fontsize=12)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Generate bar plots for each metric\n",
        "plot_metrics_bar(results_df, 'MAE', 'Comparación de Modelos por MAE', 'MAE', ascending=True)\n",
        "plot_metrics_bar(results_df, 'MSE', 'Comparación de Modelos por MSE', 'MSE', ascending=True)\n",
        "plot_metrics_bar(results_df, 'R2', 'Comparación de Modelos por R2', 'R2 Score', ascending=False)\n",
        "plot_metrics_bar(results_df, 'MAPE', 'Comparación de Modelos por MAPE', 'MAPE (%)', ascending=True)\n",
        "\n",
        "# Function to get the best estimator for a given model name (optimized via BayesSearchCV if available)\n",
        "def get_best_estimator(model_name):\n",
        "    # Make sure to have the optimizer objects defined in the environment if needed\n",
        "    # (e.g., bayes_search, bayes_search_en, etc.)\n",
        "    if model_name == 'Linear Regression (Default)':\n",
        "        return LinearRegression().fit(X_train, y_train) # Train default Linear Regression for coefficients\n",
        "    elif model_name == 'Lasso (Bayes)':\n",
        "        # Assuming bayes_search is the BayesSearchCV object for Lasso\n",
        "        return bayes_search.best_estimator_\n",
        "    elif model_name == 'ElasticNet (Bayes)':\n",
        "         # Assuming bayes_search_en is the BayesSearchCV object for ElasticNet\n",
        "        return bayes_search_en.best_estimator_\n",
        "    elif model_name == 'KernelRidge (Bayes)':\n",
        "         # KRR does not have feature coefficients like linear models or tree-based models\n",
        "         print(f\"Advertencia: {model_name} no proporciona importancia de características directa (coeficientes).\")\n",
        "         return None\n",
        "    elif model_name == 'SGDRegressor (Bayes)':\n",
        "         # Assuming bayes_search_sgd is the BayesSearchCV object for SGDRegressor\n",
        "        return bayes_search_sgd.best_estimator_\n",
        "    elif model_name == 'BayesianRidge (Default)':\n",
        "        # Train a default BayesianRidge model for coefficients\n",
        "        return BayesianRidge().fit(X_train, y_train)\n",
        "    elif model_name == 'GaussianProcessRegressor (Bayes)':\n",
        "        # GPR does not have feature importance based on coefficients or tree structure\n",
        "        print(f\"Advertencia: {model_name} no proporciona importancia de características directa.\")\n",
        "        return None\n",
        "    elif model_name == 'RandomForestRegressor (Bayes)':\n",
        "        # Assuming bayes_search_rf is the BayesSearchCV object for RandomForestRegressor\n",
        "        return bayes_search_rf.best_estimator_\n",
        "    elif model_name == 'SVR (Bayes)':\n",
        "        # SVR with RBF kernel does not provide linear coefficients\n",
        "        print(f\"Advertencia: {model_name} no proporciona importancia de características directa.\")\n",
        "        return None\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "# Generate Feature Importance plots for the top 3 models (based on MAE)\n",
        "print(\"\\nGenerando gráficas de importancia de características para los 3 mejores modelos (según MAE):\")\n",
        "\n",
        "for model_name in top_3_models_for_importance:\n",
        "    print(f\"\\nImportancia de Características para: {model_name}\")\n",
        "    best_model = get_best_estimator(model_name)\n",
        "\n",
        "    if best_model:\n",
        "        if hasattr(best_model, 'coef_'): # For linear models (LinearRegression, Lasso, ElasticNet, SGDRegressor, BayesianRidge)\n",
        "            # Ensure X_train is a pandas DataFrame with named columns for plotting\n",
        "            if isinstance(X_train, pd.DataFrame):\n",
        "                coefficients = pd.Series(best_model.coef_, index=X_train.columns)\n",
        "                # Sort coefficients by absolute value for importance\n",
        "                sorted_coefficients = coefficients.abs().sort_values(ascending=False)\n",
        "\n",
        "                plt.figure(figsize=(10, min(20, len(sorted_coefficients) * 0.3))) # Adjust figure size based on num features\n",
        "                sns.barplot(x=sorted_coefficients.values, y=sorted_coefficients.index)\n",
        "                plt.title(f'Importancia de Características (Coeficientes Absolutos) - {model_name}', fontsize=14)\n",
        "                plt.xlabel('Magnitud del Coeficiente', fontsize=12)\n",
        "                plt.ylabel('Característica', fontsize=12)\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "            else:\n",
        "                 print(f\"Advertencia: X_train no es un DataFrame. No se puede generar la gráfica de importancia de características para {model_name}.\")\n",
        "\n",
        "\n",
        "        elif hasattr(best_model, 'feature_importances_'): # For tree-based models (RandomForestRegressor)\n",
        "             # Ensure X_train is a pandas DataFrame with named columns for plotting\n",
        "            if isinstance(X_train, pd.DataFrame):\n",
        "                importances = pd.Series(best_model.feature_importances_, index=X_train.columns)\n",
        "                # Sort feature importances\n",
        "                sorted_importances = importances.sort_values(ascending=False)\n",
        "\n",
        "                plt.figure(figsize=(10, min(20, len(sorted_importances) * 0.3))) # Adjust figure size\n",
        "                sns.barplot(x=sorted_importances.values, y=sorted_importances.index)\n",
        "                plt.title(f'Importancia de Características - {model_name}', fontsize=14)\n",
        "                plt.xlabel('Importancia', fontsize=12)\n",
        "                plt.ylabel('Característica', fontsize=12)\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "            else:\n",
        "                 print(f\"Advertencia: X_train no es un DataFrame. No se puede generar la gráfica de importancia de características para {model_name}.\")\n",
        "        else:\n",
        "             print(f\"El modelo {model_name} no tiene atributos 'coef_' o 'feature_importances_'.\")\n",
        "    else:\n",
        "        print(f\"No se pudo obtener el mejor estimador para {model_name} o no aplica para importancia de características.\")\n",
        "\n",
        "\n",
        "# Print the top 3 models based on each metric explicitly again\n",
        "print(\"\\n--- Resumen de los 3 Mejores Modelos por Métrica ---\")\n",
        "print(\"Según MAE (menor es mejor):\")\n",
        "print(results_df_sorted_mae.head(3)[['MAE']])\n",
        "print(\"\\nSegún MSE (menor es mejor):\")\n",
        "print(results_df_sorted_mse.head(3)[['MSE']])\n",
        "print(\"\\nSegún R2 (mayor es mejor):\")\n",
        "print(results_df_sorted_r2.head(3)[['R2']])\n",
        "print(\"\\nSegún MAPE (menor es mejor):\")\n",
        "print(results_df_sorted_mape.head(3)[['MAPE']])"
      ],
      "metadata": {
        "id": "P_Gsj2pjb8h4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DashBoard"
      ],
      "metadata": {
        "id": "SFUbGc-hlZFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_validate\n",
        "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet, SGDRegressor, BayesianRidge\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Integer\n",
        "from scipy.stats import loguniform, uniform, randint\n",
        "import pickle\n",
        "from joblib import dump\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Configuración de la página de Streamlit\n",
        "st.set_page_config(layout=\"wide\", page_title=\"Análisis y Modelización de Ames Housing\")\n",
        "\n",
        "# Título\n",
        "st.title('Análisis y Modelización de Precios de Viviendas - Ames Housing')\n",
        "\n",
        "# Documentación Formal: Introducción\n",
        "st.header('Documentación Formal: Análisis Exploratorio y Modelización')\n",
        "st.markdown(\"\"\"\n",
        "### Introducción\n",
        "Este dashboard presenta el análisis exploratorio de datos (EDA), el preprocesamiento y la modelización del dataset Ames Housing para predecir el precio de venta ('SalePrice'). El dataset incluye 2930 observaciones y 81 características. Este trabajo cumple con los requisitos del punto 3 del examen de Teoría de Aprendizaje de Máquina.\n",
        "\n",
        "### Descripción de los Datos\n",
        "El dataset Ames Housing contiene variables numéricas (como 'GrLivArea', 'TotalBsmtSF') y categóricas (como 'Neighborhood', 'MSZoning'). La variable objetivo es 'SalePrice'.\n",
        "\n",
        "### Metodología\n",
        "- **Preprocesamiento:** Manejo de valores faltantes (mediana para numéricas, moda o 'missing' para categóricas), codificación de variables categóricas (One-Hot para nominales, Ordinal para ordinales), ingeniería de características (creación de 'Age' y 'TotalSF'), y escalado de variables numéricas.\n",
        "- **EDA:** Visualizaciones para entender la distribución de variables y su relación con 'SalePrice'.\n",
        "- **Modelización:** Entrenamiento de múltiples modelos de regresión con optimización de hiperparámetros (GridSearchCV, RandomizedSearchCV, BayesSearchCV), evaluación con métricas MAE, MSE, R2 y MAPE.\n",
        "- **Dashboard:** Visualización interactiva de los datos y comparación de los tres mejores modelos.\n",
        "\"\"\")\n",
        "\n",
        "# Cargar los datos\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    try:\n",
        "        df = pd.read_csv('/content/drive/Shareddrives/UNAL_Colab/Teoría de Aprendizaje de Máquina/AmesHousing.csv')\n",
        "        df.columns = df.columns.str.replace(' ', '_').str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        st.error(\"Error: Asegúrese de que el archivo 'AmesHousing.csv' esté disponible.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "df = load_data()\n",
        "if df.empty:\n",
        "    st.stop()\n",
        "\n",
        "st.subheader(\"Datos Originales (primeras 5 filas)\")\n",
        "st.dataframe(df.head())\n",
        "\n",
        "# Documentación Formal: Preprocesamiento\n",
        "st.header('Preprocesamiento de Datos')\n",
        "st.markdown(\"\"\"\n",
        "### Metodología de Preprocesamiento\n",
        "El preprocesamiento asegura que los datos estén limpios y listos para el modelado, abordando los siguientes pasos:\n",
        "\n",
        "1. **Manejo de Valores Faltantes**\n",
        "   - **Numéricas:** Se imputaron con la mediana para columnas como 'LotFrontage' y 'GarageYrBlt', ya que es robusta frente a outliers.\n",
        "   - **Categóricas:** Se imputaron con 'missing' para columnas como 'Alley', 'Fence', 'MiscFeature', 'PoolQC' y 'FireplaceQu', reconociendo que la ausencia de datos puede ser informativa (por ejemplo, 'missing' en 'Alley' indica que no hay callejón). Otras categóricas, como 'MasVnrType' y 'Electrical', se imputaron con la moda para preservar la distribución.\n",
        "   - **Justificación:** La mediana minimiza el impacto de valores extremos en numéricas, mientras que 'missing' y la moda son adecuadas para categóricas según el contexto.\n",
        "\n",
        "2. **Codificación de Variables Categóricas**\n",
        "   - **Nominales:** Columnas como 'MSZoning', 'Neighborhood', 'MasVnrType' se codificaron con One-Hot Encoding, generando variables binarias para cada categoría, eliminando la primera categoría para evitar multicolinealidad.\n",
        "   - **Ordinales:** Columnas como 'ExterQual', 'BsmtQual', 'HeatingQC' se codificaron con Ordinal Encoding, asignando valores numéricos según un orden definido (por ejemplo, 'Po' < 'Fa' < 'TA' < 'Gd' < 'Ex').\n",
        "   - **Justificación:** One-Hot Encoding es ideal para variables sin orden inherente, mientras que Ordinal Encoding respeta la jerarquía en variables de calidad o condición.\n",
        "\n",
        "3. **Ingeniería de Características**\n",
        "   - **TotalSF:** Suma de 'TotalBsmtSF', '1stFlrSF' y '2ndFlrSF', representando el área total habitable.\n",
        "   - **Age:** Diferencia entre 'YrSold' y 'YearBuilt', indicando la antigüedad de la propiedad.\n",
        "   - **Justificación:** Estas características combinan información relevante, reduciendo la dimensionalidad y capturando factores clave que afectan el precio de venta.\n",
        "\n",
        "4. **Escalado de Variables Numéricas**\n",
        "   - Se aplicó `StandardScaler` a todas las variables numéricas, transformándolas a una distribución con media 0 y desviación estándar 1.\n",
        "   - **Justificación:** El escalado es esencial para modelos sensibles a la escala, como KernelRidge o SVR, y mejora la comparabilidad entre características.\n",
        "\n",
        "5. **Análisis de Correlaciones**\n",
        "   - Se identificaron pares de características con correlaciones altas (>0.8), como 'TotalBsmtSF' y '1stFlrSF', para evaluar redundancias.\n",
        "   - **Justificación:** Eliminar características redundantes reduce el riesgo de multicolinealidad y mejora la eficiencia del modelado.\n",
        "\n",
        "**Guardado de Datos Preprocesados**\n",
        "Los datos preprocesados se guardan en 'preprocessed_data.csv' para su uso en el entrenamiento de modelos, asegurando consistencia y evitando reprocesamiento redundante.\n",
        "\"\"\")\n",
        "\n",
        "# Preprocesamiento\n",
        "@st.cache_data\n",
        "def preprocess_data(df):\n",
        "    df_processed = df.copy()\n",
        "    threshold = len(df_processed) * 0.5\n",
        "    cols_to_drop_nulls = df_processed.columns[df_processed.isnull().sum() > threshold].tolist()\n",
        "    df_processed = df_processed.drop(columns=cols_to_drop_nulls)\n",
        "    st.sidebar.write(f\"Columnas eliminadas por exceso de nulos (>50%): {cols_to_drop_nulls}\")\n",
        "\n",
        "    numerical_cols = df_processed.select_dtypes(include=np.number).columns.tolist()\n",
        "    categorical_cols = df_processed.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "    numeric_cols_median = ['LotFrontage', 'GarageYrBlt']\n",
        "    categorical_cols_mode = ['MasVnrType', 'Electrical']\n",
        "    categorical_cols_missing = ['Alley', 'Fence', 'MiscFeature', 'PoolQC', 'FireplaceQu']\n",
        "\n",
        "    for col in numeric_cols_median:\n",
        "        if col in df_processed.columns and df_processed[col].isnull().any():\n",
        "            imputer_median = SimpleImputer(strategy='median')\n",
        "            df_processed[col] = imputer_median.fit_transform(df_processed[[col]]).ravel()\n",
        "\n",
        "    for col in categorical_cols_mode:\n",
        "        if col in df_processed.columns and df_processed[col].isnull().any():\n",
        "            imputer_mode = SimpleImputer(strategy='most_frequent')\n",
        "            df_processed[col] = imputer_mode.fit_transform(df_processed[[col]]).ravel()\n",
        "\n",
        "    for col in categorical_cols_missing:\n",
        "        if col in df_processed.columns and df_processed[col].isnull().any():\n",
        "            imputer_missing = SimpleImputer(strategy='constant', fill_value='missing')\n",
        "            df_processed[col] = imputer_missing.fit_transform(df_processed[[col]]).ravel()\n",
        "\n",
        "    if 'YearBuilt' in df_processed.columns:\n",
        "        df_processed['Age'] = 2023 - df_processed['YearBuilt']\n",
        "        df_processed = df_processed.drop(columns=['YearBuilt'])\n",
        "    if 'YearRemodAdd' in df_processed.columns:\n",
        "        df_processed['YearsSinceRemodel'] = 2023 - df_processed['YearRemodAdd']\n",
        "        df_processed = df_processed.drop(columns=['YearRemodAdd'])\n",
        "    if all(col in df_processed.columns for col in ['TotalBsmtSF', '1stFlrSF', '2ndFlrSF']):\n",
        "        df_processed['TotalSF'] = df_processed['TotalBsmtSF'] + df_processed['1stFlrSF'] + df_processed['2ndFlrSF']\n",
        "\n",
        "    nominal_cols = ['MSZoning', 'Neighborhood', 'MasVnrType', 'Exterior1st', 'Exterior2nd', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'SaleType', 'SaleCondition']\n",
        "    nominal_cols = [col for col in nominal_cols if col in df_processed.columns]\n",
        "    ordinal_mapping = {\n",
        "        'ExterQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex', 'missing'],\n",
        "        'ExterCond': ['Po', 'Fa', 'TA', 'Gd', 'Ex', 'missing'],\n",
        "        'BsmtQual': ['missing', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
        "        'BsmtCond': ['missing', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
        "        'HeatingQC': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
        "        'KitchenQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
        "        'FireplaceQu': ['missing', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
        "        'GarageQual': ['missing', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
        "        'GarageCond': ['missing', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
        "        'PoolQC': ['missing', 'Fa', 'Gd', 'Ex']\n",
        "    }\n",
        "    ordinal_cols = [col for col in ordinal_mapping.keys() if col in df_processed.columns]\n",
        "\n",
        "    if nominal_cols:\n",
        "        df_processed = pd.get_dummies(df_processed, columns=nominal_cols, drop_first=True)\n",
        "    if ordinal_cols:\n",
        "        ordinal_encoder = OrdinalEncoder(categories=[ordinal_mapping[col] for col in ordinal_cols])\n",
        "        df_processed[ordinal_cols] = ordinal_encoder.fit_transform(df_processed[ordinal_cols])\n",
        "\n",
        "    numeric_cols_to_scale = df_processed.select_dtypes(include=np.number).columns.tolist()\n",
        "    if 'SalePrice' in numeric_cols_to_scale:\n",
        "        numeric_cols_to_scale.remove('SalePrice')\n",
        "    if 'Id' in numeric_cols_to_scale:\n",
        "        numeric_cols_to_scale.remove('Id')\n",
        "    if numeric_cols_to_scale:\n",
        "        scaler = StandardScaler()\n",
        "        df_processed[numeric_cols_to_scale] = scaler.fit_transform(df_processed[numeric_cols_to_scale])\n",
        "\n",
        "    numeric_df = df_processed.select_dtypes(include=np.number)\n",
        "    correlation_matrix = numeric_df.corr()\n",
        "    correlation_threshold = 0.8\n",
        "    upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
        "    to_drop_high_corr = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\n",
        "    st.sidebar.write(f\"Columnas con alta correlación (> {correlation_threshold}): {to_drop_high_corr}\")\n",
        "\n",
        "    # Guardar datos preprocesados\n",
        "    df_processed.to_csv('preprocessed_data.csv', index=False)\n",
        "    return df_processed\n",
        "\n",
        "df_processed = preprocess_data(df.copy())\n",
        "st.subheader(\"Datos Preprocesados (primeras 5 filas)\")\n",
        "st.dataframe(df_processed.head())\n",
        "\n",
        "# Documentación Formal: Visualizaciones\n",
        "st.header('Análisis Exploratorio de Datos')\n",
        "st.markdown(\"\"\"\n",
        "### Visualizaciones Clave\n",
        "El análisis exploratorio utiliza visualizaciones para identificar patrones, distribuciones y relaciones en el dataset:\n",
        "\n",
        "1. **Distribución de SalePrice**\n",
        "   - **Histograma:** Muestra la distribución de los precios de venta, con un sesgo positivo que indica una mayoría de propiedades de menor valor y algunos outliers de alto valor.\n",
        "   - **Boxplot:** Identifica outliers en los precios altos, sugiriendo la necesidad de transformaciones o manejo de valores extremos.\n",
        "\n",
        "2. **Distribución de Variables Numéricas**\n",
        "   - Histogramas y boxplots para variables como 'GrLivArea', 'TotalBsmtSF', 'Age', 'TotalSF' muestran sus distribuciones y posibles outliers, proporcionando información sobre su impacto en el precio.\n",
        "\n",
        "3. **Relación con SalePrice**\n",
        "   - Gráficos de dispersión entre 'SalePrice' y las variables más correlacionadas (como 'OverallQual', 'GrLivArea') revelan relaciones lineales y no lineales, destacando características predictivas clave.\n",
        "\n",
        "4. **Distribución de Variables Categóricas**\n",
        "   - Gráficos de barras para variables como 'Neighborhood' y 'MSZoning' muestran la frecuencia de categorías, ayudando a entender su distribución y posible influencia en 'SalePrice'.\n",
        "\n",
        "5. **Matriz de Correlación**\n",
        "   - Un mapa de calor identifica correlaciones entre variables numéricas, destacando relaciones fuertes y posibles redundancias.\n",
        "\"\"\")\n",
        "\n",
        "# Visualizaciones\n",
        "st.sidebar.subheader(\"Opciones de Visualización\")\n",
        "viz_option = st.sidebar.selectbox(\"Selecciona una visualización:\",\n",
        "                                  [\"Distribución de SalePrice\",\n",
        "                                   \"Mapa de Calor de Correlación\",\n",
        "                                   \"Relación con SalePrice (Scatter plots)\",\n",
        "                                   \"Distribución de Variables Categóricas\",\n",
        "                                   \"Distribución de Variables Numéricas\"])\n",
        "\n",
        "if viz_option == \"Distribución de SalePrice\":\n",
        "    st.subheader(\"Distribución de SalePrice\")\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    sns.histplot(df['SalePrice'], kde=True, ax=axes[0])\n",
        "    axes[0].set_title('Distribución de SalePrice')\n",
        "    axes[0].set_xlabel('SalePrice')\n",
        "    axes[0].set_ylabel('Frecuencia')\n",
        "    sns.boxplot(y=df['SalePrice'], ax=axes[1])\n",
        "    axes[1].set_title('Boxplot de SalePrice')\n",
        "    axes[1].set_ylabel('SalePrice')\n",
        "    plt.tight_layout()\n",
        "    st.pyplot(fig)\n",
        "    st.markdown(\"\"\"\n",
        "    **Análisis:** La distribución de 'SalePrice' muestra un sesgo positivo (skewness > 0.5), con outliers en valores altos. Esto sugiere considerar una transformación logarítmica para normalizar los datos en el modelado.\n",
        "    \"\"\")\n",
        "\n",
        "elif viz_option == \"Mapa de Calor de Correlación\":\n",
        "    st.subheader(\"Mapa de Calor de Correlación\")\n",
        "    numeric_df = df_processed.select_dtypes(include=np.number)\n",
        "    if 'SalePrice' in numeric_df.columns:\n",
        "        correlation_with_saleprice = numeric_df.corr()['SalePrice'].sort_values(ascending=False)\n",
        "        st.write(\"Correlación de variables numéricas con SalePrice:\")\n",
        "        st.dataframe(correlation_with_saleprice)\n",
        "        fig, ax = plt.subplots(figsize=(8, 10))\n",
        "        sns.heatmap(correlation_with_saleprice.to_frame(), cmap='coolwarm', annot=True, fmt=\".2f\", cbar=False, ax=ax)\n",
        "        ax.set_title('Correlación con SalePrice')\n",
        "        ax.tick_params(axis='y', rotation=0)\n",
        "        st.pyplot(fig)\n",
        "        st.markdown(\"\"\"\n",
        "        **Análisis:** Variables como 'OverallQual' y 'GrLivArea' tienen correlaciones altas con 'SalePrice', indicando su importancia predictiva. Las correlaciones negativas son menos comunes, pero pueden reflejar factores como la antigüedad ('Age').\n",
        "        \"\"\")\n",
        "\n",
        "elif viz_option == \"Relación con SalePrice (Scatter plots)\":\n",
        "    st.subheader(\"Relación entre SalePrice y Variables Numéricas Clave\")\n",
        "    numeric_df = df_processed.select_dtypes(include=np.number)\n",
        "    if 'SalePrice' in numeric_df.columns:\n",
        "        correlation_with_saleprice = numeric_df.corr()['SalePrice'].sort_values(ascending=False)\n",
        "        top_n = st.slider(\"Número de variables más correlacionadas a mostrar:\", 3, 10, 5)\n",
        "        top_correlated_cols = correlation_with_saleprice.head(top_n + 1).index.tolist()\n",
        "        if 'SalePrice' in top_correlated_cols:\n",
        "            top_correlated_cols.remove('SalePrice')\n",
        "        st.write(f\"Mostrando la relación de SalePrice con las {len(top_correlated_cols)} variables más correlacionadas:\")\n",
        "        num_cols_plot = 3\n",
        "        num_rows_plot = int(np.ceil(len(top_correlated_cols) / num_cols_plot))\n",
        "        fig, axes = plt.subplots(num_rows_plot, num_cols_plot, figsize=(15, 5 * num_rows_plot))\n",
        "        axes = axes.flatten() if num_rows_plot > 1 else [axes]\n",
        "        for i, col in enumerate(top_correlated_cols):\n",
        "            if col in numeric_df.columns:\n",
        "                sns.scatterplot(x=numeric_df[col], y=numeric_df['SalePrice'], ax=axes[i])\n",
        "                axes[i].set_title(f'SalePrice vs {col}')\n",
        "                axes[i].set_xlabel(col)\n",
        "                axes[i].set_ylabel('SalePrice')\n",
        "            else:\n",
        "                axes[i].set_visible(False)\n",
        "        for j in range(i + 1, len(axes)):\n",
        "            axes[j].set_visible(False)\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(fig)\n",
        "        st.markdown(\"\"\"\n",
        "        **Análisis:** Los gráficos de dispersión muestran relaciones lineales y no lineales entre 'SalePrice' y variables clave, como 'GrLivArea' (positiva) y 'Age' (potencialmente negativa), destacando su relevancia para la predicción.\n",
        "        \"\"\")\n",
        "\n",
        "elif viz_option == \"Distribución de Variables Categóricas\":\n",
        "    st.subheader(\"Distribución de Variables Categóricas\")\n",
        "    df_original = load_data()\n",
        "    df_original.columns = df_original.columns.str.replace(' ', '_').str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
        "    categorical_cols = df_original.select_dtypes(include='object').columns.tolist()\n",
        "    for col in categorical_cols:\n",
        "        if df_original[col].isnull().any():\n",
        "            df_original[col].fillna('Missing', inplace=True)\n",
        "    if categorical_cols:\n",
        "        selected_cols = st.multiselect(\"Selecciona variables categóricas para visualizar:\", categorical_cols, default=categorical_cols[:5])\n",
        "        if selected_cols:\n",
        "            num_cols_plot = 2\n",
        "            num_rows_plot = int(np.ceil(len(selected_cols) / num_cols_plot))\n",
        "            fig, axes = plt.subplots(num_rows_plot, num_cols_plot, figsize=(15, 5 * num_rows_plot))\n",
        "            axes = axes.flatten() if num_rows_plot > 1 else [axes]\n",
        "            for i, col in enumerate(selected_cols):\n",
        "                if col in df_original.columns and df_original[col].dtype == 'object':\n",
        "                    sns.countplot(y=col, data=df_original, order=df_original[col].value_counts().index, ax=axes[i])\n",
        "                    axes[i].set_title(f'Distribución de {col}')\n",
        "                    axes[i].set_xlabel('Frecuencia')\n",
        "                    axes[i].set_ylabel(col)\n",
        "                else:\n",
        "                    axes[i].set_visible(False)\n",
        "            for j in range(i + 1, len(axes)):\n",
        "                axes[j].set_visible(False)\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "            st.markdown(\"\"\"\n",
        "            **Análisis:** Los gráficos de barras muestran la distribución de categorías, como 'Neighborhood', revelando la composición del dataset y posibles influencias en 'SalePrice'.\n",
        "            \"\"\")\n",
        "\n",
        "elif viz_option == \"Distribución de Variables Numéricas\":\n",
        "    st.subheader(\"Distribución de Variables Numéricas\")\n",
        "    numeric_cols = df_processed.select_dtypes(include=np.number).columns.tolist()\n",
        "    if 'SalePrice' in numeric_cols:\n",
        "        numeric_cols.remove('SalePrice')\n",
        "    if numeric_cols:\n",
        "        selected_cols = st.multiselect(\"Selecciona variables numéricas para visualizar:\", numeric_cols, default=numeric_cols[:5])\n",
        "        if selected_cols:\n",
        "            num_cols_plot = 3\n",
        "            num_rows_plot = int(np.ceil(len(selected_cols) / num_cols_plot)) * 2\n",
        "            fig, axes = plt.subplots(num_rows_plot, num_cols_plot, figsize=(15, 5 * num_rows_plot/2))\n",
        "            axes = axes.flatten() if num_rows_plot > 1 else [axes]\n",
        "            plot_index = 0\n",
        "            for i, col in enumerate(selected_cols):\n",
        "                if col in df_processed.columns and df_processed[col].dtype in [np.number]:\n",
        "                    sns.histplot(df_processed[col], kde=True, ax=axes[plot_index])\n",
        "                    axes[plot_index].set_title(f'Distribución de {col}')\n",
        "                    axes[plot_index].set_xlabel(col)\n",
        "                    axes[plot_index].set_ylabel('Frecuencia')\n",
        "                    plot_index += 1\n",
        "                    sns.boxplot(y=df_processed[col], ax=axes[plot_index])\n",
        "                    axes[plot_index].set_title(f'Boxplot de {col}')\n",
        "                    axes[plot_index].set_ylabel(col)\n",
        "                    plot_index += 1\n",
        "                else:\n",
        "                    axes[plot_index].set_visible(False)\n",
        "                    axes[plot_index+1].set_visible(False)\n",
        "                    plot_index += 2\n",
        "            for j in range(plot_index, len(axes)):\n",
        "                axes[j].set_visible(False)\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "            st.markdown(\"\"\"\n",
        "            **Análisis:** Los histogramas y boxplots muestran la distribución y presencia de outliers en variables numéricas, como 'GrLivArea' y 'TotalSF', proporcionando información sobre su impacto en el modelado.\n",
        "            \"\"\")\n",
        "\n",
        "# Documentación Formal: Modelización\n",
        "# Documentación Formal: Modelización\n",
        "st.header('Modelización de Datos')\n",
        "st.markdown(\"\"\"\n",
        "### Metodología de Modelización\n",
        "Se entrenaron y evaluaron múltiples modelos de regresión para predecir 'SalePrice', utilizando validación cruzada de 5 pliegues y optimización de hiperparámetros con GridSearchCV, RandomizedSearchCV y BayesSearchCV. Los modelos son:\n",
        "\n",
        "- **LinearRegression**: Modelo base para relaciones lineales simples, sin optimización de hiperparámetros debido a su simplicidad.\n",
        "- **Lasso**: Regularización L1 para selección de características, útil para datasets con multicolinealidad.\n",
        "- **ElasticNet**: Combina regularización L1 y L2, balanceando selección y estabilidad.\n",
        "- **KernelRidge**: Captura relaciones no lineales mediante un kernel RBF, adecuado para patrones complejos.\n",
        "- **SGDRegressor**: Optimización basada en gradiente descendente, eficiente para datasets grandes.\n",
        "- **BayesianRidge**: Modelo bayesiano robusto a multicolinealidad, con parámetros predeterminados.\n",
        "- **RandomForestRegressor**: Ensamblado de árboles, robusto a valores faltantes y relaciones no lineales.\n",
        "- **SVR**: Máquinas de soporte vectorial con kernel RBF, efectivas para relaciones no lineales.\n",
        "\n",
        "### Optimización de Hiperparámetros\n",
        "Se utilizaron tres métodos de optimización:\n",
        "- **GridSearchCV**: Explora exhaustivamente un conjunto finito de hiperparámetros, garantizando la mejor combinación dentro del rango definido.\n",
        "- **RandomizedSearchCV**: Muestrea aleatoriamente combinaciones de hiperparámetros, eficiente para espacios grandes.\n",
        "- **BayesSearchCV**: Usa optimización bayesiana para explorar el espacio de hiperparámetros de manera eficiente, priorizando combinaciones prometedoras.\n",
        "\n",
        "### Rangos de Hiperparámetros y Justificación\n",
        "- **Lasso**:\n",
        "  - **GridSearchCV**: `alpha = [0.001, 0.01, 0.1, 1, 10]`. Rango logarítmico para cubrir diferentes niveles de regularización, desde baja (0.001) hasta alta (10), permitiendo selección de características efectiva.\n",
        "  - **RandomizedSearchCV/BayesSearchCV**: `alpha = loguniform(0.001, 10)`. La distribución loguniforme explora un rango continuo, capturando valores pequeños y grandes para adaptarse a la escala de los datos.\n",
        "- **ElasticNet**:\n",
        "  - **GridSearchCV**: `alpha = [0.001, 0.01, 0.1, 1]`, `l1_ratio = [0.1, 0.3, 0.5]`. Rangos discretos para balancear regularización L1 y L2, enfocándose en valores bajos para evitar sobre-regularización.\n",
        "  - **RandomizedSearchCV/BayesSearchCV**: `alpha = loguniform(0.001, 1)`, `l1_ratio = uniform(0, 1)`. La distribución loguniforme para `alpha` cubre un espectro amplio, mientras que `l1_ratio` explora todo el rango de balance entre L1 y L2.\n",
        "- **KernelRidge**:\n",
        "  - **GridSearchCV**: `alpha = [0.001, 0.01, 0.1]`, `gamma = [0.001, 0.01]`. Rangos discretos para regularización y escala del kernel RBF, enfocados en valores pequeños para capturar no linealidades.\n",
        "  - **RandomizedSearchCV/BayesSearchCV**: `alpha = loguniform(0.001, 1)`, `gamma = loguniform(0.001, 1)`. Distribuciones loguniformes para explorar un rango continuo, adecuado para la sensibilidad del kernel RBF.\n",
        "- **SGDRegressor**:\n",
        "  - **GridSearchCV**: `alpha = [0.0001, 0.001]`. Rangos pequeños para regularización, ya que valores grandes pueden causar inestabilidad en el gradiente descendente.\n",
        "  - **RandomizedSearchCV/BayesSearchCV**: `alpha = loguniform(0.0001, 0.01)`. Rango logarítmico para explorar regularización fina, manteniendo estabilidad.\n",
        "- **RandomForestRegressor**:\n",
        "  - **GridSearchCV**: `n_estimators = [50, 100]`, `max_depth = [None, 10]`, `min_samples_split = [2, 5]`. Rangos reducidos para acelerar la optimización, cubriendo un número moderado de árboles y profundidades.\n",
        "  - **RandomizedSearchCV**: `n_estimators = randint(50, 150)`, `max_depth = [None] + list(range(5, 15))`, `min_samples_split = randint(2, 10)`. Rangos discretos para explorar complejidad del modelo.\n",
        "  - **BayesSearchCV**: `n_estimators = Integer(50, 150)`, `max_depth = Integer(5, 15)`, `min_samples_split = Integer(2, 10)`. Rangos continuos para una búsqueda más precisa.\n",
        "- **SVR**:\n",
        "  - **GridSearchCV**: `C = [1, 10, 100]`, `epsilon = [0.01, 0.1, 0.5]`, `gamma = ['scale', 'auto', 0.001]`. Rangos discretos para regularización, margen y escala del kernel.\n",
        "  - **RandomizedSearchCV/BayesSearchCV**: `C = loguniform(0.1, 1000)`, `epsilon = uniform(0.001, 1)`, `gamma = loguniform(0.0001, 1)`. Distribuciones continuas para explorar un espacio amplio.\n",
        "- **LinearRegression y BayesianRidge**: Sin optimización de hiperparámetros, ya que sus parámetros predeterminados son robustos para el dataset.\n",
        "\n",
        "### Número de Iteraciones\n",
        "- **RandomizedSearchCV y BayesSearchCV**: Se usaron 10 iteraciones para RandomForestRegressor y 50-100 para otros modelos, para balancear precisión y tiempo de cómputo. Menos iteraciones (10) se usaron en RandomForestRegressor debido a su alto costo computacional, mientras que 50-100 iteraciones en modelos más rápidos (como Lasso, ElasticNet) permiten una exploración más exhaustiva.\n",
        "- **Justificación**: El número de iteraciones refleja un compromiso entre explorar el espacio de hiperparámetros y mantener la eficiencia computacional, considerando que el dataset Ames Housing (~1460 filas tras división) no requiere búsquedas extensas para modelos lineales, pero sí para modelos no lineales.\n",
        "\n",
        "### Evaluación\n",
        "Se utilizó validación cruzada de 5 pliegues para garantizar estimaciones robustas de las métricas MAE, MSE, R2 y MAPE. Los resultados se guardan en un archivo pickle ('model_results.pkl') para su uso en el dashboard de Streamlit. Los mejores estimadores se guardan con joblib para pruebas posteriores.\n",
        "\"\"\")\n",
        "\n",
        "# Entrenamiento de Modelos\n",
        "st.subheader(\"Entrenamiento de Modelos\")\n",
        "\n",
        "# Verificar si los resultados ya existen para evitar reentrenamiento\n",
        "if not os.path.exists('model_results.pkl'):\n",
        "    try:\n",
        "        # Cargar datos preprocesados\n",
        "        st.write(\"Cargando datos preprocesados desde preprocessed_data.csv...\")\n",
        "        df = pd.read_csv('preprocessed_data.csv')\n",
        "\n",
        "        # Separar características y variable objetivo\n",
        "        st.write(\"Separando características y variable objetivo...\")\n",
        "        if 'SalePrice' in df.columns:\n",
        "            y = df['SalePrice']\n",
        "            X = df.drop('SalePrice', axis=1)\n",
        "        else:\n",
        "            st.error(\"La columna 'SalePrice' no se encontró.\")\n",
        "            st.stop()\n",
        "\n",
        "        # Asegurarse de que X sea numérico\n",
        "        X = X.select_dtypes(include=np.number)\n",
        "        X = X.dropna()\n",
        "        y = y.loc[X.index]\n",
        "\n",
        "        # División en conjuntos de entrenamiento y prueba\n",
        "        st.write(\"Dividiendo datos en entrenamiento y prueba...\")\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Definir métricas\n",
        "        def mean_absolute_percentage_error(y_true, y_pred):\n",
        "            y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "            epsilon = 1e-8\n",
        "            return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
        "\n",
        "        scoring = {\n",
        "            'mae': make_scorer(mean_absolute_error, greater_is_better=False),\n",
        "            'mse': make_scorer(mean_squared_error, greater_is_better=False),\n",
        "            'r2': make_scorer(r2_score, greater_is_better=True),\n",
        "            'mape': make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
        "        }\n",
        "        scoring_optimizer = {'mae': make_scorer(mean_absolute_error, greater_is_better=False)}\n",
        "\n",
        "        # Configurar KFold\n",
        "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "        # Definir modelos y espacios de búsqueda\n",
        "        models = {\n",
        "            'LinearRegression': LinearRegression(),\n",
        "            'Lasso': Lasso(random_state=42, max_iter=10000),\n",
        "            'ElasticNet': ElasticNet(random_state=42, max_iter=10000),\n",
        "            'KernelRidge': KernelRidge(kernel='rbf'),\n",
        "            'SGDRegressor': SGDRegressor(learning_rate='invscaling', early_stopping=True, random_state=42, max_iter=10000),\n",
        "            'BayesianRidge': BayesianRidge(),\n",
        "            'RandomForestRegressor': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
        "            'SVR': SVR(kernel='rbf')\n",
        "        }\n",
        "\n",
        "        param_grids = {\n",
        "            'Lasso': {'alpha': [0.001, 0.01, 0.1, 1, 10]},\n",
        "            'ElasticNet': {'alpha': [0.001, 0.01, 0.1, 1], 'l1_ratio': [0.1, 0.3, 0.5]},\n",
        "            'KernelRidge': {'alpha': [0.001, 0.01, 0.1], 'gamma': [0.001, 0.01]},\n",
        "            'SGDRegressor': {'alpha': [0.0001, 0.001]},\n",
        "            'RandomForestRegressor': {'n_estimators': [50, 100], 'max_depth': [None, 10], 'min_samples_split': [2, 5]},\n",
        "            'SVR': {'C': [1, 10, 100], 'epsilon': [0.01, 0.1, 0.5], 'gamma': ['scale', 'auto', 0.001]}\n",
        "        }\n",
        "\n",
        "        param_dists = {\n",
        "            'Lasso': {'alpha': loguniform(0.001, 10)},\n",
        "            'ElasticNet': {'alpha': loguniform(0.001, 1), 'l1_ratio': uniform(0, 1)},\n",
        "            'KernelRidge': {'alpha': loguniform(0.001, 1), 'gamma': loguniform(0.001, 1)},\n",
        "            'SGDRegressor': {'alpha': loguniform(0.0001, 0.01)},\n",
        "            'RandomForestRegressor': {'n_estimators': randint(50, 150), 'max_depth': [None] + list(range(5, 15)), 'min_samples_split': randint(2, 10)},\n",
        "            'SVR': {'C': loguniform(0.1, 1000), 'epsilon': uniform(0.001, 1), 'gamma': loguniform(0.0001, 1)}\n",
        "        }\n",
        "\n",
        "        param_spaces = {\n",
        "            'Lasso': {'alpha': Real(0.001, 10, prior='log-uniform')},\n",
        "            'ElasticNet': {'alpha': Real(0.001, 1, prior='log-uniform'), 'l1_ratio': Real(0, 1, prior='uniform')},\n",
        "            'KernelRidge': {'alpha': Real(0.001, 1, prior='log-uniform'), 'gamma': Real(0.001, 1, prior='log-uniform')},\n",
        "            'SGDRegressor': {'alpha': Real(0.0001, 0.01, prior='log-uniform')},\n",
        "            'RandomForestRegressor': {'n_estimators': Integer(50, 150), 'max_depth': Integer(5, 15), 'min_samples_split': Integer(2, 10)},\n",
        "            'SVR': {'C': Real(0.1, 1000, prior='log-uniform'), 'epsilon': Real(0.001, 1, prior='uniform'), 'gamma': Real(0.0001, 1, prior='log-uniform')}\n",
        "        }\n",
        "\n",
        "        # Entrenar y evaluar modelos\n",
        "        st.write(\"Iniciando entrenamiento de modelos...\")\n",
        "        model_results = {}\n",
        "        for name, model in models.items():\n",
        "            st.write(f\"Entrenando modelo: {name}\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            # GridSearchCV\n",
        "            if name not in ['LinearRegression', 'BayesianRidge']:\n",
        "                st.write(f\"Optimizando {name} con GridSearchCV...\")\n",
        "                grid_search = GridSearchCV(estimator=model, param_grid=param_grids[name], scoring=scoring_optimizer, refit='mae', cv=kf, verbose=1, n_jobs=-1)\n",
        "                grid_search.fit(X_train, y_train)\n",
        "                grid_results = cross_validate(grid_search.best_estimator_, X_train, y_train, cv=kf, scoring=scoring)\n",
        "                model_results[f\"{name}_Grid\"] = {\n",
        "                    'MAE': -grid_results['test_mae'].mean(),\n",
        "                    'MSE': -grid_results['test_mse'].mean(),\n",
        "                    'R2': grid_results['test_r2'].mean(),\n",
        "                    'MAPE': -grid_results['test_mape'].mean(),\n",
        "                    'Time (s)': time.time() - start_time,\n",
        "                    'Best Estimator': grid_search.best_estimator_,\n",
        "                    'Best Params': grid_search.best_params_\n",
        "                }\n",
        "\n",
        "            # RandomizedSearchCV\n",
        "            if name not in ['LinearRegression', 'BayesianRidge']:\n",
        "                st.write(f\"Optimizando {name} con RandomizedSearchCV...\")\n",
        "                random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dists[name], n_iter=50 if name != 'RandomForestRegressor' else 10, scoring=scoring_optimizer, refit='mae', cv=kf, verbose=1, random_state=42, n_jobs=-1)\n",
        "                random_search.fit(X_train, y_train)\n",
        "                random_results = cross_validate(random_search.best_estimator_, X_train, y_train, cv=kf, scoring=scoring)\n",
        "                model_results[f\"{name}_Random\"] = {\n",
        "                    'MAE': -random_results['test_mae'].mean(),\n",
        "                    'MSE': -random_results['test_mse'].mean(),\n",
        "                    'R2': random_results['test_r2'].mean(),\n",
        "                    'MAPE': -random_results['test_mape'].mean(),\n",
        "                    'Time (s)': time.time() - start_time,\n",
        "                    'Best Estimator': random_search.best_estimator_,\n",
        "                    'Best Params': random_search.best_params_\n",
        "                }\n",
        "\n",
        "            # BayesSearchCV\n",
        "            if name not in ['LinearRegression', 'BayesianRidge']:\n",
        "                st.write(f\"Optimizando {name} con BayesSearchCV...\")\n",
        "                bayes_search = BayesSearchCV(estimator=model, search_spaces=param_spaces[name], n_iter=50 if name != 'RandomForestRegressor' else 10, scoring=scoring_optimizer, refit='mae', cv=kf, verbose=1, random_state=42, n_jobs=-1)\n",
        "                bayes_search.fit(X_train, y_train)\n",
        "                bayes_results = cross_validate(bayes_search.best_estimator_, X_train, y_train, cv=kf, scoring=scoring)\n",
        "                model_results[f\"{name}_Bayes\"] = {\n",
        "                    'MAE': -bayes_results['test_mae'].mean(),\n",
        "                    'MSE': -bayes_results['test_mse'].mean(),\n",
        "                    'R2': bayes_results['test_r2'].mean(),\n",
        "                    'MAPE': -bayes_results['test_mape'].mean(),\n",
        "                    'Time (s)': time.time() - start_time,\n",
        "                    'Best Estimator': bayes_search.best_estimator_,\n",
        "                    'Best Params': bayes_search.best_params_\n",
        "                }\n",
        "\n",
        "            # Si no requiere optimización, evaluar directamente\n",
        "            if name in ['LinearRegression', 'BayesianRidge']:\n",
        "                st.write(f\"Evaluando {name} sin optimización...\")\n",
        "                model.fit(X_train, y_train)\n",
        "                results = cross_validate(model, X_train, y_train, cv=kf, scoring=scoring)\n",
        "                model_results[name] = {\n",
        "                    'MAE': -results['test_mae'].mean(),\n",
        "                    'MSE': -results['test_mse'].mean(),\n",
        "                    'R2': results['test_r2'].mean(),\n",
        "                    'MAPE': -results['test_mape'].mean(),\n",
        "                    'Time (s)': time.time() - start_time,\n",
        "                    'Best Estimator': model,\n",
        "                    'Best Params': 'Default'\n",
        "                }\n",
        "\n",
        "        # Guardar resultados en archivo pickle\n",
        "        st.write(\"Guardando resultados en model_results.pkl...\")\n",
        "        with open('model_results.pkl', 'wb') as f:\n",
        "            pickle.dump(model_results, f)\n",
        "\n",
        "        # Guardar los mejores estimadores con joblib\n",
        "        st.write(\"Guardando los mejores estimadores con joblib...\")\n",
        "        for key, result in model_results.items():\n",
        "            if '_Grid' in key or '_Random' in key or '_Bayes' in key:\n",
        "                model_name = key.split('_')[0]\n",
        "                dump(result['Best Estimator'], f\"{model_name}_best_model.joblib\")\n",
        "            elif key in ['LinearRegression', 'BayesianRidge']:\n",
        "                dump(result['Best Estimator'], f\"{key}_model.joblib\")\n",
        "\n",
        "        st.write(\"Entrenamiento y evaluación completados con éxito.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error durante la ejecución: {e}\")\n",
        "        st.stop()\n",
        "\n",
        "# Visualización de Resultados del Modelado\n",
        "st.header(\"Resultados del Entrenamiento de Modelos\")\n",
        "\n",
        "# Verificar si el archivo existe\n",
        "if os.path.exists(\"model_results.pkl\"):\n",
        "    with open(\"model_results.pkl\", \"rb\") as f:\n",
        "        model_results = pickle.load(f)\n",
        "\n",
        "    # Convertir resultados en DataFrame para visualización\n",
        "    results_summary = []\n",
        "    for model_name, metrics in model_results.items():\n",
        "        results_summary.append({\n",
        "            \"Modelo\": model_name,\n",
        "            \"MAE\": round(metrics[\"MAE\"], 2),\n",
        "            \"MSE\": round(metrics[\"MSE\"], 2),\n",
        "            \"R2\": round(metrics[\"R2\"], 3),\n",
        "            \"MAPE (%)\": round(metrics[\"MAPE\"], 2),\n",
        "            \"Tiempo (s)\": round(metrics[\"Time (s)\"], 2)\n",
        "        })\n",
        "    results_df = pd.DataFrame(results_summary)\n",
        "    st.subheader(\"Resumen de Métricas por Modelo\")\n",
        "    st.dataframe(results_df.sort_values(by=\"MAE\"))\n",
        "\n",
        "    # Visualización de Métricas\n",
        "    metric_to_plot = st.selectbox(\"Selecciona una métrica para comparar modelos:\", [\"MAE\", \"MSE\", \"R2\", \"MAPE (%)\", \"Tiempo (s)\"])\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    sns.barplot(data=results_df.sort_values(by=metric_to_plot, ascending=(metric_to_plot != \"R2\")), x=\"Modelo\", y=metric_to_plot, ax=ax)\n",
        "    ax.set_title(f\"Comparación de Modelos según {metric_to_plot}\")\n",
        "    ax.set_ylabel(metric_to_plot)\n",
        "    ax.set_xlabel(\"Modelo\")\n",
        "    plt.xticks(rotation=45)\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    # Mostrar los 3 mejores modelos según MAE\n",
        "    st.subheader(\"Top 3 Modelos según MAE\")\n",
        "    top_3_models = results_df.sort_values(by=\"MAE\").head(3)\n",
        "    st.dataframe(top_3_models)\n",
        "\n",
        "    # Comparación gráfica de los 3 mejores modelos\n",
        "    st.subheader(\"Comparación Gráfica de los 3 Mejores Modelos según MAE\")\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    sns.barplot(data=top_3_models, x=\"Modelo\", y=\"MAE\", ax=ax)\n",
        "    ax.set_title(\"Top 3 Modelos según MAE\")\n",
        "    ax.set_ylabel(\"MAE\")\n",
        "    ax.set_xlabel(\"Modelo\")\n",
        "    plt.xticks(rotation=45)\n",
        "    st.pyplot(fig)\n",
        "\n",
        "else:\n",
        "    st.warning(\"El archivo 'model_results.pkl' no se encuentra. Por favor, ejecuta el entrenamiento de modelos primero.\")\n",
        "\n",
        "# Documentación Formal: Conclusión\n",
        "st.header('Conclusión')\n",
        "st.markdown(\"\"\"\n",
        "El análisis exploratorio, preprocesamiento y modelización del dataset Ames Housing han permitido preparar los datos y entrenar modelos predictivos para 'SalePrice'. El preprocesamiento abordó valores faltantes, codificación de variables categóricas, ingeniería de características y escalado. Las visualizaciones del EDA identificaron patrones clave, como el sesgo en 'SalePrice' y la importancia de variables como 'OverallQual'. Los modelos fueron optimizados y evaluados, con los mejores resultados guardados para análisis futuros. Este dashboard permite explorar los datos y comparar modelos de manera interactiva.\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "DrW3pCPNNpco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Autenticar ngrok con tu authtoken\n",
        "try:\n",
        "    ngrok.set_auth_token(\"2xWiGNbMFuGVKYCpiKAC7OnphqY_6rpzZZckfZCpW1B3K3WQc\")\n",
        "    print(\"Authtoken de ngrok configurado correctamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al configurar el authtoken de ngrok: {e}\")\n",
        "\n",
        "# Ejecutar Streamlit en segundo plano\n",
        "streamlit_command = \"streamlit run TAM.py\"\n",
        "def run_streamlit():\n",
        "    os.system(streamlit_command)\n",
        "\n",
        "# Iniciar Streamlit en un hilo separado\n",
        "thread = threading.Thread(target=run_streamlit)\n",
        "thread.start()\n",
        "\n",
        "# Esperar a que Streamlit inicie\n",
        "time.sleep(5)\n",
        "\n",
        "# Crear un túnel ngrok al puerto 8501 con protocolo HTTP\n",
        "try:\n",
        "    public_url = ngrok.connect(8501, \"http\")\n",
        "    print(\"Tu dashboard está disponible en:\", public_url)\n",
        "except Exception as e:\n",
        "    print(f\"Error al crear el túnel ngrok: {e}\")\n",
        "    ngrok.kill()\n",
        "\n"
      ],
      "metadata": {
        "id": "EEcG3uKCt6OA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ps aux | grep ngrok"
      ],
      "metadata": {
        "id": "5ngdlg_0FQNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kill -9 48401"
      ],
      "metadata": {
        "id": "GMOFWhQVFwSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kill -9 48399"
      ],
      "metadata": {
        "id": "fh2ENrPIF4Ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok api tunnels --authtoken=2xWiGNbMFuGVKYCpiKAC7OnphqY_6rpzZZckfZCpW1B3K3WQc"
      ],
      "metadata": {
        "id": "QLTC6l-SGMFY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}